<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="niuoo" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="It is my Blog!">
<meta property="og:type" content="website">
<meta property="og:title" content="niuoo">
<meta property="og:url" content="http://niuoo.github.io/index.html">
<meta property="og:site_name" content="niuoo">
<meta property="og:description" content="It is my Blog!">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="niuoo">
<meta name="twitter:description" content="It is my Blog!">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://niuoo.github.io/"/>

  <title> niuoo </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">niuoo</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Develop with pleasure!</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/27/漫漫长夜/" itemprop="url">
                  漫漫长夜
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-27T00:47:46+08:00" content="May 27 2017">
              May 27 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>凌晨Sat May 27 00:49:20 CST 2017，我坐在书桌前，抒发心情。 最近工作上毫无意义的琐事消耗了太多时间。突然想着要不要去学校读个博什么的，可是会不会被很多无聊的事情烦扰，比如批改那些本科生的作业，监考之类的，貌似这些是硕士的事情，但是博士到底都做什么呢，必然也有一堆枯燥的事情吧。另外，批改作业这种的，机器学习能解决吗，是不是可以搞个自动批改各种各样作业的系统呢？<br>关于Python的机器学习，已经停滞一段时间了，最近实在太荒废。人生大部分时间就是这么荒废过去的。荒唐。<br>蒙特卡洛树搜索的算法也没有看，本来打算好好看完，写一篇文章的。<br>总之就是太无聊了，最近，工作上的事情，对我简直是折磨，眼见那些破事一点点吞噬我的时间。<br>不废话了，沉下心来，好好学习，然后教机器学习，让机器学会做起简单的事情来。这真是一件有意义的事情。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/19/做个机器学习Python调包侠/" itemprop="url">
                  做个机器学习Python调包侠
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-19T14:39:55+08:00" content="May 19 2017">
              May 19 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/14/Recommender-Systems-推荐系统/" itemprop="url">
                  Recommender Systems 推荐系统
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-14T13:06:31+08:00" content="May 14 2017">
              May 14 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>推荐系统是机器学习的一个重要应用。很多团队都致力于建立更好的推荐系统，比如亚马逊、Netflix、eBay或者苹果公司的iTunes Genius做的事情，有很多网站或者系统试图向用户推荐新产品，比如亚马逊向你推荐新书，Netflix想你推荐新电影，诸如此类。而这些推进系统可能会看看你以前购买过什么书，或者以前你给哪些电影进行过评分，这些系统贡献了现今亚马逊收入的相当大一部分，而Netflix，他们向用户推荐的电影占了用户观看电影的大部分。推荐系统，其表现的一些改进就能带来显著且即刻产生的影响，这些影响关系到许多公司的最终业绩。<br>在机器学习中，特征量是重要的，选择的特征对学习算法的表现有很大影响。在机器学习领域，有这么一个宏大的想法，就是对于一些问题，可能不是所有问题，但是对于一些问题而言，存在一些算法，能试图自动地学习到一组优良的特征量，而不用人为动手设计或者手动编写特征。<br>有些情况，或许能够采用一种算法，来学习到使用什么特征量，而推荐系统，就是这种情形的一个例子，当然还有其他很多例子。通过学习推荐系统，我们能够对这种学习特征量的想法有一点了解，我们至少可以通过这个例子，来了解机器学习中的这种big idea。</p>
<h1 id="推荐系统算法">推荐系统算法</h1>
<h3 id="推荐系统在做什么">推荐系统在做什么</h3>
<p>如下图所示，给定这些r(i,j)与y(i,j)数据，其中r(i,j)表示用户j给电影i是否打分，y(i,j)表示用户j给电影i所打的分数，然后浏览全部数据，关注所有没有电影评分的地方，并试图预测这些带问号的地方，应该是什么数值。如下图，前三部电影可能是浪漫爱情电影，后两部是动作片，然后Alice和Bob看起来喜欢爱情片，我们预测Alice也许会给电影Cute puppies of love打分5，Bob给电影Romance forever打分4.5，推荐系统就是通过填充这些数值，然后推测用户更喜欢什么电影，并进行相应推荐。有一些电影，还有一些用户，用户给一些电影进行了评价打分，推荐系统所做的事情就是，通过这些评分，预测用户会怎样给还没看过的电影打分。<br><img src="/media/14947442269252.jpg"></p>
<h2 id="基于内容的推荐-cotent-based-recommendations">基于内容的推荐 Cotent Based Recommendations</h2>
<p>注意看<span class="math">\(x^{(i)}\)</span>，它表示某部电影是浪漫和动作片程度的多少。算法的目的是学到每个用户喜爱浪漫电影和动作电影程度的变量<span class="math">\(θ^{(j)}\)</span>，然后使用<span class="math">\((θ^{(j)})^Tx^{(i)}\)</span>就可以预测第j个用户可能给第i部电影所打的分数。<br><img src="/media/14947615593109.jpg"></p>
<p>学习每个<span class="math">\(θ^{(j)}\)</span>的值是一个基本的线性回归问题，算法的目的是为了学习到<span class="math">\(θ^{(j)}\)</span>使得下面的损失函数的值最小。但是这个又和我们以前所学习的线性回归问题不同，因为在这里，每个用户的<span class="math">\(θ^{(j)}\)</span>值是不同的，每个用户都有单独属于自己的<span class="math">\(θ^{(j)}\)</span>值，这就使得个性化推荐成为现实。我们可以把对每个观众打分的预测，当成一个独立的线性回归问题。<br><img src="/media/14947649924645.jpg"></p>
<p>优化目标就是图上的公式，对于每一个用户，我们都进行计算。<span class="math">\(θ^{(n_u)}\)</span>表示第n个用户的θ值。<br><img src="/media/14947714065234.jpg"></p>
<p>梯度下降过程如下。 <img src="/media/14947721242572.jpg"> 以上，就是运用线性回归的变体，来预测不同用户对不同电影的评分值。这种做法，叫做『基于内容的推荐』，因为我们具有电影的特征量<span class="math">\(x^{(i)}\)</span>，来表示电影内容的属性，比如电影爱情的成分是多少，动作的成分是多少。但是实际上，我们很少知道所有电影的特征，或者我们要卖的产品有什么特征，所以接下来，我们介绍不具有这些内容特征时，该如何进行推荐。</p>
<h2 id="协同过滤-collaborative-filtering">协同过滤 Collaborative Filtering</h2>
<p>协同过滤，可以自行学习所要使用的特征。<br>如下图所示，假如用户告诉了他们的偏好，那么我们可以知道每个<span class="math">\(θ^{(j)}\)</span>的值，来学习到<span class="math">\(x^{(i)}\)</span>的值。<br><img src="/media/14947754778241.jpg"></p>
<p>根据用户的偏好，学习电影的特征指数，优化目标如下图所示：<br><img src="/media/14947761941215.jpg"></p>
<p>如下图所示，把二者合并起来，就是系统过滤了。根据每个用户对多部电影的评分，以及每部电影由不同用户的评分，及可以反复进行这样的过程，来估计出θ和x。协同过滤算法指的是，当你执行这个算法时，你通过一大堆用得到数据，这些数据实际上在高效地进行了协同合作，来得到每个人对电影的评分值。只要用户对某几部电影进行评分，每个用户就都在帮助算法，更好的学习出特征，这些特征又可以被系统运用，为其他人做出更准确的电影预测。协同的另一层意思是，每位用户，都在帮助系统，学习出更好的特征，这就是协同过滤。<br><img src="/media/14947774794944.jpg"></p>
<h2 id="协同过滤算法-collaborative-filtering-algorithm">协同过滤算法 Collaborative Filtering Algorithm</h2>
<p>把上面讲的两个损失函数合并起来，就是我们希望优化的新的代价函数，新的代价函数是同时关于θ和x的函数，二者同时更新，最终是的J最小。<br><img src="/media/14947796300697.jpg"></p>
<p>协同算法步骤：</p>
<ul>
<li><p>首先将会把θ和x初始化为小的随机值，这有点儿像神经网络训练，所有神经网络的权值参数，我们也是用小的随机数值来初始化的。协同过滤的初始化和神经网络初始化一样，也需要打破对称symmetry breaking. <img src="/media/14947804720110.jpg"></p></li>
<li><p>使用梯度下降，或者其他的高级优化算法，把代价函数最小化，如果求导，梯度下降更新写出来的结果如图中第2项所示。通过梯度下降，我们同时更新θ和x的值。<br> <img src="/media/14947802773743.jpg"></p></li>
<li><p>根据以上求出的θ和x的值，我们就可以预测每一个用户了。</p></li>
</ul>
<p>以上就是协同过滤算法的具体过程。通过协同过滤算法，可以同时学习几乎所有电影的特征x，和所有用户的参数θ，然后有很大机会，能对不同用户会如何评价他们尚未评分的电影，做出相当准确的预测。</p>
<h3 id="低秩矩阵分解-low-rank-matrix-factorization">低秩矩阵分解 Low Rank Matrix Factorization</h3>
<p>通过向量化的计算，来对所有的用户和所有的电影，进行评分计算。 <img src="/media/14947819692395.jpg"></p>
<h3 id="向用户做出相关推荐">向用户做出相关推荐</h3>
<p>根据协同过滤算法，当给出一件产品时，可以找到与之相关的其它产品，再例如，一位用户最近看上一件产品，看有没有其它相关的产品，你可以推荐给他。<br>怎么给用户进行相关推荐呢？请看下图。用户曾给电影i打过高分，我们根据算法得到<span class="math">\(x^{(i)}\)</span>，电影i的特征指数，那么我们就找到距离<span class="math">\(x^{(i)}\)</span>最近的几个电影，推荐给用户。<span class="math">\(||x^{(i)}-x^{(j)}||\)</span>表示的是<span class="math">\(x^{(i)}与x^{(j)}\)</span>的欧式距离。<br><img src="/media/14947831732793.jpg"></p>
<h3 id="预处理步骤-均值归一化">预处理步骤-均值归一化</h3>
<p>求出每一个电影的平均分μ，然后每个打分减去该电影的平均分，每一行的总和其实还是0.我们这样做均值归一化了。当我们在预测具体某个用户对某个电影评分的时候，还是要把平均分μ加上的。对于某个用户从来没评论过任何电影，那么也默认他的评分是平均分水平。 <img src="/media/14947848429032.jpg"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/14/Dimensionality-Reduction-维度约减/" itemprop="url">
                  Dimensionality Reduction 主成分分析PCA
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-14T00:30:13+08:00" content="May 14 2017">
              May 14 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>使用Dimensionality Reduction-维数约减，一般是为了数据压缩，减少内存和硬盘空间存储，并且提高算法速度。 在实际应用中，我们会得到很多特征向量，有的特征向量之间是有关系的，比如预测房价的时候，得到的房子长度inches英寸和cm厘米，它们两个是线性相关的，所以有一个是冗余的，就可以去掉一个。又如下图所示，数据集中在一个平面Z附近，所以就可以把3D的数据降维成2D。 <img src="/media/14945865066705.jpg"></p>
<h1 id="主成分提取principal-component-analysis-problem-formulation">主成分提取(Principal Component Analysis problem formulation)</h1>
<p>对于降维问题，目前最流行的，最常用的算法就是PCA主成分分析法。 PCA做的就是，寻找一条直线，或者面，或者诸如此类，比起特征向量低维的空间，对数据进行投影，并且最小化投影距离，也就是数据点和投影后的点之间的距离。另外，在寻找vectors时，对于所有的数据集和特征向量，都是同等对待的。一般用投影的数据，当做被降维的数据使用。 <img src="/media/14946121526774.jpg"></p>
<p>关于上图的左边，注意区别PCA和线性回归，两个截然不同的概念。 <img src="/media/14946125078718.jpg"></p>
<h3 id="数据预处理-data-preprocessing">数据预处理 Data preprocessing</h3>
<p>在使用PCA之前，我们通常会有一个<strong>数据预处理</strong>的过程，就是对数据进行均值归一化。 <img src="/media/14946138068434.jpg"></p>
<h3 id="pac算法-principal-component-analysispca-algorithm">PAC算法 Principal Component Analysis(PCA) algorithm</h3>
<p>首先要做的是计算出∑协方差矩阵。<span class="math">\(∑=\frac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</span>,∑(Sigma)是个n×n的矩阵，因为<span class="math">\(x^{(i)}\)</span>是个n×1的向量，<span class="math">\((x^{(i)})^T\)</span>是个1×n的向量，两个向量相乘就是n×n的矩阵了。另外，协方差均值∑(Sigma)总满足一个数学性质，称为对称正定(symmetric positive definite)。这块线性代数的知识不懂，不过不必介意，反正就是这么求的，代码也不长。记得线性代数课中曾经讲到如何求特征向量什么的，现在用到了吧，忘了吧？呵呵^_^<br>svd(singular value decomposition)表示奇异值分解，svd(matlab奇异值分界的库函数,eig(Sigma)也有同样功能)将输出三个矩阵，分别是U、S、V，真正需要的是U矩阵，U矩阵也是个n×n矩阵。如果我们想将数据降维到k-dimensions的话，我们只需提取前k列向量，也就是用来投影数据的k个方向。 <img src="/media/14946971878921.png"> <span class="math">\(U_{reduce}\)</span>指U取前k列得到的n×k矩阵，X是训练集中的样本或者交叉训练集中的样本或者是测试集样本，然后Z矩阵的表达，是使用Z=<span class="math">\(U_{reduce}^TX\)</span>，也就是k×n矩阵和n×1矩阵相乘，所以Z就是k维的向量。 <img src="/media/14946981115876.jpg"></p>
<p>总结一下PAC的全过程，如下图所示。 <img src="/media/14946983615287.jpg"></p>
<h1 id="还原压缩数据">还原压缩数据</h1>
<p>压缩数据时，我们也许会把一千维的数据压缩到只有一百个维度，既然我们可以用算法如此压缩数据，那么也应该有办法，可以从压缩过的数据，近似地回到原始高维度的数据。假设有一个已经被压缩过的<span class="math">\(z^{(i)}\)</span>，它有100维度，怎样使它回到其最初的表示<span class="math">\(x^{(i)}\)</span>，也就是压缩前的1000维的数据呢？</p>
<p>呵呵，这一块先不写了。以后再补上。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/05/Anomaly-Detection-反常检测/" itemprop="url">
                  Anomaly Detection 反常检测
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-05T23:03:49+08:00" content="May 5 2017">
              May 5 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>异常检测最常用的应用是欺诈检测。系统的用户都在从事不同的活动，可以对不同的用户活动计算特征变量，然后可以建立一个模型，用来表示用户表现出各种行为的可能性，也就是用户行为对应的特征向量出现的频率。</p>
<h4 id="异常检测例子">异常检测例子</h4>
<ul>
<li><p>某个用户在网站上行为的特征变量，也许<span class="math">\(x_1\)</span>是用户登陆的频率，<span class="math">\(x_2\)</span>是用户访问某个页面的次数或者交易次数，<span class="math">\(x_3\)</span>是用户在论坛上发帖的次数，<span class="math">\(x_4\)</span>是用户的打字次数，有些网站是可以记录用户每秒打了多少个字母的，因此可以根据这些数据建一个模型p(x)，可以用它来发现网站上行为奇怪的用户，只需要看哪些用户的p(x)概率小于ε，接下来，你拿来这些用户的档案，做进一步的筛选，或者要求这些用户，验证他们的身份，从而让网站可防御异常行为或者欺诈行为。这样的过程，可以找到行为不正常的用户，而不只是有欺诈行为的用户，也不只是那些被盗号的用户或者有行为比较搞笑的用户，而是行为不寻常的用户。这就是许多在线购物网站，常用来识别异常用户的技术。这些用户行为奇怪，可能表示他们有欺诈行为，或者是被盗号。</p></li>
<li><p>工业生产领域，发现异常的产品，然后要求进一步细查这些产品的质量。比如飞机引擎异常检测。</p></li>
<li><p>数据中心的计算机监控。假如，管理一个计算机集群，或者一个数据中心，其中有许多计算机，那么我们可以为每台计算机计算特征变量，也许某些特征衡量计算机的内存消耗，或者硬盘访问量，CPU负载，或者更加复杂的特征，比如CPU负载于网络流量的比值。建立p(x)模型，找到异常情况不正常工作的计算机，也许它即将停机，因此可以有求系统管理员查看其工作状况。目前这种技术实际正在被各大数据中心使用，用来检测大量计算机可能发生的异常。 <img src="/media/14948382044224.jpg"></p></li>
</ul>
<h3 id="高斯分布-gaussian-normal-distribution">高斯分布 Gaussian (Normal) Distribution</h3>
<p>高斯分布也称为正态分布。μ是均值，代表钟形曲线的对称轴，σ标准差确定了高斯分布概率密度函数的宽度，<span class="math">\(σ^2\)</span>表示方差。 <img src="/media/14948391530737.jpg"></p>
<p>高斯分布的例子，曲线的阴影部分面积是1，这是概率密度函数的特性。注意看高度和宽度和对称轴。 <img src="/media/14948397775893.jpg"> 参数估计问题。其实这里也是极大似然估计。 注意看下图中对μ和<span class="math">\(σ^2\)</span>的确定。 <img src="/media/14948406033181.jpg"></p>
<h3 id="异常检测算法">异常检测算法</h3>
<p>这里运用高斯分布开发异常检测算法。步骤如图所示：</p>
<ol style="list-style-type: decimal">
<li>首先选择特征，找出可能可以看出他们的反常和欺诈行为的特征<span class="math">\(x_i\)</span>，这个特征值或者过大或者过小，尽可能找出那些能够描述数据相关的属性特征；</li>
<li>计算均值和方差；</li>
<li>计算p(x)，如果p(x)&lt;ε,则定为异常。 <img src="/media/14948421441058.jpg"> 如下图，分布在桃红色区域的样本，就是异常的样本。<br><img src="/media/14948440235452.jpg"></li>
</ol>
<h3 id="分配样本数据">分配样本数据</h3>
<p>如果有10000个好的正常的引擎，20个异常的引擎，那么会分配6000个好的引擎作为训练集，2000好的引擎和10个不正常的引擎作为交叉验证集，剩下的2000好的引擎和10个不正常的引擎作为测试集。一般是6:2:2作为Training set:CV:Test<br><img src="/media/14948623383598.jpg"></p>
<h3 id="如何评估异常检测系统">如何评估异常检测系统</h3>
<p>对于异常检测系统，分类准确率不是一个好的评估度量方式。因为如果出现数据非常偏斜，异常数据非常少，我们直接预测所有样本都是正常的，那么这时候准确率还是很高的，但是显然这不是一个好的做法。那么用什么评价度量好呢？取而代之的是，我们应该算出真阳性、假阳性、假阴性和真阴性的比率。我们也可以算出查准率与召回率的比值；或者算出<span class="math">\(F_1-\)</span>积分，通过一个很简单的数字来表现出查准和召回的大小。通过这些方法，就可以较公平地评价异常检测算法在交叉验证和测试集样本中的表现。<br> 那么如何确定阈值ε呢？可以试试多个不同的ε取值，然后选出一个使得<span class="math">\(F_1-Score\)</span>值最大的那个ε，也就是在交叉验证集中表现最好的ε。<br><img src="/media/14948651051920.jpg"></p>
<h3 id="anomaly-detection-vs.supervised-learning">Anomaly detection vs. supervised learning</h3>
<p>那么我们什么时候应该用异常检测，什么时候用监督学习分类算法呢？在异常检测算法中，我们只有一小撮正(异常)样本，因此算法不可能从这些正样本中学出太多东西，因此取而代之的是我们使用一组大量的负(正常)样本，这样样本就能学到更多，或者能从大量的负样本中学出p(x)模型，另外会预留一小部分正样本来评价算法，既用于交叉验证集，也用于测试集。而对于垃圾邮件的样本，我们能得到绝大多数不同类型的垃圾邮件，因为我们有大量的垃圾邮件样本的集合，这就是为什么我们通常把垃圾邮件问题看作是监督学习问题的原因。<br>请看下图： <img src="/media/14949132950585.jpg"></p>
<h2 id="关于特征变量的选择">关于特征变量的选择</h2>
<p>对于异常检测算法效率影响最大的因素之一是使用什么特征变量，选择什么特征变量来输入异常检测算法。</p>
<h4 id="处理不符合高斯分布的特征">处理不符合高斯分布的特征</h4>
<p>在MATLAB中绘制直方图的函数是hist，如果画出来的柱状图近似像高斯分布，那么就可以很放心地把它们送入学习算法了，但如果画出来的图像如下图中的左下角图像，分布很不对称，峰值非常偏向一边，类似高斯分布图像的右半边，通常直接这样使用数据，算法也会运行很好，但是如果使用一些方法使得数据更像高斯分布的话，算法会工作的更好。对于左下角图像，就可以进行取对数log(x)的转换，结果就很像高斯分布了。<br><img src="/media/14949358396911.jpg"></p>
<h4 id="如何得到异常检测的特征变量">如何得到异常检测的特征变量</h4>
<p>通过误差分析步骤。如果最终表现不理想，对于很多正样本和负样本都有很大的p(x)的值，那么我们最好还是多引入其它的特征，以便于更好的区分出正样本和负样本。 <img src="/media/14949381932356.jpg"></p>
<h2 id="多元高斯分布-multivariate-gaussian-distribution">多元高斯分布 Multivariate Gaussian Distribution</h2>
<p><img src="/media/14949427576171.jpg"> 多元高斯分布模型和原始高斯分布模型的关系： <img src="/media/14949431790982.jpg"></p>
<h3 id="高斯分布模型-vs-多元高斯分布">高斯分布模型 VS 多元高斯分布</h3>
<p>原始的高斯分布模型使用的更多，但是多元高斯分布可以自动捕获不同特征变量之间的相关性。但是原始模型也有其他很重要的优势，一个很重要的优势，就是运算量很小，如果n的值非常大，也就是说特征变量很多的情况，即使n=100,000，原始模型都可以很好的运行。但是对于多元模型，计算∑的逆矩阵，∑是个n*n的矩阵，也就是100,000乘100,000的矩阵，那么这个计算量会非常大，所以多元模型不是n很大的情况。另外，对于原始的模型，训练集很小也就是m相对小的情况下，它也能运行的还可以，但是多元模型必须要求m&gt;n，样本的数量要大于特征变量的数量，因为如果m小于等于n，那么∑矩阵是不可逆的，是奇异矩阵，这种情况下，不能使用多元模型。一个合理的经验法则，m大于等于n的10倍的时候，再使用高斯模型。<br> 一般情况下，原始模型比较常用，如果需要捕捉特征变量之间的相关性，一般人都会手动增加这样的额外特征变量，来捕捉特定的不正常的值的组合，但是在训练集m很大，n不太大的情况，那么多元高斯模型是值得考虑的，或许可以运行得更好，还可以帮忙省去为了捕捉不正常的特征值组合而手动建立额外特征变量所花费的时间。<br> 另外，如果协方差矩阵∑是不可逆的，奇异的，那么一般只有两种情况:1.没有满足m&gt;n；2.有冗余的特征变量，就是不小心把一个特征变量复制了两份，或者是高度冗余的特征变量，如<span class="math">\(x_3=x_4+x_5\)</span>，那么<span class="math">\(x_3\)</span>就不含有额外的信息，也是冗余。所谓冗余的特征变量，也就是线性相关的特征变量。 <img src="/media/14950095213212.jpg"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/05/无监督式学习/" itemprop="url">
                  无监督式学习之K-means
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-05T23:03:18+08:00" content="May 5 2017">
              May 5 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>无监督学习算法，就是从未标记的数据中进行学习。 <img src="/media/14944286743216.jpg"></p>
<ul>
<li>组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。</li>
<li>社交网络里面也有，区分哪些用户之间是很亲密，哪些用户之间仅仅是认识。</li>
<li>根据商业系统中的数据，细分市场，然后把客户再分到不同的细分市场中。</li>
<li>分析星云的状态。</li>
</ul>
<h1 id="聚类问题clustering-k-means-algorithm">聚类问题(Clustering) K-means Algorithm</h1>
<p>有一堆无标签的数据，K-means能够自动的把这些数据分成有紧密关系的子集或者簇，是现在最为广泛运用的聚类方法。</p>
<ul>
<li><p>首先，要确定分类需要几个簇，也就是K的大小。 <img src="/media/14944822633878.jpg"></p></li>
<li><p>随机初始化K个簇中心，分别是<span class="math">\(μ_1,μ_2,…,μ_K ∈ R^n\)</span>(n是特征向量的纬度)。 <img src="/media/14944860345126.jpg"></p></li>
<li>不断重复一下步骤：
<ol style="list-style-type: decimal">
<li>找出数据<span class="math">\(x^{(i)}离哪个簇中心最近，用c^{(i)}记录簇中心的索引，c^{(i)}= min_k||x^{(i)}-μ^{(k)}||^2\)</span>。m是训练数据的总个数。<span class="math">\(c^{(i)}\)</span> = index of cluster(1,2,…,k) to which example <span class="math">\(x^{(i)}\)</span>is currently assigned。这一步就是把m个<span class="math">\(x^{(i)}\)</span>划分给各自所属的聚类中心。</li>
<li>根据当前分开的簇，再重新计算每个簇的簇中心。例如 <span class="math">\(c^{(1)}=2,c^{(5)}=2,c^{(6)}=2,c^{(10)}=2\)</span>，那么<span class="math">\(μ_2=\frac{1}{4}[x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)}]\)</span>。如果某个簇里面没有分配任何1个点，那么就把这个簇中心移除掉，或者重新随机找一个聚类中心，但是直接移除是更为常见的方法。</li>
<li>我们优化的目标是找到最好的<span class="math">\(μ_1,…μ_K\)</span>,使得<span class="math">\(J(c^{(1)},…,c^{(m)},μ_1,…μ_K)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-μ_{c^{(i)}}||^2\)</span>取得最小值，同时J是一直收敛的。 <img src="/media/14944823083506.jpg"></li>
</ol></li>
</ul>
<h4 id="局部最优解">局部最优解</h4>
<p>另外，局部最优解是可能产生的，有时候2个聚类中心会在一起，卡在了局部最优。如下图所示，右下角的2个函数图像，表示了2种不同的局部最优。 <img src="/media/14944905514428.jpg"> #### 解决局部最优 为了让K-means方法找到较好的局部最优解或者全局最优解，我们可以尝试多次随机的初始化来保证我们最终能得到一个足够好的结果，而不仅仅初始化一次K-means，就希望得到很好的结果。特别是K处于2到10之间的话，聚类数相对较小的体系里，多次随机初始化效果会非常好，会有较大的影响。但是如果K的值几百上千的话，很可能初次随机初始化就得到很好的结果，多次随机初始化也许会得到稍微好一点的结果，但是不会好太多。 <img src="/media/14944862061717.jpg"></p>
<p>看下图的右边，是一个市场细分的例子，根据数据，将市场分为3个部分，然后区别对待三类不同的顾客群体，更好的适应他们不同的需求，为大中小号3种聚类的用户，设计更合身的S,M,L尺码的衣服。 <img src="/media/14944871641347.jpg"></p>
<h4 id="关于聚类数目k的选取">关于聚类数目K的选取</h4>
<p>关于K应该取什么值，这个问题没有非常标准的解答，或者能自动解决他的方法。目前用来决定聚类数目最常用的方法仍然是通过看可视化的图，或者看聚类算法的输出结果，或者通过其他一切东西来手动选择聚类的数目。嘿嘿，就通过洞察力决定呗。 一般情况下使用肘部法则，但是不用期待表现很好。更多情况下，选择聚类数目的更好方法是，去问一下运行K-means是为了什么目的，然后想想聚类的数目是多少，才适合K-means聚类的后续目的。</p>
<ol style="list-style-type: decimal">
<li><p>肘部法则(Elbow method) 在选择聚类数目K的时候，我们可以使用下肘部方法，如果图像如下图左边的曲线，折点非常明显，那么选择肘点K是个很好的方式，但是如果图像如下图右边所示，那么选取哪个数目，则看起来非常困难。肘部方式，我们可以使用，但是很多时候，往往得不到左图那种有个明显的折点的情况，所以此方式值得尝试，但是我们也不要太期待。 <img src="/media/14944940159311.jpg"></p></li>
<li><p>看不同的聚类数量能为后续下游的目的提供多好的结果。从生意的角度来选择聚类数量，如下图，生产5种大小的T恤可以更加适合顾客，但是3种T恤的话，公司也可以降低成本，更便宜的卖给更多的顾客，因此T恤销售业务的观点，可能会提供一个决定采用3个类还是5个类的方法。 <img src="/media/14944958853442.jpg"></p></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/05/Support-Vector-Machines/" itemprop="url">
                  Support Vector Machines
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-05T23:02:21+08:00" content="May 5 2017">
              May 5 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于支持向量机，这里有<a href="https://www.zhihu.com/question/21094489" target="_blank" rel="external">一个比较有意思的讲解</a>，还有这里<a href="https://www.youtube.com/watch?v=3liCbRZPrZA" target="_blank" rel="external">一个小视频</a>。<br>SVM是数据挖掘里面很重要的算法，我们有时会把SVM看做是一个大间距分类器Large Magin Intuition。SVM Decision Boundary具有鲁棒性，因为它努力用一个最大间距，来分离样本。<br>有个小的知识点，向量内积以及<a href="http://www.cnblogs.com/vive/p/4563803.html" target="_blank" rel="external">证明</a></p>
<h1 id="线性核函数">线性核函数</h1>
<p>SVM线性核函数，就是SVM不使用核函数。其实是在逻辑回归的基础上进行稍微变化而成的。<br><img src="/media/14943143749438.jpg"></p>
<p>使用SVM软件包来确定参数θ的时候，我们需要指定常数C，C值如果太大，那么就会过拟合，效果不好。如以下示图，注意看左下角那个红叉叉，如果C太大的时候，决策边界就会是斜着的那条很线，这样并不好: <img src="/media/14943142227688.jpg"></p>
<h1 id="kernels">Kernels</h1>
<p>构造非线性复杂的分类器，我们用“Kernels函数”来达到此目的。<br>先来看一张图，如果决策分界是曲线的时候，那么就会有多项式特征变量的出现，如果不进行变化，就直接去求解，那么运算量是非常大的，有太多的高阶项需要被计算，所以我们需要通过其他方式来构造特征变量，来嵌入到假设函数中：<br><img src="/media/14943156955239.jpg"></p>
<p>Kernels核函数有很多，我们用的最多的是高斯核函数(Gaussian kernel)，$ f_i=exp{ }$ 其中 <span class="math">\(l^(i) =x^(i) ，x^(i)\)</span>表示m个训练数据的第i个样本，高斯核函数描述了某个样本和其他样本的距离程度。这个函数类似高斯分布，因此称为高斯核函数。 <img src="/media/14943147936462.jpg"></p>
<p>看下面这张图，可以更加理解高斯核函数的作用，注意看图中的红色大圈，如果<span class="math">\(f_1≈1,f_2≈0,f_3≈0\)</span>，那么证明这个点离<span class="math">\(l^(1)\)</span>很近，y=1。图中训练出的θ值，表明了红圈内是预测y=1，红圈外是y=0，即离点<span class="math">\(l^(1),l^(2)\)</span>都很远。这就是我们如何通过标记点以及核函数，来训练出非常复杂的非线性决策边界的方法。</p>
<div class="figure">
<img src="/media/14943222658500.jpg">

</div>
<p>高斯核函数的损失函数如下图所示，不过这些我们自己平时直接使用优化好的SVM包，并不需要关心这些细节，这些都是内部优化好的，不需要来定义。 <img src="/media/14943195312828.jpg"></p>
<p>在使用高斯核函数的时候，我们需要选择<span class="math">\(σ^2\)</span>，如果<span class="math">\(σ^2\)</span>偏大我们就会得到一个较大误差较低方差的分类器，高斯核函数返回的是(0,1)区间的实数。另外，在使用核函数的时候，进行数据归一化也是很必要的，因为如果不进行归一化，那么值比较大的特征向量会占据很大的地位，弱化其他特征向量的影响，这是不公平的。 <img src="/media/14943198501799.jpg"></p>
<p>支持向量机算法的核函数必须满足莫塞尔定理“Mercer’s Theorem”。对于其他的核函数，此处不做过多介绍。这里有个<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988406.html" target="_blank" rel="external">核函数的介绍</a></p>
<h1 id="如何选择使用何种算法呢">如何选择使用何种算法呢</h1>
<ol style="list-style-type: decimal">
<li>如果特征向量的个数很大,而训练集很小时，我们通常使用逻辑回归，或者使用SVM线性核函数。因为没有足够的数据来拟合非常复杂的非线性函数。</li>
<li>如果特征向量很小，而训练数据量是中等大小，那么核函数表现就可以很好。<br></li>
<li>如果如果特征向量很小，而训练数据量巨大，那么高斯核函数就会运行很慢，这种情况下，可以尝试手动建立更多的特征变量，然后使用逻辑回归或者SVM线性核函数。</li>
</ol>
<div class="figure">
<img src="/media/14943200907676.jpg">

</div>
<p>以上，第2种，例如，特征向量为1000左右，训练集是10,000的情况下，高斯函数的支持向量机会表现的非常突出。另外第1种和第3种情况，其实使用逻辑回归和SVM线性核函数效果都差不多。<br>SVM是凸优化的，所以局部最优，就是全局最优了。</p>
<p>在使用SVM软件包的时候，需要我们自己按照需求，配置以下参数：<br><img src="/media/14943202565702.jpg"></p>
<p>多种分类时：<br><img src="/media/14943205457069.jpg"></p>
<p>其实呢这课，我没怎么看懂，向量内积怎么使用我也不知道，SVM算法怎么使用倒是有个模糊的印象，但是具体内部的算法原理，没搞太明白，不知道最后的决策边界怎么产生的最大边距。这篇SVM我写的好吃力啊，重新刷斯坦福视频的时候居然看不懂了，真是……，不过还好最后看的自己觉得明白了，就先记录下来。吃力。</p>
<p>练习题中，垃圾邮件分类器，采用SVM线性核函数。通过训练集，训练出字典表中每个单词的权重(就是θ参数)。进行垃圾邮件分类时，是需要对邮件内容进行预处理的，比如网址啊，钱啊，之类的，都会用一个单词或者符号代替，出现的哪些单词(当然最后都向量化数字处理了，类似于x = [ 0 0 0 0 1 0 0 0 … 0 0 0 0 1 … 0 0 0 1 0 ..]，x的长度是字典长度，数据1表示出现了这个单词)，最后带入我们的式子<span class="math">\(θ^TX\)</span>里面，可进行是否垃圾邮件的预测，如果大于等于0，就是垃圾邮件，小于0，是正常邮件。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/04/神经网络初探/" itemprop="url">
                  神经网络初探
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-04T14:14:20+08:00" content="May 4 2017">
              May 4 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>神经网络是当今最强大的学习算法之一，自动驾驶对周围景物的识别，就用到了神经网络。<br>我们以下讲述，神经网络在分类问题中的应用，在给定训练集下，为神经网络拟合参数的学习方法。<br><span class="math">\(\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}\)</span><br>在整个神经网络算法过程中，我们的最终目标是找到最佳的Θ矩阵中的权值。<br><img src="/media/14943440970717.jpg"></p>
<h1 id="神经网络的损失函数">神经网络的损失函数</h1>
<p>其中逻辑回归的损失函数为：<br><span class="math">\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\)</span><br>神经网络的损失函数在逻辑回归的基础上，稍微复杂了一点儿：<br><span class="math">\(\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\)</span></p>
<ul>
<li>L = 神经网络的层数。<br></li>
<li><span class="math">\(s_l\)</span> = 在第l层，神经单元的个数(不包括偏移量单元)</li>
<li>K = 神经网络输出层的单元个数。就是分类的类别总数。数字识别的话就是10.</li>
</ul>
<p>在Θ矩阵中，列代表当前层的单元个数，包括偏移单元；行代表下一层的神经元个数，不包含偏移单元。</p>
<p>注意：</p>
<ul>
<li>双和嵌套，只是简单的把输出层每个分类的逻辑回归损失加在一起了。m代表训练集的个数。</li>
<li>三个和嵌套的，是把神经网络里面，所有Θ矩阵的元素平方加在一起了。</li>
<li>i在三和嵌套中，代表的不是训练样本个数，而是代表当前层的神经元个数。</li>
</ul>
<h1 id="反向传播算法-backpropagation-algorithm">反向传播算法 Backpropagation Algorithm</h1>
<p>Back propagation的本质就是复合函数求导（following the chain rule）链式法则，本可以对这个网络里的每一个参数分别求偏导，但何苦呢，因为计算过程中的很多项都是重复的。为了不重复运算把把后层算好的导数传回前层，因为前层一定用得到。<br>关于反向传播算法，这里有个<a href="https://www.zhihu.com/question/27239198?rf=24827633" target="_blank" rel="external">浅显易懂的描述</a>。这里请看<a href="http://www.cnblogs.com/dengdan890730/p/5537451.html" target="_blank" rel="external">反向传播算法的推导过程</a>和<a href="http://blog.csdn.net/u014403897/article/details/46347351" target="_blank" rel="external">这里最后一层的求导</a>,这几个地方写的都不错。Andrew Ng老师的公开课对反向传播算法没有做过多的介绍，讲的也不够清晰。反向传播算法，就是为了计算损失函数的倒数。<br>先贴个图片留作记号，便于下面Δ计算的理解</p>
<div class="figure">
<img src="/media/14939758124978.jpg">

</div>
<h2 id="反向传播算法的使用过程">反向传播算法的使用过程：</h2>
<ol style="list-style-type: decimal">
<li>Set <span class="math">\(a^{(1)} := x^{(t)}\)</span></li>
<li><p>正向计算每层的每个节点的<span class="math">\(a^{(l)}\)</span>for l=2,3,…,L</p>
<div class="figure">
<img src="/media/14939747131225.jpg">

</div></li>
<li>使用<span class="math">\(y^{(t)}，计算出\delta^{(L)} = a^{(L)} - y^{(t)}\)</span>，最后输出层的错误率。</li>
<li><p>使用<span class="math">\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</span>，计算出<span class="math">\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</span>。<br> 其中$g’(z^{(l)}) = a^{(l)} .* (1 - a^{(l)})；_j^{(l)} =  cost(t)； $</p></li>
</ol>
<p><span class="math">\(cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li><span class="math">\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}，或者向量化一下，\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</span>(Δ的初始化为0)<br> 至此，我们更新的权值累加器矩阵的为new Δ matrix，其中<span class="math">\(\frac \partial {\partial \Theta_{ij}^{(l)}} J(\Theta)=D_{ij}^{(l)}\)</span>
<ul>
<li><p><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</span> if j≠0.</p></li>
<li><p><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</span> If j=0</p></li>
</ul></li>
</ol>
<h3 id="梯度检验">梯度检验</h3>
<p>在神经网络中使用反向传播算法的时候，因为有很多细节，会导致各种各样小bug，即使J看起来每次都是下降的，但是，最终结果的误差却很大。因此梯度检验也是需要的过程，它减少这种错误的概率。在其他比较复杂的模型中使用梯度算法的时候，进行这种检查也是有意义的，这么做，将会对模型更加自信，确信模型是100%正确。<br><span class="math">\(\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}\)</span></p>
<p>在确定算法无误后，真正执行学习算法前，一定要关掉梯度检验，否则会很慢的哟。</p>
<h3 id="θ的初始化">Θ的初始化</h3>
<p>Θ初始化在逻辑回归的时候是可以为0的，但是在神经网络中不可以全部为0的。因为都为0的时候，隐藏层的结果都是一样的，这样做，隐藏层就完全是冗余的，神经网络就没作用了，完全就是个逻辑回归。所以我们需要打破对称，随机初始化。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/02/线性回归和逻辑回归/" itemprop="url">
                  线性回归和逻辑回归
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-02T15:25:46+08:00" content="May 2 2017">
              May 2 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 <img src="/media/14938262502932.jpg"></p>
<h1 id="线性回归">线性回归</h1>
<p>线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如<span class="math">\(θ_{1}x_{1}+θ_2x_{2}+θ_3x_{3}=y\)</span>。都使用向量vector表示，<span class="math">\(θ^T =[θ_1,θ_2,θ_3],X^T= [x_1,x_2,x_3]\)</span>，假设函数为<span class="math">\(h_\theta(x)=θ^TX\)</span>。我们的目的是为了求出最恰当的θ，即<span class="math">\(θ_1,θ_2,θ_3\)</span>三者的值，使得损失函数<span class="math">\(J(θ)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})^2\)</span>的值最小。</p>
<h3 id="什么是梯度gradient">什么是梯度(Gradient)？</h3>
<p>简单来说，梯度就是损失函数关于权值θ的倒数，也就是关于θ的变化率(导数即变化率吧)。</p>
<h3 id="梯度下降算法gradient-descent">梯度下降算法(Gradient descent)</h3>
<p>我们得到的一般是个局部最优解。如果是个碗状或者叫做倒钟型的函数，那局部最优也是全局最优解。<br>The gradient descent algorithm is:<br>repeat until convergence:<br><span class="math">\(θ_j:=θ_j−α\frac∂{∂θ_j}J(θ_1,θ_2,θ_3)\)</span><br>where j=1,2,3represents the feature index number.<br><a href="https://d3c33hcgiwev3.cloudfront.net/_ec21cea314b2ac7d9e627706501b5baa_Lecture2.pdf?Expires=1493856000&amp;Signature=GSAbIM5AmG64UdFqdjCCWIn5hN~JZ8IheTVb6mliIEMdfhHgTrecl9toRVElelfaWZGY3vPkI33K7uOHicFc52EldArFxSunmfh4Mr4yjiEpZBbSF8-Tl9cWVTy2pAixsdpkmlL37Lku8VGax-LoenwwvR0i055g8j2wKJCGOrQ_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" target="_blank" rel="external">斯坦福课程线性回归教案拉到三分之二处看</a><br><a href="http://m.blog.csdn.net/article/details?id=51188876" target="_blank" rel="external">几种梯度下降方法</a><br><a href="http://blog.csdn.net/qq_34206952/article/details/54316285" target="_blank" rel="external">求导公式证明</a><br>α我们称之为learning rate步长，如果太大，那么损失函数不能收敛，如果太小，会收敛过慢。</p>
<p>另外需要注意特征缩放(feature scaling)，确保不同特征的取值在相近的范围内，这样梯度下降法就能更快的收敛。因为如果不做这个，J(θ)在θ1和θ2轴上的投影将会非常的瘦长，收敛的过程需要走很多步，才能到达最小值，做特征缩放，就是为了让偏移没那么严重，投影看起来更圆一些。进行特征缩放时，将特征的取值约束到−1≤x(i)≤1或者−0.5≤x(i)≤0.5。<br><span class="math">\(x_i:=\frac{x_i−μ_i}{s_i}\)</span> μi表示feature(i)的平均值，si是feature(i)值的范围(max - min),或者是标准差。<br>For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, <span class="math">\(xi:=\frac{price−1000}{1900}\)</span>.<br>说句题外话，这个特征缩放也很像将一般的正态分布转化成标准正态分布的过程:p</p>
<p>另外还有对应线性回归的，多项式回归(曲线回归)。有时候，线性函数无法合适的进行数据的拟合，这时候也许多项式进行拟合效果更好（曲线曲面等）。</p>
<h3 id="正规方程法normal-equation">正规方程法(Normal Equation)</h3>
<p>此乃最小二乘法，<span class="math">\(θ = (X^TX)^{-1}X^Ty\)</span>,不需要进行特征的缩放，不需要选择α，不需要迭代，但梯度下降法在很多特征变量的情况下，也能运行地相当好，即使有上百万的特征变量，通常很有效，时间复杂度为<span class="math">\(O(kn^2)\)</span>;而正规方程法，为了求解参数θ，需要求解<span class="math">\((X^TX)^{-1}\)</span>，其中<span class="math">\(X^TX\)</span>这是个n*n的矩阵，然后求逆矩阵的计算量，大概是矩阵纬度的三次方,<span class="math">\(O(n^3)\)</span>，因此当n很大时，这个方法会非常慢的。n&gt;1W就考虑梯度下降吧。只要特征数目不是很大，用正规方程法是非常好的，对于线性回归的模型。</p>
<table>
<thead>
<tr class="header">
<th align="left">Gradient Descent</th>
<th align="left">Normal Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Need to choose alpha</td>
<td align="left">No need to choose alpha</td>
</tr>
<tr class="even">
<td align="left">Needs many iterations<span class="Apple-tab-span" style="white-space:pre"></span></td>
<td align="left">No need to iterate</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(O(kn^2)\)</span></td>
<td align="left"><span class="math">\(O(n^3)\)</span>,need to calculate inverse of <span class="math">\(X^TX\)</span></td>
</tr>
<tr class="even">
<td align="left">Works well when n is large</td>
<td align="left">Slow if n is very large</td>
</tr>
</tbody>
</table>
<p>如果<span class="math">\(X^TX\)</span>是不可逆(不可逆的矩阵为奇异或退化矩阵)的时候呢？如果存在两个特征值有线性相关的时候，矩阵不可逆，或者特征向量太多的时候(e.g. m ≤ n)，矩阵或许也不可逆，这种情况就删除重复特征的一个，无须同时保留。一般情况下，算法库也会给我们一个伪逆矩阵或者是逆矩阵的解。<br>注:设A是数域上的一个n阶方阵，若在相同数域上存在另一个n阶矩阵B，使得：AB=BA=E。则我们称B是A的逆矩阵，而A则被称为可逆矩阵。</p>
<h1 id="逻辑回归logistic-regression">逻辑回归(Logistic Regression)</h1>
<p>sigmoid可以轻松处理0/1分类问题sigmoid函数，又称逻辑函数Logistic Function，用来进行归一化处理(促使<span class="math">\(y^{(i)}∈(0,1)\)</span>)。逻辑回归的主体还是回归操作：回归对象是sigmoid函数，它将输入映射为处于0到1之间的小数，得到这个小数之后人为将其解读成概率，然后根据事先设定的阈值进行分类。 <span class="math">\(\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}\)</span></p>
<p>sigmoid函数图像如下 <img src="/media/14938277151973.jpg"> <span class="math">\(h_θ(x)\)</span>代表了输出是1的概率为多少。如<span class="math">\(h_θ(x)\)</span>=0.7表示结果有70%的可能性是1。<br><span class="math">\(\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}\)</span><br>因为结果都是离散数值，0或者1，所以当结果大于等于0.5的时候，就认为是1，反之为0.<br><span class="math">\(\begin{align*}&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \newline\end{align*}\)</span><br>等价于<br><span class="math">\(\begin{align*}&amp; \theta^T x \geq 0 \Rightarrow y = 1 \newline&amp; \theta^T x &lt; 0 \Rightarrow y = 0 \newline\end{align*}\)</span><br>另外<span class="math">\(z = \theta^T x\)</span> 函数也不一定是线性的，例<span class="math">\(z =θ_0+θ_1x^2_1+θ_2x^2_2\)</span>也可以是个圆，或者其他的形状，只要拟合数据就OK。<br>逻辑回归的损失函数如下：<br><span class="math">\(\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*}\)</span><br>当y = 1的时候，关于J(θ)和$ h_θ(x)<span class="math">\(的函数\)</span>Cost(h_(x),y) = -(h_(x))$图像如下: <img src="/media/14938660514848.jpg"></p>
<p><span class="math">\(\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline \end{align*}\)</span></p>
<p>当y = 0的时候，关于J(θ)和$ h_θ(x)<span class="math">\(的函数\)</span>Cost(h_(x),y) = -(1-h_(x))$图像如下: <img src="/media/14938664062585.jpg"> <span class="math">\(\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y\newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*}\)</span></p>
<p>综上，得到损失函数为<span class="math">\(\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))\)</span><br>也写成<span class="math">\(J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]\)</span><br>向量化可表示成<br><span class="math">\(\begin{align*} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align*}\)</span><br>梯度下降:<br><span class="math">\(\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta)=\theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace\end{align*}\)</span><br>向量化表示为:<span class="math">\(\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})\)</span></p>
<h1 id="综述">综述</h1>
<p><img src="/media/14938678719388.jpg"> 第一个图像是欠拟合的(underfitting),第二个相对合适，第三个过拟合。 欠拟合，一般都是因为h函数太简单，或者是特征向量过少。而过度拟合问题(Overfitting)，往往是因为函数太复杂了，虽然很完美的拟合了输入数据，但是产生了许多不必要的曲线角度，对于往后的测试，其实是不利的。<br>解决过度拟合问题:</p>
<ol style="list-style-type: decimal">
<li>减少特征数：手动选择要保留的特征，或者使用模型选择算法</li>
<li>正则化：保留所有的特征，但是减轻θ的影响。 <span class="math">\(min_\theta\ \dfrac{1}{2m}\  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2\)</span><br>λ太大，就会欠拟合，因为θ会很小，甚至为0；如果λ太小，就会过拟合，因为θ会很大，正则化也失去了相应的作用。<br>线性回归的正则化的梯度下降和最小二乘法，看这里<a href="https://www.coursera.org/learn/machine-learning/supplement/pKAsc/regularized-linear-regression" target="_blank" rel="external">课程资料</a><br>逻辑回归正则化看这里<a href="https://www.coursera.org/learn/machine-learning/supplement/v51eg/regularized-logistic-regression" target="_blank" rel="external">课程资料</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/05/02/机器学习可以做什么/" itemprop="url">
                  关于机器学习
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-02T10:26:03+08:00" content="May 2 2017">
              May 2 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="先开个玩笑">先开个玩笑</h2>
<div class="figure">
<img src="/media/14938266335179.jpg">

</div>
<p>以上这个图片只是开个玩笑啦！哈哈哈<br>我目前做的事情呢，比较偏向于倒数第三张图片，目前，我是个观望的门外汉。<br>最后一整图片，scikit-learn封装了大部分机器学习算法的Python库。通常ML都在调用现有算法库，然后调参数。有一句话说，你以为我是科学家，其实我是调包侠。那么接下来呢，我不会告诉大家怎么来正确调包，我只是想分享一下关于机器学习，我了解到的都有什么知识。<br>另外，我讲解的所有东西，来源是吴恩达Andrew Ng教授的机器学习课程，或者网络知乎回答等。全部内容皆非原创。<br><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="external">Andrew Ng机器学习课程</a></p>
<h2 id="机器学习可以做什么">机器学习可以做什么</h2>
<ol style="list-style-type: decimal">
<li>数据挖掘。房价预测之类。<br></li>
<li>不能通过精确编程完成的应用。如手写数字的识别，自然语言解析，计算机视觉等。<br></li>
<li>推荐系统。亚马逊和Netflix的产品推荐。<br><a href="https://vimeo.com/57513893" target="_blank" rel="external">用机器学习做过什么有趣的事情</a></li>
</ol>
<h2 id="监督式学习supervised-learning">监督式学习–Supervised Learning</h2>
<p>监督式学习可以分为“回归regression”和“分类classification”问题。输入数据集a data set(features)，和每条数据对应的正确输出(label)，并且数据集和输出之间有一定的关系(map or function)。在回归问题中，我们尝试找到一个连续函数，可以最好的映射输入输出，然后通过这个continuous function来进行后续的预测。而分类问题也差不多如此，只不过，输出都是离散的值。</p>
<ol style="list-style-type: decimal">
<li>根据房屋大小，预测房子价格。这是个线性回归的问题。(这很像数据挖掘或者统计学的问题:p)<br></li>
<li>给一张人物照片，预测这个人的年龄。线性回归问题。<br></li>
<li>给定一个病人的肿瘤数据，预测肿瘤是良性的还是恶性的。分类问题。<br></li>
<li>手写数字的识别、区分是否垃圾邮件、是否金融欺诈。分类问题。</li>
</ol>
<h2 id="无监督式学习unsupervised-learning">无监督式学习–Unsupervised Learning</h2>
<p>在未加标签的数据中，试图找到隐藏的结构。</p>
<ol style="list-style-type: decimal">
<li>聚类。给定数据集，分类成一个个组合，如新闻的分类等。组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。社交网络里面也有，区分哪些用户之间是很亲密，哪些用户之间仅仅是认识。根据商业系统中的数据，细分市场，然后把客户再分到不同的细分市场中。(k-means)<br></li>
<li>信号分离。使用特征提取的技术降维。鸡尾酒会，说话声音和音乐声音分离。(PCA)</li>
</ol>
<h2 id="强化学习reinforcement-learning">强化学习–Reinforcement Learning</h2>
<p>强化学习是agent(与环境有交互的对象)自己去学习，并且得到反馈reward，以便RL进行迭代，学习到策略链，是个决策模型，而监督学习是跟着programmer的idea在收敛。<br><a href="https://zhuanlan.zhihu.com/p/25319023" target="_blank" rel="external">强化学习的知识整理</a></p>
<p>对于强化学习，我几乎没有任何了解。接下来的文章，我会讲解一下机器学习中，比较基础简单的算法。 :p</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel  sidebar-panel-active ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="niuoo" />
          <p class="site-author-name" itemprop="name">niuoo</p>
          <p class="site-description motion-element" itemprop="description">It is my Blog!</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">16</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">niuoo</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
