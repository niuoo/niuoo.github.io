<!doctype html>



  


<html class="theme-next mist use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="niuoo" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.0.1" />






<meta name="description" content="回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 
线性回归
线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如\(θ_{1}x_{1}+θ_2x_{2}+θ_3x_{3}=y\)。都使用向量vector表示，\(θ^T =[θ_1,θ_2,θ_3],X^T= [x_1,x_2,x_3]\)，假设函数为\(h_\theta(x)=θ^TX\)。我们的目的是为了求出最">
<meta property="og:type" content="article">
<meta property="og:title" content="线性回归和逻辑回归">
<meta property="og:url" content="http://niuoo.github.io/2017/05/02/线性回归和逻辑回归/index.html">
<meta property="og:site_name" content="niuoo">
<meta property="og:description" content="回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 
线性回归
线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如\(θ_{1}x_{1}+θ_2x_{2}+θ_3x_{3}=y\)。都使用向量vector表示，\(θ^T =[θ_1,θ_2,θ_3],X^T= [x_1,x_2,x_3]\)，假设函数为\(h_\theta(x)=θ^TX\)。我们的目的是为了求出最">
<meta property="og:image" content="http://niuoo.github.io/media/14938262502932.jpg">
<meta property="og:image" content="http://niuoo.github.io/media/14938277151973.jpg">
<meta property="og:image" content="http://niuoo.github.io/media/14938660514848.jpg">
<meta property="og:image" content="http://niuoo.github.io/media/14938664062585.jpg">
<meta property="og:image" content="http://niuoo.github.io/media/14938678719388.jpg">
<meta property="og:updated_time" content="2017-05-10T10:09:58.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="线性回归和逻辑回归">
<meta name="twitter:description" content="回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 
线性回归
线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如\(θ_{1}x_{1}+θ_2x_{2}+θ_3x_{3}=y\)。都使用向量vector表示，\(θ^T =[θ_1,θ_2,θ_3],X^T= [x_1,x_2,x_3]\)，假设函数为\(h_\theta(x)=θ^TX\)。我们的目的是为了求出最">
<meta name="twitter:image" content="http://niuoo.github.io/media/14938262502932.jpg">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://niuoo.github.io/2017/05/02/线性回归和逻辑回归/"/>

  <title> 线性回归和逻辑回归 | niuoo </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">niuoo</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Develop with pleasure!</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                线性回归和逻辑回归
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-05-02T15:25:46+08:00" content="May 2 2017">
              May 2 2017
            </time>
          </span>

          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 <img src="/media/14938262502932.jpg"></p>
<h1 id="线性回归">线性回归</h1>
<p>线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如<span class="math">\(θ_{1}x_{1}+θ_2x_{2}+θ_3x_{3}=y\)</span>。都使用向量vector表示，<span class="math">\(θ^T =[θ_1,θ_2,θ_3],X^T= [x_1,x_2,x_3]\)</span>，假设函数为<span class="math">\(h_\theta(x)=θ^TX\)</span>。我们的目的是为了求出最恰当的θ，即<span class="math">\(θ_1,θ_2,θ_3\)</span>三者的值，使得损失函数<span class="math">\(J(θ)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})^2\)</span>的值最小。</p>
<h3 id="什么是梯度gradient">什么是梯度(Gradient)？</h3>
<p>简单来说，梯度就是损失函数关于权值θ的倒数，也就是关于θ的变化率(导数即变化率吧)。</p>
<h3 id="梯度下降算法gradient-descent">梯度下降算法(Gradient descent)</h3>
<p>我们得到的一般是个局部最优解。如果是个碗状或者叫做倒钟型的函数，那局部最优也是全局最优解。<br>The gradient descent algorithm is:<br>repeat until convergence:<br><span class="math">\(θ_j:=θ_j−α\frac∂{∂θ_j}J(θ_1,θ_2,θ_3)\)</span><br>where j=1,2,3represents the feature index number.<br><a href="https://d3c33hcgiwev3.cloudfront.net/_ec21cea314b2ac7d9e627706501b5baa_Lecture2.pdf?Expires=1493856000&amp;Signature=GSAbIM5AmG64UdFqdjCCWIn5hN~JZ8IheTVb6mliIEMdfhHgTrecl9toRVElelfaWZGY3vPkI33K7uOHicFc52EldArFxSunmfh4Mr4yjiEpZBbSF8-Tl9cWVTy2pAixsdpkmlL37Lku8VGax-LoenwwvR0i055g8j2wKJCGOrQ_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" target="_blank" rel="external">斯坦福课程线性回归教案拉到三分之二处看</a><br><a href="http://m.blog.csdn.net/article/details?id=51188876" target="_blank" rel="external">几种梯度下降方法</a><br><a href="http://blog.csdn.net/qq_34206952/article/details/54316285" target="_blank" rel="external">求导公式证明</a><br>α我们称之为learning rate步长，如果太大，那么损失函数不能收敛，如果太小，会收敛过慢。</p>
<p>另外需要注意特征缩放(feature scaling)，确保不同特征的取值在相近的范围内，这样梯度下降法就能更快的收敛。因为如果不做这个，J(θ)在θ1和θ2轴上的投影将会非常的瘦长，收敛的过程需要走很多步，才能到达最小值，做特征缩放，就是为了让偏移没那么严重，投影看起来更圆一些。进行特征缩放时，将特征的取值约束到−1≤x(i)≤1或者−0.5≤x(i)≤0.5。<br><span class="math">\(x_i:=\frac{x_i−μ_i}{s_i}\)</span> μi表示feature(i)的平均值，si是feature(i)值的范围(max - min),或者是标准差。<br>For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, <span class="math">\(xi:=\frac{price−1000}{1900}\)</span>.<br>说句题外话，这个特征缩放也很像将一般的正态分布转化成标准正态分布的过程:p</p>
<p>另外还有对应线性回归的，多项式回归(曲线回归)。有时候，线性函数无法合适的进行数据的拟合，这时候也许多项式进行拟合效果更好（曲线曲面等）。</p>
<h3 id="正规方程法normal-equation">正规方程法(Normal Equation)</h3>
<p>此乃最小二乘法，<span class="math">\(θ = (X^TX)^{-1}X^Ty\)</span>,不需要进行特征的缩放，不需要选择α，不需要迭代，但梯度下降法在很多特征变量的情况下，也能运行地相当好，即使有上百万的特征变量，通常很有效，时间复杂度为<span class="math">\(O(kn^2)\)</span>;而正规方程法，为了求解参数θ，需要求解<span class="math">\((X^TX)^{-1}\)</span>，其中<span class="math">\(X^TX\)</span>这是个n*n的矩阵，然后求逆矩阵的计算量，大概是矩阵纬度的三次方,<span class="math">\(O(n^3)\)</span>，因此当n很大时，这个方法会非常慢的。n&gt;1W就考虑梯度下降吧。只要特征数目不是很大，用正规方程法是非常好的，对于线性回归的模型。</p>
<table>
<thead>
<tr class="header">
<th align="left">Gradient Descent</th>
<th align="left">Normal Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Need to choose alpha</td>
<td align="left">No need to choose alpha</td>
</tr>
<tr class="even">
<td align="left">Needs many iterations<span class="Apple-tab-span" style="white-space:pre"></span></td>
<td align="left">No need to iterate</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(O(kn^2)\)</span></td>
<td align="left"><span class="math">\(O(n^3)\)</span>,need to calculate inverse of <span class="math">\(X^TX\)</span></td>
</tr>
<tr class="even">
<td align="left">Works well when n is large</td>
<td align="left">Slow if n is very large</td>
</tr>
</tbody>
</table>
<p>如果<span class="math">\(X^TX\)</span>是不可逆(不可逆的矩阵为奇异或退化矩阵)的时候呢？如果存在两个特征值有线性相关的时候，矩阵不可逆，或者特征向量太多的时候(e.g. m ≤ n)，矩阵或许也不可逆，这种情况就删除重复特征的一个，无须同时保留。一般情况下，算法库也会给我们一个伪逆矩阵或者是逆矩阵的解。<br>注:设A是数域上的一个n阶方阵，若在相同数域上存在另一个n阶矩阵B，使得：AB=BA=E。则我们称B是A的逆矩阵，而A则被称为可逆矩阵。</p>
<h1 id="逻辑回归logistic-regression">逻辑回归(Logistic Regression)</h1>
<p>sigmoid可以轻松处理0/1分类问题sigmoid函数，又称逻辑函数Logistic Function，用来进行归一化处理(促使<span class="math">\(y^{(i)}∈(0,1)\)</span>)。逻辑回归的主体还是回归操作：回归对象是sigmoid函数，它将输入映射为处于0到1之间的小数，得到这个小数之后人为将其解读成概率，然后根据事先设定的阈值进行分类。 <span class="math">\(\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}\)</span></p>
<p>sigmoid函数图像如下 <img src="/media/14938277151973.jpg"> <span class="math">\(h_θ(x)\)</span>代表了输出是1的概率为多少。如<span class="math">\(h_θ(x)\)</span>=0.7表示结果有70%的可能性是1。<br><span class="math">\(\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}\)</span><br>因为结果都是离散数值，0或者1，所以当结果大于等于0.5的时候，就认为是1，反之为0.<br><span class="math">\(\begin{align*}&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \newline\end{align*}\)</span><br>等价于<br><span class="math">\(\begin{align*}&amp; \theta^T x \geq 0 \Rightarrow y = 1 \newline&amp; \theta^T x &lt; 0 \Rightarrow y = 0 \newline\end{align*}\)</span><br>另外<span class="math">\(z = \theta^T x\)</span> 函数也不一定是线性的，例<span class="math">\(z =θ_0+θ_1x^2_1+θ_2x^2_2\)</span>也可以是个圆，或者其他的形状，只要拟合数据就OK。<br>逻辑回归的损失函数如下：<br><span class="math">\(\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*}\)</span><br>当y = 1的时候，关于J(θ)和$ h_θ(x)<span class="math">\(的函数\)</span>Cost(h_(x),y) = -(h_(x))$图像如下: <img src="/media/14938660514848.jpg"></p>
<p><span class="math">\(\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline \end{align*}\)</span></p>
<p>当y = 0的时候，关于J(θ)和$ h_θ(x)<span class="math">\(的函数\)</span>Cost(h_(x),y) = -(1-h_(x))$图像如下: <img src="/media/14938664062585.jpg"> <span class="math">\(\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y\newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*}\)</span></p>
<p>综上，得到损失函数为<span class="math">\(\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))\)</span><br>也写成<span class="math">\(J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]\)</span><br>向量化可表示成<br><span class="math">\(\begin{align*} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align*}\)</span><br>梯度下降:<br><span class="math">\(\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta)=\theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace\end{align*}\)</span><br>向量化表示为:<span class="math">\(\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})\)</span></p>
<h1 id="综述">综述</h1>
<p><img src="/media/14938678719388.jpg"> 第一个图像是欠拟合的(underfitting),第二个相对合适，第三个过拟合。 欠拟合，一般都是因为h函数太简单，或者是特征向量过少。而过度拟合问题(Overfitting)，往往是因为函数太复杂了，虽然很完美的拟合了输入数据，但是产生了许多不必要的曲线角度，对于往后的测试，其实是不利的。<br>解决过度拟合问题:</p>
<ol style="list-style-type: decimal">
<li>减少特征数：手动选择要保留的特征，或者使用模型选择算法</li>
<li>正则化：保留所有的特征，但是减轻θ的影响。 <span class="math">\(min_\theta\ \dfrac{1}{2m}\  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2\)</span><br>λ太大，就会欠拟合，因为θ会很小，甚至为0；如果λ太小，就会过拟合，因为θ会很大，正则化也失去了相应的作用。<br>线性回归的正则化的梯度下降和最小二乘法，看这里<a href="https://www.coursera.org/learn/machine-learning/supplement/pKAsc/regularized-linear-regression" target="_blank" rel="external">课程资料</a><br>逻辑回归正则化看这里<a href="https://www.coursera.org/learn/machine-learning/supplement/v51eg/regularized-logistic-regression" target="_blank" rel="external">课程资料</a></li>
</ol>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/05/02/机器学习可以做什么/" rel="next" title="关于机器学习">
                <i class="fa fa-chevron-left"></i> 关于机器学习
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/05/04/神经网络初探/" rel="prev" title="神经网络初探">
                神经网络初探 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.gif"
               alt="niuoo" />
          <p class="site-author-name" itemprop="name">niuoo</p>
          <p class="site-description motion-element" itemprop="description">It is my Blog!</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">15</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#线性回归"><span class="nav-number">1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是梯度gradient"><span class="nav-number">1.0.1.</span> <span class="nav-text">什么是梯度(Gradient)？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降算法gradient-descent"><span class="nav-number">1.0.2.</span> <span class="nav-text">梯度下降算法(Gradient descent)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#正规方程法normal-equation"><span class="nav-number">1.0.3.</span> <span class="nav-text">正规方程法(Normal Equation)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#逻辑回归logistic-regression"><span class="nav-number">2.</span> <span class="nav-text">逻辑回归(Logistic Regression)</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#综述"><span class="nav-number">3.</span> <span class="nav-text">综述</span></a></li></ol></div>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">niuoo</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  

  

</body>
</html>
