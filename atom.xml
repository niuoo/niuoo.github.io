<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>niuoo</title>
  <subtitle>Develop with pleasure!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://niuoo.github.io/"/>
  <updated>2018-03-04T10:51:27.000Z</updated>
  <id>http://niuoo.github.io/</id>
  
  <author>
    <name>niuoo</name>
    <email>niuooo@yeah.net</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>余弦定理证明</title>
    <link href="http://niuoo.github.io/2018/03/04/%E4%BD%99%E5%BC%A6%E5%AE%9A%E7%90%86%E8%AF%81%E6%98%8E/"/>
    <id>http://niuoo.github.io/2018/03/04/余弦定理证明/</id>
    <published>2018-03-04T10:51:27.000Z</published>
    <updated>2018-03-04T10:51:27.000Z</updated>
    
    <content type="html"><![CDATA[
]]></content>
    
    <summary type="html">
    
      

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2018-02-04 把计划变成习惯</title>
    <link href="http://niuoo.github.io/2018/02/04/2018-02-04-%E6%8A%8A%E8%AE%A1%E5%88%92%E5%8F%98%E6%88%90%E4%B9%A0%E6%83%AF/"/>
    <id>http://niuoo.github.io/2018/02/04/2018-02-04-把计划变成习惯/</id>
    <published>2018-02-04T04:56:38.000Z</published>
    <updated>2018-02-09T08:54:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>节奏很重要。适合的节奏才是最好的，一开始跑得快乱了节奏反而不好。节奏不用太快，合适就好。每天把做的事情都定成课程表，到点就做，把节奏变成习惯就好了。这点很有意义，因为人按习惯走非常轻松，按计划走很痛苦，一个折中的方法就是把计划变成习惯，就好了。</p>
<p>今天周末，昨天晚上3点睡，上午9点多就醒了。白天坚持住，千万不要睡觉了。今天开始好好调节生物钟。</p>
<p>现在是中午1点开始吃饭，顺便看吐槽大会。2点吃好饭。 2点开始看fastai的第二课。把代码撸一遍，放到aws上面，结合视频看一下。不要急，看多少是多少，坚持到晚上10点睡觉。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;节奏很重要。适合的节奏才是最好的，一开始跑得快乱了节奏反而不好。节奏不用太快，合适就好。每天把做的事情都定成课程表，到点就做，把节奏变成习惯就好了。这点很有意义，因为人按习惯走非常轻松，按计划走很痛苦，一个折中的方法就是把计划变成习惯，就好了。&lt;/p&gt;
&lt;p&gt;今天周末，昨天
    
    </summary>
    
      <category term="日记" scheme="http://niuoo.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>SVM数学原理详解</title>
    <link href="http://niuoo.github.io/2018/01/18/SVM%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/"/>
    <id>http://niuoo.github.io/2018/01/18/SVM数学原理详解/</id>
    <published>2018-01-18T15:15:31.000Z</published>
    <updated>2018-03-04T10:54:34.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="准备知识">准备知识</h2>
<p>余弦定理的证明 如图所示 <img src="/media/15162913744684.jpg"></p>
<p>在△ABC中，BC=a，AC=b，AB=c，作AD⊥BC于D，则AD=c<em>sinB，DC=a-BD=a-c</em>cosB 在直角△ABD中， b²=AD²+DC²=(c<em>sinB)²+(a-c</em>cosB)² =c²sin²B+a²-2ac<em>cosB+c²cos²B =c²(sin²B+cos²B)+a²-2ac</em>cosB =c²+a²-2ac*cosB</p>
<ol style="list-style-type: decimal">
<li><a href="https://www.cnblogs.com/vive/p/4563803.html" target="_blank" rel="external">向量点积(内积)</a></li>
<li><a href="https://www.cnblogs.com/graphics/archive/2010/07/10/1774809.html" target="_blank" rel="external">点到平面的距离公式</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;准备知识&quot;&gt;准备知识&lt;/h2&gt;
&lt;p&gt;余弦定理的证明 如图所示 &lt;img src=&quot;/media/15162913744684.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;在△ABC中，BC=a，AC=b，AB=c，作AD⊥BC于D，则AD=c&lt;em&gt;sinB，DC=a-BD=a-
    
    </summary>
    
      <category term="SVM" scheme="http://niuoo.github.io/categories/SVM/"/>
    
      <category term="零碎知识" scheme="http://niuoo.github.io/categories/SVM/%E9%9B%B6%E7%A2%8E%E7%9F%A5%E8%AF%86/"/>
    
    
  </entry>
  
  <entry>
    <title>2018-01-16日记</title>
    <link href="http://niuoo.github.io/2018/01/16/2018-01-04%E6%97%A5%E8%AE%B0/"/>
    <id>http://niuoo.github.io/2018/01/16/2018-01-04日记/</id>
    <published>2018-01-16T07:22:55.000Z</published>
    <updated>2018-03-04T10:57:40.000Z</updated>
    
    <content type="html"><![CDATA[<p>19:30-21:30 看完 <a href="http://zyy1217.com/2016/10/11/Word2vec%20%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3/" target="_blank" rel="external">word2vec数学原理</a> 21:30-23:00 看完 <a href="http://blog.csdn.net/v_july_v/article/details/7624837" target="_blank" rel="external">SVM</a></p>
<p>一定要做完啊 ，到时候过来打卡。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;19:30-21:30 看完 &lt;a href=&quot;http://zyy1217.com/2016/10/11/Word2vec%20%E4%B8%AD%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3
    
    </summary>
    
      <category term="日记" scheme="http://niuoo.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>keras 修改 backend</title>
    <link href="http://niuoo.github.io/2017/12/31/keras-%E4%BF%AE%E6%94%B9-backend/"/>
    <id>http://niuoo.github.io/2017/12/31/keras-修改-backend/</id>
    <published>2017-12-31T13:32:07.000Z</published>
    <updated>2018-02-04T15:06:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="keras后端">Keras后端</h2>
<p>什么是“后端” Keras是一个模型级的库，提供了快速构建深度学习网络的模块。Keras并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。Keras依赖于处理张量的库就称为“后端引擎”。Keras提供了三种后端引擎Theano/Tensorflow/CNTK，并将其函数统一封装，使得用户可以以同一个接口调用不同后端引擎的函数</p>
<p>CNTK是微软家的。</p>
<h2 id="keras修改backend的方法">keras修改backend的方法</h2>
<p>如果你至少运行过一次Keras，你将在下面的目录下找到Keras的配置文件：</p>
<p><code>$HOME/.keras/keras.json</code> 如果该目录下没有该文件，你可以手动创建一个</p>
<p>文件的默认配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;image_data_format&quot;: &quot;channels_last&quot;,</div><div class="line">    &quot;epsilon&quot;: 1e-07,</div><div class="line">    &quot;floatx&quot;: &quot;float32&quot;,</div><div class="line">    &quot;backend&quot;: &quot;tensorflow&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>将backend字段的值改写为你需要使用的后端：theano或tensorflow或者CNTK，即可完成后端的切换</p>
<p>我们也可以通过定义环境变量KERAS_BACKEND来覆盖上面配置文件中定义的后端：</p>
<p><code>KERAS_BACKEND=tensorflow python -c &quot;from keras import backend;&quot;</code></p>
<h2 id="using-tensorflow-backend.">Using TensorFlow backend.</h2>
<p>keras.json 细节</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;image_data_format&quot;: &quot;channels_last&quot;,</div><div class="line">    &quot;epsilon&quot;: 1e-07,</div><div class="line">    &quot;floatx&quot;: &quot;float32&quot;,</div><div class="line">    &quot;backend&quot;: &quot;tensorflow&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>或者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;image_dim_ordering&quot;: &quot;tf&quot;, </div><div class="line">    &quot;epsilon&quot;: 1e-07, </div><div class="line">    &quot;floatx&quot;: &quot;float32&quot;, </div><div class="line">    &quot;backend&quot;: &quot;tensorflow&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="using-theano-backend.">Using Theano backend.</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    &quot;image_dim_ordering&quot;: &quot;th&quot;, </div><div class="line">    &quot;epsilon&quot;: 1e-07, </div><div class="line">    &quot;floatx&quot;: &quot;float32&quot;, </div><div class="line">    &quot;backend&quot;: &quot;theano&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>但是上面这种方式有一个弊端就是每次当你想切换backend的时候都需要手动修改这个文件，所以有一种能够动态修改的方式不是更好吗？请看下面的方法：</p>
<h2 id="代码载入backend">代码载入backend</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> os</div><div class="line">os.environ[<span class="string">'KERAS_BACKEND'</span>] = <span class="string">'tensorflow'</span></div><div class="line"><span class="keyword">import</span> keras.backend <span class="keyword">as</span> K</div><div class="line">K.set_image_dim_ordering(<span class="string">'tf'</span>)</div></pre></td></tr></table></figure>
<p>首先载入os库，将keras的backend修改为tensorflow，再将keras的backend加载，设置order为tensorflow格式。</p>
<p>附： <a href="https://keras-cn.readthedocs.io/en/latest/backend/" target="_blank" rel="external">keras中文文档</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;keras后端&quot;&gt;Keras后端&lt;/h2&gt;
&lt;p&gt;什么是“后端” Keras是一个模型级的库，提供了快速构建深度学习网络的模块。Keras并不处理如张量乘法、卷积等底层操作。这些操作依赖于某种特定的、优化良好的张量操作库。Keras依赖于处理张量的库就称为“后端引
    
    </summary>
    
      <category term="环境" scheme="http://niuoo.github.io/categories/%E7%8E%AF%E5%A2%83/"/>
    
    
  </entry>
  
  <entry>
    <title>推荐系统</title>
    <link href="http://niuoo.github.io/2017/12/27/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>http://niuoo.github.io/2017/12/27/推荐系统/</id>
    <published>2017-12-27T03:36:23.000Z</published>
    <updated>2018-02-28T16:56:47.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="svd">SVD++</h1>
<p>关于SVD++，看这篇博客 <a href="https://plushunter.github.io/2017/03/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%B3%BB%E5%88%97%EF%BC%8813%EF%BC%89%EF%BC%9A%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%EF%BC%883%EF%BC%89%E2%80%94%E2%80%94%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E6%8A%80%E6%9C%AF/" target="_blank" rel="external">矩阵分解技术</a></p>
<h1 id="word2vec">Word2Vec</h1>
<p><a href="https://www.cnblogs.com/peizhe123/p/7447164.html" target="_blank" rel="external">58同城的推荐</a> 深度学习正逐渐被各大公司应用于推荐系统中，我们也正在进行尝试。目前，FNN（Factorisation machine supported neuralnetwork）模型应用，相比单机器学习模型，FNN有较稳定的效果提升，但比融合模型效果要稍差，目前正在进行深度模型的调优，并在尝试引入Wide&amp;Deep等其他深度模型。</p>
<p>网易考拉的这篇论文是对用户的历史购买订单使用FNN，然后对用户的实时浏览点击网页使用RNN<a href="https://arxiv.org/pdf/1706.10231.pdf" target="_blank" rel="external">网易考拉推荐系统</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;svd&quot;&gt;SVD++&lt;/h1&gt;
&lt;p&gt;关于SVD++，看这篇博客 &lt;a href=&quot;https://plushunter.github.io/2017/03/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>fast.ai 环境配置</title>
    <link href="http://niuoo.github.io/2017/12/19/fast-ai-%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    <id>http://niuoo.github.io/2017/12/19/fast-ai-环境配置/</id>
    <published>2017-12-19T15:52:23.000Z</published>
    <updated>2018-03-04T10:52:51.000Z</updated>
    
    <content type="html"><![CDATA[<p>我最最讨厌的就是做如下的事情。求老天鹅赐男友一枚，能帮我配环境的那种。</p>
<h1 id="申请工单">申请工单</h1>
<p>首先要注册一个aws帐号，申请工单的过程请看 <a href="http://course.fast.ai/lessons/aws.html" target="_blank" rel="external">AWS Lesson</a>，根据视频要求填写，填写申请描述的时候，根据视频填写fast-ai-course，这个貌似是人家搞好的，所以填写完相关信息，提交后就不用管了，可以进行后续操作了。注意一点，我们申请的是<strong>p2</strong>的实例，貌似中国没有，所以选区域的时候，最好填写<strong>us-west-2 俄勒冈</strong>。</p>
<h1 id="生成密钥对-key-pair">生成密钥对 key-pair</h1>
<p>生成密钥对，如下图所示，点进去后，就可以create了，密钥对名称最好叫做aws-key-fast-ai，因为脚本里面是这么写的嘛，密钥对创建后，会被自动下载到本地，下载完，请手动copy到目录~/.ssh/aws-key-fast-ai.pem <img src="/media/15136997192930.jpg"></p>
<h1 id="获取安装脚本">获取安装脚本</h1>
<p>执行 git clone https://github.com/fastai/courses.git ，获取相关的执行脚本，所有环境脚本在相对目录courses/setup下面。</p>
<h1 id="执行alias脚本">执行alias脚本</h1>
<p>aws-alias.sh 这个脚本是一个alias命令脚本，给命令起别名，以简化命令。这个脚本执行完，alias的命令只能在当前目录下生效，若想在iTerm全局生效，还要添加到环境变量，在文件.zshrc里面添加<code>source ~/deeplearning/courses/setup/aws-alias.sh</code>，当然了，这是我自己的路径，此处请酌情换成你自己的路径，我的系统是mac OS X。</p>
<p>如果出现aws命令不识别的情况，那么请先敲命令<code>pip install awscli</code></p>
<h1 id="执行setup_p2.sh脚本">执行setup_p2.sh脚本</h1>
<p>前面密钥对生成完了，我们就可以执行setup_p2.sh脚本，这个脚本会让AWS创建一个实例，并且启动实例。必须有密钥对，本步骤才可以执行喔。就是要有<code>~/.ssh/aws-key-fast-ai.pem</code>这个文件</p>
<p>启动完，可以使用<code>aws-get-p2</code>，获取实例名称，如果创建好实例，会出现『i-0cfc671a9e32fb6a4』之类的实例ID。<code>aws-ip</code>可以获取弹性ip。这些aws相关的命令，可以通过<code>cat aws-alias.sh</code>进行查看。</p>
<p>aws configure 可以查看aws的相关配置信息，自己不要输入修改了，这个是干嘛的，我也不知道，其实相关的信息我也进行了修改，but没什么用啊，如果谁知道，联系我喔。</p>
<h1 id="登录">登录</h1>
<p>最后，直接通过aws-ssh，就可以登录AWS的ubuntu啦！出现如下界面，代表着陆成功。 <img src="/media/15137016427663.jpg"></p>
<p>那么问题来了，今天下午15点的时候，谁登录了我的ip，这里到底是怎么回事，Last login是谁搞得，反正不是我。如果谁知道这个地方怎么回事，请联系我。</p>
<h1 id="从此以后如何启动关闭实例">从此以后，如何启动关闭实例</h1>
<p><code>aws-start</code> 启动实例。<code>aws-stop</code> 关闭实例。</p>
<p>另外，很多命令也可以在文件<code>fast-ai-commands.txt</code>里面查看。如果需要删除相关的按照内容，那么执行当前courses/setup目录下的<code>fast-ai-remove.sh</code>脚本。</p>
<h1 id="其他">其他</h1>
<p>t2的实例是免费的。调代码的时候，最好使用t2实例。p2是每小时0.9美元的价格。使用aws每个月还有固定硬盘的费用，以及绑定固定IP的费用。每个月大约200元左右。飙泪……以后用习惯了，尝试AWS的竞价模式。</p>
<p>t2的安装也根据相应的指示来，目录下有t2的安装脚本。</p>
<p>如果换了一台电脑，那么把原来的密钥对aws-key-fast-ai.pem文件copy到对应目录下面。setup_p2.sh脚本就不用执行了，直接执行一下aws-alias，就可以了。然后通过aws的命令进行登录。</p>
<p>最近，我的情绪好暴躁啊。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我最最讨厌的就是做如下的事情。求老天鹅赐男友一枚，能帮我配环境的那种。&lt;/p&gt;
&lt;h1 id=&quot;申请工单&quot;&gt;申请工单&lt;/h1&gt;
&lt;p&gt;首先要注册一个aws帐号，申请工单的过程请看 &lt;a href=&quot;http://course.fast.ai/lessons/aws.html
    
    </summary>
    
      <category term="环境" scheme="http://niuoo.github.io/categories/%E7%8E%AF%E5%A2%83/"/>
    
    
  </entry>
  
  <entry>
    <title>iTerm 快捷键 shortcuts</title>
    <link href="http://niuoo.github.io/2017/12/19/iTerm-%E5%BF%AB%E6%8D%B7%E9%94%AE-shortcuts/"/>
    <id>http://niuoo.github.io/2017/12/19/iTerm-快捷键-shortcuts/</id>
    <published>2017-12-19T08:10:16.000Z</published>
    <updated>2018-01-29T02:32:48.000Z</updated>
    
    <content type="html"><![CDATA[<p>iTerm的快捷键，还是记录一下为好</p>
<h2 id="文本控制">文本控制</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">⌘ + shift + h      列出剪切板历史</div><div class="line">⌘ + ;              输入前缀几个字母，列出相关输入过的命令</div><div class="line">ctrl +u            清除当前行，无论光标在什么位置</div><div class="line">ctrl + w           删除光标前的单词</div><div class="line">ctrl + k           删除到文本末尾</div><div class="line">ctrl + f/b         前进后退，相当于左右方向键</div><div class="line">ctrl + p           上一条命令，相当于方向键上</div><div class="line">ctrl + r           搜索命令历史</div><div class="line">ctrl + h           删除之前的字符</div><div class="line">ctrl + t           交换光标处文本</div><div class="line">⌘ + —/+/0         调整字体大小，0是恢复默认大小</div><div class="line">⌘ + r              清屏，其实是滚到新的一屏，并没有清空。</div><div class="line">ctrl + l           同上</div><div class="line">⌘+/                高亮当前鼠标的位置</div></pre></td></tr></table></figure>
<h2 id="窗口切分">窗口切分</h2>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">shift + ⌘ + d    水平方向分割</div><div class="line">⌘ + d            垂直方向分割</div><div class="line">⌘ +`             前一个窗口</div><div class="line">⌘ + ~            后一个窗口</div></pre></td></tr></table></figure>
<h2 id="回放功能">回放功能</h2>
<p>即时回放 使用Command + Opt + b 打开即时回放，按Esc退出。即时回放可以记录终端输出的状态，让你“穿越时间”查看终端内容。默认每个会话最多储存4MB的内容，可以在设置中更改（Preferences -&gt; Genernal -&gt; Instant Replay）。这只能回放本次打开的iTerm信息。</p>
<h2 id="保存窗口状态">保存窗口状态</h2>
<p>通过 Window -&gt; Save Window Arrangement 可以保存当前窗口状态的快照，包括打开的窗口，标签页和面板。通过 Window -&gt; Restore Window Arrangement 还原。还可以在 Preferences -&gt; General -&gt; Open saved window arrangement 中设置在启动iTerm2时自动恢复窗口状态</p>
<h2 id="选取文本">选取文本</h2>
<p>Command + Option 可以用鼠标画出一个矩形，用类似截图的方式选取文本</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;iTerm的快捷键，还是记录一下为好&lt;/p&gt;
&lt;h2 id=&quot;文本控制&quot;&gt;文本控制&lt;/h2&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/di
    
    </summary>
    
      <category term="工具使用" scheme="http://niuoo.github.io/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>tmux 快捷键 shortcuts</title>
    <link href="http://niuoo.github.io/2017/12/19/tmux-%E5%BF%AB%E6%8D%B7%E9%94%AE-shortcuts/"/>
    <id>http://niuoo.github.io/2017/12/19/tmux-快捷键-shortcuts/</id>
    <published>2017-12-19T08:09:41.000Z</published>
    <updated>2018-01-29T02:32:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>本来打算用一下tmux呢，后来发现还是mac OS下面还是直接iTerm方便，不过切到Linux的时候，估计还是要用tmux吧。 tmux使用也比较简单。先把脑海中的记录一下，防止以后忘记。感觉和vim的使用体验好像喔。我挺不喜欢这种设定。</p>
<h2 id="tmux安装">tmux安装</h2>
<p><code>brew install tmux</code></p>
<h2 id="新建一个tmux-session">新建一个tmux session</h2>
<table>
<thead>
<tr class="header">
<th align="left">命令</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tmux new -s myname</td>
<td align="left">启动一个session</td>
</tr>
<tr class="even">
<td align="left">tmux a # (or at, or attach)</td>
<td align="left">恢复到已经存在的session number</td>
</tr>
<tr class="odd">
<td align="left">tmux a -t myname</td>
<td align="left">恢复到某个会话</td>
</tr>
<tr class="even">
<td align="left">tmux ls</td>
<td align="left">列出当前会话 list sessions</td>
</tr>
<tr class="odd">
<td align="left">tmux kill-session -t myname</td>
<td align="left">终止一个session. Kill session</td>
</tr>
</tbody>
</table>
<h2 id="切窗口-split-panes">切窗口 split panes</h2>
<p>Ctrl+b 等于是切到命令模式</p>
<pre>
Ctrl+b %        水平分割窗口  
Ctrl+b "        垂直分割窗口
Ctrl+b x        kill pane
Ctrl+b o        移动光标到不同的一个小窗口 Swap panes
ctrl+b d        跳出当前的tmux
Ctrl+B $        Name session
Ctrl+B s        list session
</pre>
<h2 id="管理session">管理session</h2>
<pre>
Ctrl+B c      Create window
Ctrl+B w      List Windows
Ctrl+B n      Next window
Ctrl+B p      Previous window
Ctrl+B f      Find window
Ctrl+B ,      Name window
Ctrl+B &      Kill window
</pre>
<h2 id="关掉tmux">关掉tmux</h2>
<p><code>tmux kill-server</code></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本来打算用一下tmux呢，后来发现还是mac OS下面还是直接iTerm方便，不过切到Linux的时候，估计还是要用tmux吧。 tmux使用也比较简单。先把脑海中的记录一下，防止以后忘记。感觉和vim的使用体验好像喔。我挺不喜欢这种设定。&lt;/p&gt;
&lt;h2 id=&quot;tmux
    
    </summary>
    
      <category term="工具使用" scheme="http://niuoo.github.io/categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"/>
    
    
  </entry>
  
  <entry>
    <title>2017-12-17日记</title>
    <link href="http://niuoo.github.io/2017/12/17/2017-12-17%E6%97%A5%E8%AE%B0/"/>
    <id>http://niuoo.github.io/2017/12/17/2017-12-17日记/</id>
    <published>2017-12-17T05:08:26.000Z</published>
    <updated>2018-02-28T16:55:53.000Z</updated>
    
    <content type="html"><![CDATA[<p>14:00~17:00 看贝叶斯和论文 &lt;<a href="https://pdfs.semanticscholar.org/a5c4/35690c717d04801a68950f14036c38f2a9ab.pdf" target="_blank" rel="external">What is the expectation maximization algorithm?</a>&gt;</p>
<p>18:00~21:00 fast.ai 第二课看完。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;14:00~17:00 看贝叶斯和论文 &amp;lt;&lt;a href=&quot;https://pdfs.semanticscholar.org/a5c4/35690c717d04801a68950f14036c38f2a9ab.pdf&quot; target=&quot;_blank&quot; rel=&quot;ext
    
    </summary>
    
      <category term="日记" scheme="http://niuoo.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>2017-12-16日记</title>
    <link href="http://niuoo.github.io/2017/12/17/2017-12-16%E6%97%A5%E8%AE%B0-1/"/>
    <id>http://niuoo.github.io/2017/12/17/2017-12-16日记-1/</id>
    <published>2017-12-16T20:21:53.000Z</published>
    <updated>2018-03-04T10:53:39.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安排">2017-12-16安排</h1>
<p>17:30~20:00 看<a href="https://mp.weixin.qq.com/s?__biz=MzIzMjU1NTg3Ng==&amp;mid=2247486415&amp;idx=1&amp;sn=b50f4cc8ef9cd4d4d34cbdb0d30b4afc&amp;chksm=e8925fc4dfe5d6d217d46bca560eb7030232c14183cf0b791aa9af7da5671ac536f8a45693f5&amp;mpshare=1&amp;scene=1&amp;srcid=1213nA8yTjxK7pw2LxnuSRGu&amp;key=da103cb67940783e381102fb14674ebd0d039d798c7b1932fa1ce5f48d91ad89962437c729f7545312a6ce1fa03df0a1640dbb145e25544cd7300fea7763d40720cd1adc84907bd0a8c3c52bc836bfe7&amp;ascene=0&amp;uin=MTA2MTM3MjcwNA%3D%3D&amp;devicetype=iMac+MacBookPro11%2C1+OSX+OSX+10.11.6+build(15G1510)&amp;version=12020810&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100&amp;pass_ticket=2%2B82FeLMFDTNJse7id71tUxAiI0JCuIWakI%2FV%2FKqOK8Veq9r%2FRy%2Bbxvfzgt2E1Cw" target="_blank" rel="external">Hinton和Jordan理解的EM算法</a></p>
<p>这篇文章我居然没读通，明天把不懂的地方请教下小伙伴。根据文章的引导，我想看看Daphne Koller的神书“Probabilistic Graphical Models: Principles and Techniques”，其中的第8章：The Exponential Family 和第19章 Partially Observed Data。 这两章几乎是Hinton对VBEM算法研究的高度浓缩。结果，我看全是英文作罢了。根据我看EM的过程，发现补理论知识，实在旷日持久。概率图模型，有机会成为下一个深度学习的理论之一。配套的视频，<a href="https://www.coursera.org/specializations/probabilistic-graphical-models" class="uri" target="_blank" rel="external">https://www.coursera.org/specializations/probabilistic-graphical-models</a></p>
<p>于是我做出个重大的决定，先不管理论了。直接fast.ai继续。硬着头皮把第一阶段的课程看完。然后再看看吴恩达deeplearning.ai课程。以后肯定会用到各种算法，到时候再慢慢理解。需要什么看什么。当然，拔苗助长也不好，但是可以先来一个成长周期短一点的种子，先果腹一下，别饿死最重要。</p>
<p>另外，Chuong B Do &amp; Serafim Batzoglou的Tutorial论文“What is the expectation maximization algorithm?”。明天下载下来读一下。目测这篇论文应该不难吧，我现在需要看简单明了容易理解的东西。这篇论文去哪里下载，我刚刚没找到。</p>
<p>20:00~22:00 <深度学习>看2小时。时间安排出问题了，这件事情没做。呵呵哒。</深度学习></p>
<p>22:00~23:00 <普林斯顿微积分>看1个小时，进入梦乡。也呵呵哒。</普林斯顿微积分></p>
<p>每做好一步，过来打勾勾哦!</p>
<p>发现自己现在话特别多，话多了，真正思考的时间就没了。但是又需要一定的表达，把想法作为日记写进博客里面，可以帮自己理清楚思路。不过缺点是，博客变得杂乱无章， 毕竟这个博客，最初的想法是作为技术博客的。</p>
<p>不过嘛，反正是自己的博客，随便DIY了。</p>
<p>好了，开始废话时间了。</p>
<p>希望自己以后的时光，可以身体健康，杨柳细腰，体态轻盈。岁月，请对我的温柔。</p>
<p>另外远离各类社交网络。社交网络固然人才济济，可是仅仅做个观众，可是有点傻兮兮的。参与进去，又会浪费很多时间。</p>
<p>慢慢丰富自己的生活，可以钻研深度学习，也可以学自己感兴趣的东西，学个乐器啊，舞蹈之类什么的都可以。当然了，慢慢跑起来啦，运动还是需要的啦。</p>
<p>前段时间，我没处理好自己的思想，以前觉得自己会坦然处之。后来发现，自己毫无缚鸡之力，就雄赳赳气昂昂去了角斗场，现在想来，自己真是傻乎乎的可爱，只不过最终，还是血淋淋的受伤了。</p>
<p>写诗，是个逻辑思维的过程，找出合理的词汇，表达一定的意思，用理智分析情感，即可得。</p>
<p>我能做的，就是把自己做好了，打理好了，有些许魅力特征，讨人喜欢，就够了。</p>
<p>自己要过得开心，快乐，做自己喜欢的事情。</p>
<p>另外，关于工作上的事情，前段时间，搞的回购客预测的二分类问题，最后使用了特征化成了woe，进入LR模型，然后理论上，最终权值应该都是正的。因为sogmoid是个单调递增函数，特征的值的woe如果是负的，那么证明这个特征值会造成负影响，不利于结果变成正的，那么应该拉低结果值，如果是正的，理想上，是应该提升结果，所以权值系数应该全部为正数。但是我们模型最终的结果，却很多都是负数，经过我的思考，我认为是因为我们的最终结果本身就是非常非常差的，当然不会理想的系数都是正数。很多特征，为了使结果尽可能的好，都被迫，系数被拉成负的了。。网上说，可能是出现了多重共线性，对于多重共线性怎么检验判断和解决，我暂时不知道。周一上班了，看看，筛选一下系数绝对值大的，或是只看正的特征，再做做实验看看。。</p>
<p>如果系数不能都为理想的正数，恰恰说明了一个问题，因为我们的数据，本身就不是线性的啊，不能很好的拟合成我们理想的线性，只能自己freestyle了。无奈啊。</p>
<p>人生也好无奈啊。我最近好无奈啊。</p>
<p>我的外形，我的工作，我的学识，我的斗志，都是人生谷底啊，谷底啊。我还能爬出来吗，我要的可不是梯度下降啊，目标不是最小值啊。</p>
<p>我的内心戏，好多啊！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;安排&quot;&gt;2017-12-16安排&lt;/h1&gt;
&lt;p&gt;17:30~20:00 看&lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzIzMjU1NTg3Ng==&amp;amp;mid=2247486415&amp;amp;idx=1&amp;amp;sn
    
    </summary>
    
      <category term="日记" scheme="http://niuoo.github.io/categories/%E6%97%A5%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>召回率</title>
    <link href="http://niuoo.github.io/2017/07/12/%E5%8F%AC%E5%9B%9E%E7%8E%87/"/>
    <id>http://niuoo.github.io/2017/07/12/召回率/</id>
    <published>2017-07-12T14:01:59.000Z</published>
    <updated>2017-07-12T14:05:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/media/14998682171580.jpg"> <img src="/media/14998682636712.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/media/14998682171580.jpg&quot;&gt; &lt;img src=&quot;/media/14998682636712.jpg&quot;&gt;&lt;/p&gt;

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>林轩田机器学习基石笔记</title>
    <link href="http://niuoo.github.io/2017/06/06/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B0/"/>
    <id>http://niuoo.github.io/2017/06/06/林轩田机器学习基石笔记/</id>
    <published>2017-06-06T08:36:53.000Z</published>
    <updated>2017-07-24T07:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/niuoo/machine-learning-foundations/tree/master/foundations" target="_blank" rel="external"><strong>我的课程作业源码github地址</strong></a></p>
<p><a href="https://www.douban.com/doulist/3381853/" target="_blank" rel="external">这位同学的笔记做的很好</a> <img src="/media/14966834620659.jpg"> 知错能改，善莫大焉！:-)</p>
<p><img src="/media/14998352577521.jpg"> <a href="https://www.douban.com/note/319669984/" target="_blank" rel="external">PLA(Perceptron Learning Algorithm)感知机算法有效性证明过程</a>PLA不断进行错误修正的算法，是可以停下来的。可是这个算法有个很大的缺点，因为此算法必须建立在假设线性可分的基础上。如果假设不成立的话，PLA根本跑得停不下来。一开始我们是不会知道PLA能不能停下来的，这个时候可采用Pocket Algorithm，做得还不错。 <img src="/media/14998653580014.jpg"></p>
<div class="figure">
<img src="/media/14999565852705.jpg">

</div>
<p>Hoeffding’s inequality Hoeffding不等式 <img src="/media/15002855220854.jpg"></p>
<p><img src="/media/15002612963279.jpg"> 看到这一点的时候颇为迷惑，B(3,3)=7，B(4,3)是通过复制B(3,3)的一部分得来的。那么为什么α(复制的部分)需要no shatter any 2，其实想想假如 α shatter 2了，也就是B(3,2)，那么复制后，加上<span class="math">\(x_4\)</span>总共8种情形，必然是shatter 3 了，因为<span class="math">\(8 = 2^3\)</span>。 所以，得到：B(4,3) = 2*a + b &lt;= B(3,3) + B(3,2). 对于任意N &gt; k, 利用上述思路，可以证明 B(N,k) &lt;= B(N-1, k) + B(N-1,k-1). 有了递推不等式，通过数学归纳法，可证明下面的Bounding Function (N &gt; k) : <img src="/media/15002711060448.jpg"></p>
<p>归纳法证明过程如下： <img src="/media/15002882090009.jpg"></p>
<p>为什么机器学习是有效的？ <img src="/media/15002851562508.jpg"></p>
<div class="figure">
<img src="/media/15008820248463.jpg">

</div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/niuoo/machine-learning-foundations/tree/master/foundations&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;strong&gt;我的课程作业源码gi
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>做个机器学习Python调包侠</title>
    <link href="http://niuoo.github.io/2017/05/19/%E5%81%9A%E4%B8%AA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Python%E8%B0%83%E5%8C%85%E4%BE%A0/"/>
    <id>http://niuoo.github.io/2017/05/19/做个机器学习Python调包侠/</id>
    <published>2017-05-19T06:39:55.000Z</published>
    <updated>2017-06-06T08:45:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>算法</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;算法&lt;/p&gt;

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Recommender Systems 推荐系统</title>
    <link href="http://niuoo.github.io/2017/05/14/Recommender-Systems-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>http://niuoo.github.io/2017/05/14/Recommender-Systems-推荐系统/</id>
    <published>2017-05-14T05:06:31.000Z</published>
    <updated>2017-06-05T10:04:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统是机器学习的一个重要应用。很多团队都致力于建立更好的推荐系统，比如亚马逊、Netflix、eBay或者苹果公司的iTunes Genius做的事情，有很多网站或者系统试图向用户推荐新产品，亚马逊向你推荐新书，Netflix想你推荐新电影，诸如此类。而这些推进系统可能会看看你以前购买过什么书，或者以前你给哪些电影进行过评分，这些系统贡献了现今亚马逊收入的相当大一部分，而Netflix，他们向用户推荐的电影占了用户观看电影的大部分。推荐系统，其表现的一些改进就能带来显著且即刻产生的影响，这些影响关系到许多公司的最终业绩。<br>在机器学习中，特征量是重要的，选择的特征对学习算法的表现有很大影响。在机器学习领域，有这么一个宏大的想法，就是对于一些问题，可能不是所有问题，但是对于一些问题而言，存在一些算法，能试图自动地学习到一组优良的特征量，而不用人为动手设计或者手动编写特征。<br>有些情况，或许能够采用一种算法，来学习到使用什么特征量，而推荐系统，就是这种情形的一个例子，当然还有其他很多例子。通过学习推荐系统，我们能够对这种学习特征量的想法有一点了解，我们至少可以通过这个例子，来了解机器学习中的这种big idea。</p>
<h1 id="推荐系统算法">推荐系统算法</h1>
<h3 id="推荐系统在做什么">推荐系统在做什么</h3>
<p>如下图所示，给定这些r(i,j)与y(i,j)数据，其中r(i,j)表示用户j给电影i是否打分，y(i,j)表示用户j给电影i所打的分数，然后浏览全部数据，关注所有没有电影评分的地方，并试图预测这些带问号的地方，应该是什么数值。如下图，前三部电影可能是浪漫爱情电影，后两部是动作片，然后Alice和Bob看起来喜欢爱情片，我们预测Alice也许会给电影Cute puppies of love打分5，Bob给电影Romance forever打分4.5，推荐系统就是通过填充这些数值，然后推测用户更喜欢什么电影，并进行相应推荐。有一些电影，还有一些用户，用户给一些电影进行了评价打分，推荐系统所做的事情就是，通过这些评分，预测用户会怎样给还没看过的电影打分。<br><img src="/media/14947442269252.jpg"></p>
<h2 id="基于内容的推荐-cotent-based-recommendations">基于内容的推荐 Cotent Based Recommendations</h2>
<p>注意看<span class="math">\(x^{(i)}\)</span>，它表示某部电影是浪漫和动作片程度的多少。算法的目的是学到每个用户喜爱浪漫电影和动作电影程度的变量<span class="math">\(θ^{(j)}\)</span>，然后使用<span class="math">\((θ^{(j)})^Tx^{(i)}\)</span>就可以预测第j个用户可能给第i部电影所打的分数。<br><img src="/media/14947615593109.jpg"></p>
<p>学习每个<span class="math">\(θ^{(j)}\)</span>的值是一个基本的线性回归问题，算法的目的是为了学习到<span class="math">\(θ^{(j)}\)</span>使得下面的损失函数的值最小。但是这个又和我们以前所学习的线性回归问题不同，因为在这里，每个用户的<span class="math">\(θ^{(j)}\)</span>值是不同的，每个用户都有单独属于自己的<span class="math">\(θ^{(j)}\)</span>值，这就使得个性化推荐成为现实。我们可以把对每个观众打分的预测，当成一个独立的线性回归问题。<br><img src="/media/14947649924645.jpg"></p>
<p>优化目标就是图上的公式，对于每一个用户，我们都进行计算。<span class="math">\(θ^{(n_u)}\)</span>表示第n个用户的θ值。<br><img src="/media/14947714065234.jpg"></p>
<p>梯度下降过程如下。 <img src="/media/14947721242572.jpg"> 以上，就是运用线性回归的变体，来预测不同用户对不同电影的评分值。这种做法，叫做『基于内容的推荐』，因为我们具有电影的特征量<span class="math">\(x^{(i)}\)</span>，来表示电影内容的属性，比如电影爱情的成分是多少，动作的成分是多少。但是实际上，我们很少知道所有电影的特征，或者我们要卖的产品有什么特征，所以接下来，我们介绍不具有这些内容特征时，该如何进行推荐。</p>
<h2 id="协同过滤-collaborative-filtering">协同过滤 Collaborative Filtering</h2>
<p>协同过滤，可以自行学习所要使用的特征。<br>如下图所示，假如用户告诉了他们的偏好，那么我们可以知道每个<span class="math">\(θ^{(j)}\)</span>的值，来学习到<span class="math">\(x^{(i)}\)</span>的值。<br><img src="/media/14947754778241.jpg"></p>
<p>根据用户的偏好，学习电影的特征指数，优化目标如下图所示：<br><img src="/media/14947761941215.jpg"></p>
<p>如下图所示，把二者合并起来，就是协同过滤了。根据每个用户对多部电影的评分，以及每部电影由不同用户的评分，可以反复进行这样的过程，来估计出θ和x。协同过滤算法指的是，当你执行这个算法时，你通过一大堆用得到数据，这些数据实际上在高效地进行了协同合作，来得到每个人对电影的评分值。只要用户对某几部电影进行评分，每个用户就都在帮助算法，更好的学习出特征，这些特征又可以被系统运用，为其他人做出更准确的电影预测。协同的另一层意思是，每位用户，都在帮助系统，学习出更好的特征，这就是协同过滤。<br><img src="/media/14947774794944.jpg"></p>
<h2 id="协同过滤算法-collaborative-filtering-algorithm">协同过滤算法 Collaborative Filtering Algorithm</h2>
<p>把上面讲的两个损失函数合并起来，就是我们希望优化的新的代价函数，新的代价函数是同时关于θ和x的函数，二者同时更新，最终是的J最小。<br><img src="/media/14947796300697.jpg"></p>
<p>协同算法步骤：</p>
<ul>
<li><p>首先将会把θ和x初始化为小的随机值，这有点儿像神经网络训练，所有神经网络的权值参数，我们也是用小的随机数值来初始化的。协同过滤的初始化和神经网络初始化一样，也需要打破对称symmetry breaking. <img src="/media/14947804720110.jpg"></p></li>
<li><p>使用梯度下降，或者其他的高级优化算法，把代价函数最小化，如果求导，梯度下降更新写出来的结果如图中第2项所示。通过梯度下降，我们同时更新θ和x的值。<br> <img src="/media/14947802773743.jpg"></p></li>
<li><p>根据以上求出的θ和x的值，我们就可以预测每一个用户了。</p></li>
</ul>
<p>以上就是协同过滤算法的具体过程。通过协同过滤算法，可以同时学习几乎所有电影的特征x，和所有用户的参数θ，然后有很大机会，能对不同用户会如何评价他们尚未评分的电影，做出相当准确的预测。</p>
<h3 id="低秩矩阵分解-low-rank-matrix-factorization">低秩矩阵分解 Low Rank Matrix Factorization</h3>
<p>通过向量化的计算，来对所有的用户和所有的电影，进行评分计算。 <img src="/media/14947819692395.jpg"></p>
<h3 id="向用户做出相关推荐">向用户做出相关推荐</h3>
<p>根据协同过滤算法，当给出一件产品时，可以找到与之相关的其它产品，再例如，一位用户最近看上一件产品，看有没有其它相关的产品，你可以推荐给他。<br>怎么给用户进行相关推荐呢？请看下图。用户曾给电影i打过高分，我们根据算法得到<span class="math">\(x^{(i)}\)</span>，电影i的特征指数，那么我们就找到距离<span class="math">\(x^{(i)}\)</span>最近的几个电影，推荐给用户。<span class="math">\(||x^{(i)}-x^{(j)}||\)</span>表示的是<span class="math">\(x^{(i)}与x^{(j)}\)</span>的欧式距离。<br><img src="/media/14947831732793.jpg"></p>
<h3 id="预处理步骤-均值归一化">预处理步骤-均值归一化</h3>
<p>求出每一个电影的平均分μ，然后每个打分减去该电影的平均分，每一行的总和其实还是0.我们这样做均值归一化了。当我们在预测具体某个用户对某个电影评分的时候，还是要把平均分μ加上的。对于某个用户从来没评论过任何电影，那么也默认他的评分是平均分水平。 <img src="/media/14947848429032.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;推荐系统是机器学习的一个重要应用。很多团队都致力于建立更好的推荐系统，比如亚马逊、Netflix、eBay或者苹果公司的iTunes Genius做的事情，有很多网站或者系统试图向用户推荐新产品，亚马逊向你推荐新书，Netflix想你推荐新电影，诸如此类。而这些推进系统可能
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Dimensionality Reduction 主成分分析PCA</title>
    <link href="http://niuoo.github.io/2017/05/14/Dimensionality-Reduction-%E7%BB%B4%E5%BA%A6%E7%BA%A6%E5%87%8F/"/>
    <id>http://niuoo.github.io/2017/05/14/Dimensionality-Reduction-维度约减/</id>
    <published>2017-05-13T16:30:13.000Z</published>
    <updated>2017-06-05T10:04:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>使用Dimensionality Reduction-维数约减，一般是为了数据压缩，减少内存和硬盘空间存储，并提高算法速度。在特征降维的方法中，PCA(Principle Component Analysis)是最为经典和实用的特征降维技术，特别是在辅助图像识别方面有突出表现。 特征降维是无监督学习的另一个应用。降维/压缩问题是选取数据具有代表性的特征，在保持数据多样性(Variance)的基础上，规避掉大量的特征冗余和噪声，不过这个过程也很有可能会损失一些有用的模式信息。经过大量的实践证明，相较于损失的少部分模型性能，维度压缩可节省大量用于模型训练的时间，使得模型综合效率变得更高。</p>
<p>在实际项目中，我们会得到特征维度非常高的训练样本，有的特征向量之间是有关系的，比如预测房价的时候，得到的房子长度inches英寸和cm厘米，它们两个是线性相关的，所以有一个信息是冗余的，就可以去掉一个。又如下图所示，数据集中在一个平面Z附近，所以就可以把3D的数据降维成2D。 <img src="/media/14945865066705.jpg"></p>
<h1 id="主成分提取principal-component-analysis-problem-formulation">主成分提取(Principal Component Analysis problem formulation)</h1>
<p>对于降维问题，目前最流行的，最常用的算法就是PCA主成分分析法。 PCA做的就是，寻找一条直线，或者面，或者诸如此类，比起特征向量低维的空间，对数据进行投影，并且最小化投影距离，也就是数据点和投影后的点之间的距离。另外，在寻找vectors时，对于所有的数据集和特征向量，都是同等对待的。一般用投影的数据，当做被降维的数据使用。 <img src="/media/14946121526774.jpg"></p>
<p>关于上图的左边，注意区别PCA和线性回归，两个截然不同的概念。 <img src="/media/14946125078718.jpg"></p>
<h3 id="数据预处理-data-preprocessing">数据预处理 Data preprocessing</h3>
<p>在使用PCA之前，我们通常会有一个<strong>数据预处理</strong>的过程，就是对数据进行均值归一化。 <img src="/media/14946138068434.jpg"></p>
<h3 id="pac算法-principal-component-analysispca-algorithm">PAC算法 Principal Component Analysis(PCA) algorithm</h3>
<p>首先要做的是计算出∑协方差矩阵。<span class="math">\(∑=\frac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</span>,∑(Sigma)是个n×n的矩阵，因为<span class="math">\(x^{(i)}\)</span>是个n×1的向量，<span class="math">\((x^{(i)})^T\)</span>是个1×n的向量，两个向量相乘就是n×n的矩阵了。另外，协方差均值∑(Sigma)总满足一个数学性质，称为对称正定(symmetric positive definite)。这块线性代数的知识不懂，不过不必介意，反正就是这么求的，代码也不长。记得线性代数课中曾经讲到如何求特征向量什么的，现在用到了吧，忘了吧？呵呵^_^<br>svd(singular value decomposition)表示奇异值分解，svd(matlab奇异值分界的库函数,eig(Sigma)也有同样功能)将输出三个矩阵，分别是U、S、V，真正需要的是U矩阵，U矩阵也是个n×n矩阵。如果我们想将数据降维到k-dimensions的话，我们只需提取前k列向量，也就是用来投影数据的k个方向。 <img src="/media/14946971878921.png"> <span class="math">\(U_{reduce}\)</span>指U取前k列得到的n×k矩阵，X是训练集中的样本或者交叉训练集中的样本或者是测试集样本，然后Z矩阵的表达，是使用Z=<span class="math">\(U_{reduce}^TX\)</span>，也就是k×n矩阵和n×1矩阵相乘，所以Z就是k维的向量。 <img src="/media/14946981115876.jpg"></p>
<p>总结一下PAC的全过程，如下图所示。 <img src="/media/14946983615287.jpg"></p>
<h1 id="还原压缩数据">还原压缩数据</h1>
<p>压缩数据时，我们也许会把一千维的数据压缩到只有一百个维度，既然我们可以用算法如此压缩数据，那么也应该有办法，可以从压缩过的数据，近似地回到原始高维度的数据。假设有一个已经被压缩过的<span class="math">\(z^{(i)}\)</span>，它有100维度，怎样使它回到其最初的表示<span class="math">\(x^{(i)}\)</span>，也就是压缩前的1000维的数据呢？</p>
<p>呵呵，这一块先不写了。以后再补上。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Dimensionality Reduction-维数约减，一般是为了数据压缩，减少内存和硬盘空间存储，并提高算法速度。在特征降维的方法中，PCA(Principle Component Analysis)是最为经典和实用的特征降维技术，特别是在辅助图像识别方面有突出表
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Anomaly Detection 反常检测</title>
    <link href="http://niuoo.github.io/2017/05/05/Anomaly-Detection-%E5%8F%8D%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://niuoo.github.io/2017/05/05/Anomaly-Detection-反常检测/</id>
    <published>2017-05-05T15:03:49.000Z</published>
    <updated>2017-05-19T06:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>异常检测最常用的应用是欺诈检测。系统的用户都在从事不同的活动，可以对不同的用户活动计算特征变量，然后可以建立一个模型，用来表示用户表现出各种行为的可能性，也就是用户行为对应的特征向量出现的频率。</p>
<h4 id="异常检测例子">异常检测例子</h4>
<ul>
<li><p>某个用户在网站上行为的特征变量，也许<span class="math">\(x_1\)</span>是用户登陆的频率，<span class="math">\(x_2\)</span>是用户访问某个页面的次数或者交易次数，<span class="math">\(x_3\)</span>是用户在论坛上发帖的次数，<span class="math">\(x_4\)</span>是用户的打字次数，有些网站是可以记录用户每秒打了多少个字母的，因此可以根据这些数据建一个模型p(x)，可以用它来发现网站上行为奇怪的用户，只需要看哪些用户的p(x)概率小于ε，接下来，你拿来这些用户的档案，做进一步的筛选，或者要求这些用户，验证他们的身份，从而让网站可防御异常行为或者欺诈行为。这样的过程，可以找到行为不正常的用户，而不只是有欺诈行为的用户，也不只是那些被盗号的用户或者有行为比较搞笑的用户，而是行为不寻常的用户。这就是许多在线购物网站，常用来识别异常用户的技术。这些用户行为奇怪，可能表示他们有欺诈行为，或者是被盗号。</p></li>
<li><p>工业生产领域，发现异常的产品，然后要求进一步细查这些产品的质量。比如飞机引擎异常检测。</p></li>
<li><p>数据中心的计算机监控。假如，管理一个计算机集群，或者一个数据中心，其中有许多计算机，那么我们可以为每台计算机计算特征变量，也许某些特征衡量计算机的内存消耗，或者硬盘访问量，CPU负载，或者更加复杂的特征，比如CPU负载于网络流量的比值。建立p(x)模型，找到异常情况不正常工作的计算机，也许它即将停机，因此可以有求系统管理员查看其工作状况。目前这种技术实际正在被各大数据中心使用，用来检测大量计算机可能发生的异常。 <img src="/media/14948382044224.jpg"></p></li>
</ul>
<h3 id="高斯分布-gaussian-normal-distribution">高斯分布 Gaussian (Normal) Distribution</h3>
<p>高斯分布也称为正态分布。μ是均值，代表钟形曲线的对称轴，σ标准差确定了高斯分布概率密度函数的宽度，<span class="math">\(σ^2\)</span>表示方差。 <img src="/media/14948391530737.jpg"></p>
<p>高斯分布的例子，曲线的阴影部分面积是1，这是概率密度函数的特性。注意看高度和宽度和对称轴。 <img src="/media/14948397775893.jpg"> 参数估计问题。其实这里也是极大似然估计。 注意看下图中对μ和<span class="math">\(σ^2\)</span>的确定。 <img src="/media/14948406033181.jpg"></p>
<h3 id="异常检测算法">异常检测算法</h3>
<p>这里运用高斯分布开发异常检测算法。步骤如图所示：</p>
<ol style="list-style-type: decimal">
<li>首先选择特征，找出可能可以看出他们的反常和欺诈行为的特征<span class="math">\(x_i\)</span>，这个特征值或者过大或者过小，尽可能找出那些能够描述数据相关的属性特征；</li>
<li>计算均值和方差；</li>
<li>计算p(x)，如果p(x)&lt;ε,则定为异常。 <img src="/media/14948421441058.jpg"> 如下图，分布在桃红色区域的样本，就是异常的样本。<br><img src="/media/14948440235452.jpg"></li>
</ol>
<h3 id="分配样本数据">分配样本数据</h3>
<p>如果有10000个好的正常的引擎，20个异常的引擎，那么会分配6000个好的引擎作为训练集，2000好的引擎和10个不正常的引擎作为交叉验证集，剩下的2000好的引擎和10个不正常的引擎作为测试集。一般是6:2:2作为Training set:CV:Test<br><img src="/media/14948623383598.jpg"></p>
<h3 id="如何评估异常检测系统">如何评估异常检测系统</h3>
<p>对于异常检测系统，分类准确率不是一个好的评估度量方式。因为如果出现数据非常偏斜，异常数据非常少，我们直接预测所有样本都是正常的，那么这时候准确率还是很高的，但是显然这不是一个好的做法。那么用什么评价度量好呢？取而代之的是，我们应该算出真阳性、假阳性、假阴性和真阴性的比率。我们也可以算出查准率与召回率的比值；或者算出<span class="math">\(F_1-\)</span>积分，通过一个很简单的数字来表现出查准和召回的大小。通过这些方法，就可以较公平地评价异常检测算法在交叉验证和测试集样本中的表现。<br> 那么如何确定阈值ε呢？可以试试多个不同的ε取值，然后选出一个使得<span class="math">\(F_1-Score\)</span>值最大的那个ε，也就是在交叉验证集中表现最好的ε。<br><img src="/media/14948651051920.jpg"></p>
<h3 id="anomaly-detection-vs.supervised-learning">Anomaly detection vs. supervised learning</h3>
<p>那么我们什么时候应该用异常检测，什么时候用监督学习分类算法呢？在异常检测算法中，我们只有一小撮正(异常)样本，因此算法不可能从这些正样本中学出太多东西，因此取而代之的是我们使用一组大量的负(正常)样本，这样样本就能学到更多，或者能从大量的负样本中学出p(x)模型，另外会预留一小部分正样本来评价算法，既用于交叉验证集，也用于测试集。而对于垃圾邮件的样本，我们能得到绝大多数不同类型的垃圾邮件，因为我们有大量的垃圾邮件样本的集合，这就是为什么我们通常把垃圾邮件问题看作是监督学习问题的原因。<br>请看下图： <img src="/media/14949132950585.jpg"></p>
<h2 id="关于特征变量的选择">关于特征变量的选择</h2>
<p>对于异常检测算法效率影响最大的因素之一是使用什么特征变量，选择什么特征变量来输入异常检测算法。</p>
<h4 id="处理不符合高斯分布的特征">处理不符合高斯分布的特征</h4>
<p>在MATLAB中绘制直方图的函数是hist，如果画出来的柱状图近似像高斯分布，那么就可以很放心地把它们送入学习算法了，但如果画出来的图像如下图中的左下角图像，分布很不对称，峰值非常偏向一边，类似高斯分布图像的右半边，通常直接这样使用数据，算法也会运行很好，但是如果使用一些方法使得数据更像高斯分布的话，算法会工作的更好。对于左下角图像，就可以进行取对数log(x)的转换，结果就很像高斯分布了。<br><img src="/media/14949358396911.jpg"></p>
<h4 id="如何得到异常检测的特征变量">如何得到异常检测的特征变量</h4>
<p>通过误差分析步骤。如果最终表现不理想，对于很多正样本和负样本都有很大的p(x)的值，那么我们最好还是多引入其它的特征，以便于更好的区分出正样本和负样本。 <img src="/media/14949381932356.jpg"></p>
<h2 id="多元高斯分布-multivariate-gaussian-distribution">多元高斯分布 Multivariate Gaussian Distribution</h2>
<p><img src="/media/14949427576171.jpg"> 多元高斯分布模型和原始高斯分布模型的关系： <img src="/media/14949431790982.jpg"></p>
<h3 id="高斯分布模型-vs-多元高斯分布">高斯分布模型 VS 多元高斯分布</h3>
<p>原始的高斯分布模型使用的更多，但是多元高斯分布可以自动捕获不同特征变量之间的相关性。但是原始模型也有其他很重要的优势，一个很重要的优势，就是运算量很小，如果n的值非常大，也就是说特征变量很多的情况，即使n=100,000，原始模型都可以很好的运行。但是对于多元模型，计算∑的逆矩阵，∑是个n*n的矩阵，也就是100,000乘100,000的矩阵，那么这个计算量会非常大，所以多元模型不是n很大的情况。另外，对于原始的模型，训练集很小也就是m相对小的情况下，它也能运行的还可以，但是多元模型必须要求m&gt;n，样本的数量要大于特征变量的数量，因为如果m小于等于n，那么∑矩阵是不可逆的，是奇异矩阵，这种情况下，不能使用多元模型。一个合理的经验法则，m大于等于n的10倍的时候，再使用高斯模型。<br> 一般情况下，原始模型比较常用，如果需要捕捉特征变量之间的相关性，一般人都会手动增加这样的额外特征变量，来捕捉特定的不正常的值的组合，但是在训练集m很大，n不太大的情况，那么多元高斯模型是值得考虑的，或许可以运行得更好，还可以帮忙省去为了捕捉不正常的特征值组合而手动建立额外特征变量所花费的时间。<br> 另外，如果协方差矩阵∑是不可逆的，奇异的，那么一般只有两种情况:1.没有满足m&gt;n；2.有冗余的特征变量，就是不小心把一个特征变量复制了两份，或者是高度冗余的特征变量，如<span class="math">\(x_3=x_4+x_5\)</span>，那么<span class="math">\(x_3\)</span>就不含有额外的信息，也是冗余。所谓冗余的特征变量，也就是线性相关的特征变量。 <img src="/media/14950095213212.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;异常检测最常用的应用是欺诈检测。系统的用户都在从事不同的活动，可以对不同的用户活动计算特征变量，然后可以建立一个模型，用来表示用户表现出各种行为的可能性，也就是用户行为对应的特征向量出现的频率。&lt;/p&gt;
&lt;h4 id=&quot;异常检测例子&quot;&gt;异常检测例子&lt;/h4&gt;
&lt;ul&gt;
&lt;l
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>无监督式学习之K-means</title>
    <link href="http://niuoo.github.io/2017/05/05/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://niuoo.github.io/2017/05/05/无监督式学习/</id>
    <published>2017-05-05T15:03:18.000Z</published>
    <updated>2017-06-05T09:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>无监督学习算法，就是从未标记的数据中进行学习。 <img src="/media/14944286743216.jpg"></p>
<ul>
<li>组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。</li>
<li>社交网络里面也有，区分哪些用户之间是很亲密，哪些用户之间仅仅是认识。</li>
<li>根据商业系统中的数据，细分市场，然后把客户再分到不同的细分市场中。</li>
<li>分析星云的状态。</li>
</ul>
<h1 id="聚类问题clustering-k-means-algorithm">聚类问题(Clustering) K-means Algorithm</h1>
<p>有一堆无标签的数据，K-means能够自动的把这些数据分成有紧密关系的子集或者簇，是现在最为广泛运用的聚类方法。</p>
<ul>
<li><p>首先，要确定分类需要几个簇，也就是K的大小。 <img src="/media/14944822633878.jpg"></p></li>
<li><p>随机初始化K个簇中心，分别是<span class="math">\(μ_1,μ_2,…,μ_K ∈ R^n\)</span>(n是特征向量的纬度)。 <img src="/media/14944860345126.jpg"></p></li>
<li>不断重复一下步骤：
<ol style="list-style-type: decimal">
<li>找出数据<span class="math">\(x^{(i)}离哪个簇中心最近，用c^{(i)}记录簇中心的索引，c^{(i)}= min_k||x^{(i)}-μ^{(k)}||^2\)</span>。m是训练数据的总个数。<span class="math">\(c^{(i)}\)</span> = index of cluster(1,2,…,k) to which example <span class="math">\(x^{(i)}\)</span>is currently assigned。这一步就是把m个<span class="math">\(x^{(i)}\)</span>划分给各自所属的聚类中心。</li>
<li>根据当前分开的簇，再重新计算每个簇的簇中心。例如 <span class="math">\(c^{(1)}=2,c^{(5)}=2,c^{(6)}=2,c^{(10)}=2\)</span>，那么<span class="math">\(μ_2=\frac{1}{4}[x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)}]\)</span>。如果某个簇里面没有分配任何1个点，那么就把这个簇中心移除掉，或者重新随机找一个聚类中心，但是直接移除是更为常见的方法。</li>
<li>我们优化的目标是找到最好的<span class="math">\(μ_1,…μ_K\)</span>,使得<span class="math">\(J(c^{(1)},…,c^{(m)},μ_1,…μ_K)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-μ_{c^{(i)}}||^2\)</span>取得最小值，同时J是一直收敛的。 <img src="/media/14944823083506.jpg"></li>
</ol></li>
</ul>
<h4 id="局部最优解">局部最优解</h4>
<p>另外，局部最优解是可能产生的，有时候2个聚类中心会在一起，卡在了局部最优。如下图所示，右下角的2个函数图像，表示了2种不同的局部最优。 <img src="/media/14944905514428.jpg"> #### 解决局部最优 为了让K-means方法找到较好的局部最优解或者全局最优解，我们可以尝试多次随机的初始化来保证我们最终能得到一个足够好的结果，而不仅仅初始化一次K-means，就希望得到很好的结果。特别是K处于2到10之间的话，聚类数相对较小的体系里，多次随机初始化效果会非常好，会有较大的影响。但是如果K的值几百上千的话，很可能初次随机初始化就得到很好的结果，多次随机初始化也许会得到稍微好一点的结果，但是不会好太多。 <img src="/media/14944862061717.jpg"></p>
<p>看下图的右边，是一个市场细分的例子，根据数据，将市场分为3个部分，然后区别对待三类不同的顾客群体，更好的适应他们不同的需求，为大中小号3种聚类的用户，设计更合身的S,M,L尺码的衣服。 <img src="/media/14944871641347.jpg"></p>
<h4 id="关于聚类数目k的选取">关于聚类数目K的选取</h4>
<p>关于K应该取什么值，这个问题没有非常标准的解答，或者能自动解决他的方法。目前用来决定聚类数目最常用的方法仍然是通过看可视化的图，或者看聚类算法的输出结果，或者通过其他一切东西来手动选择聚类的数目。嘿嘿，就通过洞察力决定呗。 一般情况下使用肘部法则，但是不用期待表现很好。更多情况下，选择聚类数目的更好方法是，去问一下运行K-means是为了什么目的，然后想想聚类的数目是多少，才适合K-means聚类的后续目的。</p>
<ol style="list-style-type: decimal">
<li><p>肘部法则(Elbow method) 在选择聚类数目K的时候，我们可以使用下肘部方法，如果图像如下图左边的曲线，折点非常明显，那么选择肘点K是个很好的方式，但是如果图像如下图右边所示，那么选取哪个数目，则看起来非常困难。肘部方式，我们可以使用，但是很多时候，往往得不到左图那种有个明显的折点的情况，所以此方式值得尝试，但是我们也不要太期待。 <img src="/media/14944940159311.jpg"></p></li>
<li><p>看不同的聚类数量能为后续下游的目的提供多好的结果。从生意的角度来选择聚类数量，如下图，生产5种大小的T恤可以更加适合顾客，但是3种T恤的话，公司也可以降低成本，更便宜的卖给更多的顾客，因此T恤销售业务的观点，可能会提供一个决定采用3个类还是5个类的方法。 <img src="/media/14944958853442.jpg"></p></li>
</ol>
<h3 id="使用k-means进行图像压缩">使用K-means进行图像压缩</h3>
<p><img src="/media/14966526213973.jpg"> 以上图片是一个使用K-means方法压缩图片的例子。原始图片为128×128像素，每个像素是24位bit长度，每8位表示红绿蓝(red,green,blue)的强度，即RGB编码。原始图片有数千种颜色，现在我们需要将颜色减少到16种。原始图片的存储空间为128×128×24 = 393,216 bits，经过压缩后，所需的存储空间为16×24 + 128×128×4 = 65,920 bits。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;无监督学习算法，就是从未标记的数据中进行学习。 &lt;img src=&quot;/media/14944286743216.jpg&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。&lt;/li&gt;
&lt;li
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Support Vector Machines</title>
    <link href="http://niuoo.github.io/2017/05/05/Support-Vector-Machines/"/>
    <id>http://niuoo.github.io/2017/05/05/Support-Vector-Machines/</id>
    <published>2017-05-05T15:02:21.000Z</published>
    <updated>2017-07-24T06:56:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><a href="http://blog.csdn.net/sealyao/article/details/6442403" target="_blank" rel="external">SVM中的数学和算法</a></strong> 关于支持向量机，这里有<a href="https://www.zhihu.com/question/21094489" target="_blank" rel="external">一个比较有意思的讲解</a>，还有这里<a href="https://www.youtube.com/watch?v=3liCbRZPrZA" target="_blank" rel="external">一个小视频</a>。<br>SVM是数据挖掘里面很重要的算法，我们有时会把SVM看做是一个大间距分类器Large Magin Intuition。SVM Decision Boundary具有鲁棒性，因为它努力用一个最大间距，来分离样本。<br>有个小的知识点，向量内积以及<a href="http://www.cnblogs.com/vive/p/4563803.html" target="_blank" rel="external">证明</a></p>
<h1 id="线性核函数">线性核函数</h1>
<p>SVM线性核函数，就是SVM不使用核函数。其实是在逻辑回归的基础上进行稍微变化而成的。<br><img src="/media/14943143749438.jpg"></p>
<p>使用SVM软件包来确定参数θ的时候，我们需要指定常数C，C值如果太大，那么就会过拟合，效果不好。如以下示图，注意看左下角那个红叉叉，如果C太大的时候，决策边界就会是斜着的那条很线，这样并不好: <img src="/media/14943142227688.jpg"></p>
<h1 id="kernels">Kernels</h1>
<p>构造非线性复杂的分类器，我们用“Kernels函数”来达到此目的。<br>先来看一张图，如果决策分界是曲线的时候，那么就会有多项式特征变量的出现，如果不进行变化，就直接去求解，那么运算量是非常大的，有太多的高阶项需要被计算，所以我们需要通过其他方式来构造特征变量，来嵌入到假设函数中：<br><img src="/media/14943156955239.jpg"></p>
<p>Kernels核函数有很多，我们用的最多的是高斯核函数(Gaussian kernel)，$ f_i=exp{ }$ 其中 <span class="math">\(l^(i) =x^(i) ，x^(i)\)</span>表示m个训练数据的第i个样本，高斯核函数描述了某个样本和其他样本的距离程度。这个函数类似高斯分布，因此称为高斯核函数。也叫做径向基函数(Radial Basis Function 简称RBF)。它能够把原始特征映射到无穷维。 <img src="/media/14943147936462.jpg"></p>
<p>看下面这张图，可以更加理解高斯核函数的作用，注意看图中的红色大圈，如果<span class="math">\(f_1≈1,f_2≈0,f_3≈0\)</span>，那么证明这个点离<span class="math">\(l^(1)\)</span>很近，y=1。图中训练出的θ值，表明了红圈内是预测y=1，红圈外是y=0，即离点<span class="math">\(l^(1),l^(2)\)</span>都很远。这就是我们如何通过标记点以及核函数，来训练出非常复杂的非线性决策边界的方法。</p>
<div class="figure">
<img src="/media/14943222658500.jpg">

</div>
<p>高斯核函数的损失函数如下图所示，不过这些我们自己平时直接使用优化好的SVM包，并不需要关心这些细节，这些都是内部优化好的，不需要来定义。 <img src="/media/14943195312828.jpg"></p>
<p>在使用高斯核函数的时候，我们需要选择<span class="math">\(σ^2\)</span>，如果<span class="math">\(σ^2\)</span>偏大我们就会得到一个较大误差较低方差的分类器，高斯核函数返回的是(0,1)区间的实数。另外，在使用核函数的时候，进行数据归一化也是很必要的，因为如果不进行归一化，那么值比较大的特征向量会占据很大的地位，弱化其他特征向量的影响，这是不公平的。 <img src="/media/14943198501799.jpg"></p>
<p>支持向量机算法的核函数必须满足莫塞尔定理“Mercer’s Theorem”。对于其他的核函数，此处不做过多介绍。这里有个<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988406.html" target="_blank" rel="external">核函数的介绍</a></p>
<h1 id="如何选择使用何种算法呢">如何选择使用何种算法呢</h1>
<ol style="list-style-type: decimal">
<li>如果特征向量的个数很大,而训练集很小时，我们通常使用逻辑回归，或者使用SVM线性核函数。因为没有足够的数据来拟合非常复杂的非线性函数。</li>
<li>如果特征向量很小，而训练数据量是中等大小，那么核函数表现就可以很好。<br></li>
<li>如果如果特征向量很小，而训练数据量巨大，那么高斯核函数就会运行很慢，这种情况下，可以尝试手动建立更多的特征变量，然后使用逻辑回归或者SVM线性核函数。</li>
</ol>
<div class="figure">
<img src="/media/14943200907676.jpg">

</div>
<p>以上，第2种，例如，特征向量为1000左右，训练集是10,000的情况下，高斯函数的支持向量机会表现的非常突出。另外第1种和第3种情况，其实使用逻辑回归和SVM线性核函数效果都差不多。<br>SVM是凸优化的，所以局部最优，就是全局最优了。</p>
<p>在使用SVM软件包的时候，需要我们自己按照需求，配置以下参数：<br><img src="/media/14943202565702.jpg"></p>
<p>多种分类时：<br><img src="/media/14943205457069.jpg"></p>
<p>其实呢这课，我没怎么看懂，向量内积怎么使用我也不知道，SVM算法怎么使用倒是有个模糊的印象，但是具体内部的算法原理，没搞太明白，不知道最后的决策边界怎么产生的最大边距。这篇SVM我写的好吃力啊，重新刷斯坦福视频的时候居然看不懂了，真是……，不过还好最后看的自己觉得明白了，就先记录下来。吃力。</p>
<p>练习题中，垃圾邮件分类器，采用SVM线性核函数。通过训练集，训练出字典表中每个单词的权重(就是θ参数)。进行垃圾邮件分类时，是需要对邮件内容进行预处理的，比如网址啊，钱啊，之类的，都会用一个单词或者符号代替，出现的哪些单词(当然最后都向量化数字处理了，类似于x = [ 0 0 0 0 1 0 0 0 … 0 0 0 0 1 … 0 0 0 1 0 ..]，x的长度是字典长度，数据1表示出现了这个单词)，最后带入我们的式子<span class="math">\(θ^TX\)</span>里面，可进行是否垃圾邮件的预测，如果大于等于0，就是垃圾邮件，小于0，是正常邮件。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://blog.csdn.net/sealyao/article/details/6442403&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SVM中的数学和算法&lt;/a&gt;&lt;/strong&gt; 关于支持向量机，这里有
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络初探</title>
    <link href="http://niuoo.github.io/2017/05/04/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/"/>
    <id>http://niuoo.github.io/2017/05/04/神经网络初探/</id>
    <published>2017-05-04T06:14:20.000Z</published>
    <updated>2017-08-08T13:37:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络是当今最强大的学习算法之一，自动驾驶对周围景物的识别，就用到了神经网络。<br>我们以下讲述，神经网络在分类问题中的应用，在给定训练集下，为神经网络拟合参数的学习方法。<br><span class="math">\(\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}\)</span><br>在整个神经网络算法过程中，我们的最终目标是找到最佳的Θ矩阵中的权值。<br><img src="/media/14943440970717.jpg"></p>
<h1 id="神经网络的损失函数">神经网络的损失函数</h1>
<p>其中逻辑回归的损失函数为：<br><span class="math">\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\)</span><br>神经网络的损失函数在逻辑回归的基础上，稍微复杂了一点儿：<br><span class="math">\(\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\)</span></p>
<ul>
<li>L = 神经网络的层数。<br></li>
<li><span class="math">\(s_l\)</span> = 在第l层，神经单元的个数(不包括偏移量单元)</li>
<li>K = 神经网络输出层的单元个数。就是分类的类别总数。数字识别的话就是10.</li>
</ul>
<p>在Θ矩阵中，列代表当前层的单元个数，包括偏移单元；行代表下一层的神经元个数，不包含偏移单元。</p>
<p>注意：</p>
<ul>
<li>双和嵌套，只是简单的把输出层每个分类的逻辑回归损失加在一起了。m代表训练集的个数。</li>
<li>三个和嵌套的，是把神经网络里面，所有Θ矩阵的元素平方加在一起了。</li>
<li>i在三和嵌套中，代表的不是训练样本个数，而是代表当前层的神经元个数。</li>
</ul>
<h1 id="反向传播算法-backpropagation-algorithm">反向传播算法 Backpropagation Algorithm</h1>
<p>Back propagation的本质就是复合函数求导（following the chain rule）链式法则，本可以对这个网络里的每一个参数分别求偏导，但何苦呢，因为计算过程中的很多项都是重复的。为了不重复运算把把后层算好的导数传回前层，因为前层一定用得到。<br>关于反向传播算法，这里有个<a href="https://www.zhihu.com/question/27239198?rf=24827633" target="_blank" rel="external">浅显易懂的描述</a>。这里请看<a href="http://www.cnblogs.com/dengdan890730/p/5537451.html" target="_blank" rel="external">反向传播算法的推导过程</a>和<a href="http://blog.csdn.net/u014403897/article/details/46347351" target="_blank" rel="external">这里最后一层的求导</a>,这几个地方写的都不错。Andrew Ng老师的公开课对反向传播算法没有做过多的介绍，讲的也不够清晰。反向传播算法，就是为了计算损失函数的导数。<br>先贴个图片留作记号，便于下面Δ计算的理解</p>
<div class="figure">
<img src="/media/14939758124978.jpg">

</div>
<h2 id="反向传播算法的使用过程">反向传播算法的使用过程：</h2>
<ol style="list-style-type: decimal">
<li>Set <span class="math">\(a^{(1)} := x^{(t)}\)</span></li>
<li><p>正向计算每层的每个节点的<span class="math">\(a^{(l)}\)</span>for l=2,3,…,L</p>
<div class="figure">
<img src="/media/14939747131225.jpg">

</div></li>
<li>使用<span class="math">\(y^{(t)}，计算出\delta^{(L)} = a^{(L)} - y^{(t)}\)</span>，最后输出层的错误率。</li>
<li><p>使用<span class="math">\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</span>，计算出<span class="math">\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</span>。<br> 其中$g’(z^{(l)}) = a^{(l)} .* (1 - a^{(l)})；_j^{(l)} =  cost(t)； $</p></li>
</ol>
<p><span class="math">\(cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li><span class="math">\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}，或者向量化一下，\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</span>(Δ的初始化为0)<br> 至此，我们更新的权值累加器矩阵的为new Δ matrix，其中<span class="math">\(\frac \partial {\partial \Theta_{ij}^{(l)}} J(\Theta)=D_{ij}^{(l)}\)</span>
<ul>
<li><p><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</span> if j≠0.</p></li>
<li><p><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</span> If j=0</p></li>
</ul></li>
</ol>
<h3 id="梯度检验">梯度检验</h3>
<p>在神经网络中使用反向传播算法的时候，因为有很多细节，会导致各种各样小bug，即使J看起来每次都是下降的，但是，最终结果的误差却很大。因此梯度检验也是需要的过程，它减少这种错误的概率。在其他比较复杂的模型中使用梯度算法的时候，进行这种检查也是有意义的，这么做，将会对模型更加自信，确信模型是100%正确。<br><span class="math">\(\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}\)</span></p>
<p>在确定算法无误后，真正执行学习算法前，一定要关掉梯度检验，否则会很慢的哟。</p>
<h3 id="θ的初始化">Θ的初始化</h3>
<p>Θ初始化在逻辑回归的时候是可以为0的，但是在神经网络中不可以全部为0的。因为都为0的时候，隐藏层的结果都是一样的，这样做，隐藏层就完全是冗余的，神经网络就没作用了，完全就是个逻辑回归。所以我们需要打破对称，随机初始化。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络是当今最强大的学习算法之一，自动驾驶对周围景物的识别，就用到了神经网络。&lt;br&gt;我们以下讲述，神经网络在分类问题中的应用，在给定训练集下，为神经网络拟合参数的学习方法。&lt;br&gt;&lt;span class=&quot;math&quot;&gt;\(\begin{align*} a_1^{(2)}
    
    </summary>
    
    
  </entry>
  
</feed>
