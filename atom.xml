<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>niuoo</title>
  <subtitle>Develop with pleasure!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://niuoo.github.io/"/>
  <updated>2017-12-19T08:10:16.000Z</updated>
  <id>http://niuoo.github.io/</id>
  
  <author>
    <name>niuoo</name>
    <email>niuooo@yeah.net</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>iTerm 快捷键 shortcuts</title>
    <link href="http://niuoo.github.io/2017/12/19/iTerm-%E5%BF%AB%E6%8D%B7%E9%94%AE-shortcuts/"/>
    <id>http://niuoo.github.io/2017/12/19/iTerm-快捷键-shortcuts/</id>
    <published>2017-12-19T08:10:16.000Z</published>
    <updated>2017-12-19T08:10:16.000Z</updated>
    
    <content type="html"><![CDATA[
]]></content>
    
    <summary type="html">
    
      

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>tmux 快捷键 shortcuts</title>
    <link href="http://niuoo.github.io/2017/12/19/tmux-%E5%BF%AB%E6%8D%B7%E9%94%AE-shortcuts/"/>
    <id>http://niuoo.github.io/2017/12/19/tmux-快捷键-shortcuts/</id>
    <published>2017-12-19T08:09:41.000Z</published>
    <updated>2017-12-19T09:59:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>本来打算用一下tmux呢，后来发现还是mac OS下面还是直接iTerm方便，不过切到Linux的时候，估计还是要用tmux吧。 tmux使用也比较简单。先把脑海中的记录一下，防止以后忘记。感觉和vim的使用体验好像啊。我挺不喜欢这种设定。</p>
<h2 id="tmux安装">tmux安装</h2>
<p><code>brew install tmux</code></p>
<h2 id="新建一个tmux-session">新建一个tmux session</h2>
<table>
<thead>
<tr class="header">
<th align="left">命令</th>
<th align="left">描述</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">tmux new -s myname</td>
<td align="left">启动一个session</td>
</tr>
<tr class="even">
<td align="left">tmux a # (or at, or attach)</td>
<td align="left">恢复到已经存在的session number</td>
</tr>
<tr class="odd">
<td align="left">tmux a -t myname</td>
<td align="left">恢复到某个会话</td>
</tr>
<tr class="even">
<td align="left">tmux ls</td>
<td align="left">列出当前会话 list sessions</td>
</tr>
<tr class="odd">
<td align="left">tmux kill-session -t myname</td>
<td align="left">终止一个session. Kill session</td>
</tr>
</tbody>
</table>
<h2 id="切窗口-split-panes">切窗口 split panes</h2>
<p>Ctrl+b 等于是切到命令模式</p>
<pre>
Ctrl+b %        水平分割窗口  
Ctrl+b "        垂直分割窗口
Ctrl+b x        kill一个小窗口
Ctrl+b o        移动光标到不同的一个小窗口 Swap panes
ctrl+b d        跳出当前的tmux
</pre>
<h2 id="管理session">管理session</h2>
<pre>
Ctrl+B c      Create window
Ctrl+B w      List Windows
Ctrl+B n      Next window
Ctrl+B p      Previous window
Ctrl+B f      Find window
Ctrl+B ,      Name window
Ctrl+B &      Kill window
</pre>
<h2 id="关掉tmux">关掉tmux</h2>
<p><code>tmux kill-server</code></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本来打算用一下tmux呢，后来发现还是mac OS下面还是直接iTerm方便，不过切到Linux的时候，估计还是要用tmux吧。 tmux使用也比较简单。先把脑海中的记录一下，防止以后忘记。感觉和vim的使用体验好像啊。我挺不喜欢这种设定。&lt;/p&gt;
&lt;h2 id=&quot;tmux
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2017-12-17日记</title>
    <link href="http://niuoo.github.io/2017/12/17/2017-12-17%E6%97%A5%E8%AE%B0/"/>
    <id>http://niuoo.github.io/2017/12/17/2017-12-17日记/</id>
    <published>2017-12-17T05:08:26.000Z</published>
    <updated>2017-12-17T05:43:59.000Z</updated>
    
    <content type="html"><![CDATA[<p>14:00~17:00 看贝叶斯和论文 &lt;<a href="https://pdfs.semanticscholar.org/a5c4/35690c717d04801a68950f14036c38f2a9ab.pdf" target="_blank" rel="external">What is the expectation maximization algorithm?</a>&gt;</p>
<p>18:00~21:00 fast.ai 第二课看完。</p>
<p>加油！小宝贝</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;14:00~17:00 看贝叶斯和论文 &amp;lt;&lt;a href=&quot;https://pdfs.semanticscholar.org/a5c4/35690c717d04801a68950f14036c38f2a9ab.pdf&quot; target=&quot;_blank&quot; rel=&quot;ext
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>2017-12-16日记</title>
    <link href="http://niuoo.github.io/2017/12/17/2017-12-16%E6%97%A5%E8%AE%B0-1/"/>
    <id>http://niuoo.github.io/2017/12/17/2017-12-16日记-1/</id>
    <published>2017-12-16T20:21:53.000Z</published>
    <updated>2017-12-19T08:15:40.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="安排">2017-12-16安排</h1>
<p>14:00~17:00 看fast.ai 3个小时 我了个擦擦的，气死啦。搞了一下午的固定IP绑定之类的，最后也没搞好，还是用dns登录的。算了算了。明天继续看课程。√</p>
<p>17:00~17:30 玩儿半个小时。吃了羊肉抓饭。买的烤鸡翅，送来的是羊肉串。√</p>
<p>17:30~20:00 看<a href="https://mp.weixin.qq.com/s?__biz=MzIzMjU1NTg3Ng==&amp;mid=2247486415&amp;idx=1&amp;sn=b50f4cc8ef9cd4d4d34cbdb0d30b4afc&amp;chksm=e8925fc4dfe5d6d217d46bca560eb7030232c14183cf0b791aa9af7da5671ac536f8a45693f5&amp;mpshare=1&amp;scene=1&amp;srcid=1213nA8yTjxK7pw2LxnuSRGu&amp;key=da103cb67940783e381102fb14674ebd0d039d798c7b1932fa1ce5f48d91ad89962437c729f7545312a6ce1fa03df0a1640dbb145e25544cd7300fea7763d40720cd1adc84907bd0a8c3c52bc836bfe7&amp;ascene=0&amp;uin=MTA2MTM3MjcwNA%3D%3D&amp;devicetype=iMac+MacBookPro11%2C1+OSX+OSX+10.11.6+build(15G1510)&amp;version=12020810&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100&amp;pass_ticket=2%2B82FeLMFDTNJse7id71tUxAiI0JCuIWakI%2FV%2FKqOK8Veq9r%2FRy%2Bbxvfzgt2E1Cw" target="_blank" rel="external">Hinton和Jordan理解的EM算法</a></p>
<p>这篇文章我居然没读通，明天把不懂的地方请教下小伙伴。根据文章的引导，我想看看Daphne Koller的神书“Probabilistic Graphical Models: Principles and Techniques”，其中的第8章：The Exponential Family 和第19章 Partially Observed Data。 这两章几乎是Hinton对VBEM算法研究的高度浓缩。结果，我看全是英文作罢了。根据我看EM的过程，发现补理论知识，实在旷日持久。概率图模型，有机会成为下一个深度学习的理论之一。配套的视频，<a href="https://www.coursera.org/specializations/probabilistic-graphical-models" class="uri" target="_blank" rel="external">https://www.coursera.org/specializations/probabilistic-graphical-models</a></p>
<p>于是我做出个重大的决定，先不管理论了。直接fast.ai继续。硬着头皮把第一阶段的课程看完。然后再看看吴恩达deeplearning.ai课程。以后肯定会用到各种算法，到时候再慢慢理解。需要什么看什么。当然，拔苗助长也不好，但是可以先来一个成长周期短一点的种子，先果腹一下，别饿死最重要。</p>
<p>另外，Chuong B Do &amp; Serafim Batzoglou的Tutorial论文“What is the expectation maximization algorithm?”。明天下载下来读一下。目测这篇论文应该不难吧，我现在需要看简单明了容易理解的东西。这篇论文去哪里下载，我刚刚没找到。</p>
<p>20:00~22:00 <深度学习>看2小时。时间安排出问题了，这件事情没做。呵呵哒。</深度学习></p>
<p>22:00~23:00 <普林斯顿微积分>看1个小时，进入梦乡。也呵呵哒。</普林斯顿微积分></p>
<p>每做好一步，过来打勾勾哦!</p>
<p>加油，小宝贝！</p>
<p>发现自己现在话特别多，话多了，真正思考的时间就没了。但是又需要一定的表达，把想法作为日记写进博客里面，可以帮自己理清楚思路。不过缺点是，博客变得杂乱无章， 毕竟这个博客，最初的想法是作为技术博客的。</p>
<p>不过嘛，反正是自己的博客，随便DIY了。</p>
<p>好了，开始废话时间了。</p>
<p>这张照片好成熟。望接下来的时光，可以恢复活力。有心情，可以post自己的照片，记录下时间的流逝。希望自己以后的时光，可以身体健康，杨柳细腰，体态轻盈。期待岁月对我的温柔。 <img src="/media/15134233580413.jpg"></p>
<p>另外远离各类社交网络。社交网络固然人才济济，可是仅仅做个观众，可是有点傻兮兮的。参与进去，又会浪费很多时间。</p>
<p>慢慢丰富自己的生活，可以钻研深度学习，也可以学自己感兴趣的东西，学个乐器啊，舞蹈之类什么的都可以。当然了，慢慢跑起来啦，运动还是需要的啦。</p>
<p>前段时间，我没处理好自己的思想，以前觉得自己会坦然处之。后来发现，自己毫无缚鸡之力，就雄赳赳气昂昂去了角斗场，现在想来，自己真是傻乎乎的可爱，只不过最终，还是血淋淋的受伤了。</p>
<p>写诗，是个逻辑思维的过程，找出合理的词汇，表达一定的意思，用理智分析情感，即可得。</p>
<p>我能做的，就是把自己做好了，打理好了，有些许魅力特征，讨人喜欢，就够了。</p>
<p>自己要过得开心，快乐，做自己喜欢的事情。</p>
<p>另外，关于工作上的事情，前段时间，搞的回购客预测的二分类问题，最后使用了特征化成了woe，进入LR模型，然后理论上，最终权值应该都是正的。因为sogmoid是个单调递增函数，特征的值的woe如果是负的，那么证明这个特征值会造成负影响，不利于结果变成正的，那么应该拉低结果值，如果是正的，理想上，是应该提升结果，所以权值系数应该全部为正数。但是我们模型最终的结果，却很多都是负数，经过我的思考，我认为是因为我们的最终结果本身就是非常非常差的，当然不会理想的系数都是正数。很多特征，为了使结果尽可能的好，都被迫，系数被拉成负的了。。网上说，可能是出现了多重共线性，对于多重共线性怎么检验判断和解决，我暂时不知道。周一上班了，看看，筛选一下系数绝对值大的，或是只看正的特征，再做做实验看看。。</p>
<p>如果系数不能都为理想的正数，恰恰说明了一个问题，因为我们的数据，本身就不是线性的啊，不能很好的拟合成我们理想的线性，只能自己freestyle了。无奈啊。</p>
<p>人生也好无奈啊。我最近好无奈啊。</p>
<p>我的外形，我的工作，我的学识，我的斗志，都是人生谷底啊，谷底啊。我还能爬出来吗，我要的可不是梯度下降啊，目标不是最小值啊。</p>
<p>我的内心戏，好多啊！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;安排&quot;&gt;2017-12-16安排&lt;/h1&gt;
&lt;p&gt;14:00~17:00 看fast.ai 3个小时 我了个擦擦的，气死啦。搞了一下午的固定IP绑定之类的，最后也没搞好，还是用dns登录的。算了算了。明天继续看课程。√&lt;/p&gt;
&lt;p&gt;17:00~17:30 玩儿半
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Jdata特征工程分析</title>
    <link href="http://niuoo.github.io/2017/07/30/Jdata%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E5%88%86%E6%9E%90/"/>
    <id>http://niuoo.github.io/2017/07/30/Jdata特征工程分析/</id>
    <published>2017-07-29T16:15:05.000Z</published>
    <updated>2017-08-16T03:27:54.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据介绍">数据介绍</h1>
<p>符号定义： S：提供的商品全集； P：候选的商品子集（JData_Product.csv），P是S的子集； U：用户集合； A：用户对S的行为数据集合； C：S的评价数据。</p>
<p>训练数据部分： 提供2016-02-01到2016-04-15日用户集合U中的用户，对商品集合S中部分商品的行为、评价、用户数据；提供部分候选商品的数据P。 选手从数据中自行组成特征和数据格式，自由组合训练测试数据比例。</p>
<p>预测数据部分： 2016-04-16到2016-04-20用户是否下单P中的商品，每个用户只会下单一个商品；抽取部分下单用户数据，A榜使用50%的测试数据来计算分数；B榜使用另外50%的数据计算分数(计算准确率时剔除用户提交结果中user_Id与A榜的交集部分)。</p>
<p>为保护用户的隐私和数据安全，所有数据均已进行了采样和脱敏。 数据中部分列存在空值或NULL，请参赛者自行处理。</p>
<ol style="list-style-type: decimal">
<li><p>用户数据 user_id 用户ID 脱敏 age 年龄段 -1表示未知 sex 性别 0表示男，1表示女，2表示保密 user_lv_cd 用户等级 有顺序的级别枚举，越高级别数字越大 user_reg_tm 用户注册日期 粒度到天</p></li>
<li><p>商品数据 sku_id 商品编号 脱敏 a1 属性1 枚举，-1表示未知 a2 属性2 枚举，-1表示未知 a3 属性3 枚举，-1表示未知 cate 品类ID 脱敏 brand 品牌ID 脱敏</p></li>
<li><p>评价数据 dt 截止到时间 粒度到天 sku_id 商品编号 脱敏 comment_num 累计评论数分段 0表示无评论，1表示有1条评论， 2表示有2-10条评论， 3表示有11-50条评论， 4表示大于50条评论 has_bad_comment 是否有差评 0表示无，1表示有 bad_comment_rate 差评率 差评数占总评论数的比重</p></li>
<li><p>行为数据 user_id 用户编号 脱敏 sku_id 商品编号 脱敏 time 行为时间<br> model_id 点击模块编号，如果是点击 脱敏 type 1.浏览（指浏览商品详情页）； 2.加入购物车；3.购物车删除；4.下单；5.关注；6.点击</p></li>
</ol>
<p>cate 品类ID 脱敏 brand 品牌ID 脱敏 登录并报名参赛后方可在“数据下载”页面下载数据。</p>
<p>任务描述： 参赛者需要使用京东多个品类下商品的历史销售数据，构建算法模型，预测用户在未来5天内，对某个目标品类下商品的购买意向。对于训练集中出现的每一个用户，参赛者的模型需要预测该用户在未来5天内是否购买目标品类下的商品以及所购买商品的SKU_ID。评测算法将针对参赛者提交的预测结果，计算加权得分。</p>
<h1 id="section">。。。</h1>
<p>用于预测 2016-4-11&lt;=time1&lt;=2016-04-15 用于训练数据确定标签的值 2016-4-06&lt;=time2&lt;2016-04-11 用户构建特征 time3&lt;2016-04-06 features &lt;– ‘select user_id, sum(case when type=2 then 1 else 0 end) as add from JData_Action where time&lt;2016-4-06 group by user_id’</p>
<p>-Xms1024M -Xmx4096M -XX:PermSize=1024M -XX:MaxNewSize=2048M -XX:MaxPermSize=4096M -Xss8192M -XX:StackShadowPages=50</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;数据介绍&quot;&gt;数据介绍&lt;/h1&gt;
&lt;p&gt;符号定义： S：提供的商品全集； P：候选的商品子集（JData_Product.csv），P是S的子集； U：用户集合； A：用户对S的行为数据集合； C：S的评价数据。&lt;/p&gt;
&lt;p&gt;训练数据部分： 提供2016-02-0
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>召回率</title>
    <link href="http://niuoo.github.io/2017/07/12/%E5%8F%AC%E5%9B%9E%E7%8E%87/"/>
    <id>http://niuoo.github.io/2017/07/12/召回率/</id>
    <published>2017-07-12T14:01:59.000Z</published>
    <updated>2017-07-12T14:05:59.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/media/14998682171580.jpg"> <img src="/media/14998682636712.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;/media/14998682171580.jpg&quot;&gt; &lt;img src=&quot;/media/14998682636712.jpg&quot;&gt;&lt;/p&gt;

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>林轩田机器学习基石笔记</title>
    <link href="http://niuoo.github.io/2017/06/06/%E6%9E%97%E8%BD%A9%E7%94%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3%E7%AC%94%E8%AE%B0/"/>
    <id>http://niuoo.github.io/2017/06/06/林轩田机器学习基石笔记/</id>
    <published>2017-06-06T08:36:53.000Z</published>
    <updated>2017-07-24T07:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/niuoo/machine-learning-foundations/tree/master/foundations" target="_blank" rel="external"><strong>我的课程作业源码github地址</strong></a></p>
<p><a href="https://www.douban.com/doulist/3381853/" target="_blank" rel="external">这位同学的笔记做的很好</a> <img src="/media/14966834620659.jpg"> 知错能改，善莫大焉！:-)</p>
<p><img src="/media/14998352577521.jpg"> <a href="https://www.douban.com/note/319669984/" target="_blank" rel="external">PLA(Perceptron Learning Algorithm)感知机算法有效性证明过程</a>PLA不断进行错误修正的算法，是可以停下来的。可是这个算法有个很大的缺点，因为此算法必须建立在假设线性可分的基础上。如果假设不成立的话，PLA根本跑得停不下来。一开始我们是不会知道PLA能不能停下来的，这个时候可采用Pocket Algorithm，做得还不错。 <img src="/media/14998653580014.jpg"></p>
<div class="figure">
<img src="/media/14999565852705.jpg">

</div>
<p>Hoeffding’s inequality Hoeffding不等式 <img src="/media/15002855220854.jpg"></p>
<p><img src="/media/15002612963279.jpg"> 看到这一点的时候颇为迷惑，B(3,3)=7，B(4,3)是通过复制B(3,3)的一部分得来的。那么为什么α(复制的部分)需要no shatter any 2，其实想想假如 α shatter 2了，也就是B(3,2)，那么复制后，加上<span class="math">\(x_4\)</span>总共8种情形，必然是shatter 3 了，因为<span class="math">\(8 = 2^3\)</span>。 所以，得到：B(4,3) = 2*a + b &lt;= B(3,3) + B(3,2). 对于任意N &gt; k, 利用上述思路，可以证明 B(N,k) &lt;= B(N-1, k) + B(N-1,k-1). 有了递推不等式，通过数学归纳法，可证明下面的Bounding Function (N &gt; k) : <img src="/media/15002711060448.jpg"></p>
<p>归纳法证明过程如下： <img src="/media/15002882090009.jpg"></p>
<p>为什么机器学习是有效的？ <img src="/media/15002851562508.jpg"></p>
<div class="figure">
<img src="/media/15008820248463.jpg">

</div>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/niuoo/machine-learning-foundations/tree/master/foundations&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;strong&gt;我的课程作业源码gi
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>做个机器学习Python调包侠</title>
    <link href="http://niuoo.github.io/2017/05/19/%E5%81%9A%E4%B8%AA%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0Python%E8%B0%83%E5%8C%85%E4%BE%A0/"/>
    <id>http://niuoo.github.io/2017/05/19/做个机器学习Python调包侠/</id>
    <published>2017-05-19T06:39:55.000Z</published>
    <updated>2017-06-06T08:45:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>算法</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;算法&lt;/p&gt;

    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Recommender Systems 推荐系统</title>
    <link href="http://niuoo.github.io/2017/05/14/Recommender-Systems-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>http://niuoo.github.io/2017/05/14/Recommender-Systems-推荐系统/</id>
    <published>2017-05-14T05:06:31.000Z</published>
    <updated>2017-06-05T10:04:45.000Z</updated>
    
    <content type="html"><![CDATA[<p>推荐系统是机器学习的一个重要应用。很多团队都致力于建立更好的推荐系统，比如亚马逊、Netflix、eBay或者苹果公司的iTunes Genius做的事情，有很多网站或者系统试图向用户推荐新产品，亚马逊向你推荐新书，Netflix想你推荐新电影，诸如此类。而这些推进系统可能会看看你以前购买过什么书，或者以前你给哪些电影进行过评分，这些系统贡献了现今亚马逊收入的相当大一部分，而Netflix，他们向用户推荐的电影占了用户观看电影的大部分。推荐系统，其表现的一些改进就能带来显著且即刻产生的影响，这些影响关系到许多公司的最终业绩。<br>在机器学习中，特征量是重要的，选择的特征对学习算法的表现有很大影响。在机器学习领域，有这么一个宏大的想法，就是对于一些问题，可能不是所有问题，但是对于一些问题而言，存在一些算法，能试图自动地学习到一组优良的特征量，而不用人为动手设计或者手动编写特征。<br>有些情况，或许能够采用一种算法，来学习到使用什么特征量，而推荐系统，就是这种情形的一个例子，当然还有其他很多例子。通过学习推荐系统，我们能够对这种学习特征量的想法有一点了解，我们至少可以通过这个例子，来了解机器学习中的这种big idea。</p>
<h1 id="推荐系统算法">推荐系统算法</h1>
<h3 id="推荐系统在做什么">推荐系统在做什么</h3>
<p>如下图所示，给定这些r(i,j)与y(i,j)数据，其中r(i,j)表示用户j给电影i是否打分，y(i,j)表示用户j给电影i所打的分数，然后浏览全部数据，关注所有没有电影评分的地方，并试图预测这些带问号的地方，应该是什么数值。如下图，前三部电影可能是浪漫爱情电影，后两部是动作片，然后Alice和Bob看起来喜欢爱情片，我们预测Alice也许会给电影Cute puppies of love打分5，Bob给电影Romance forever打分4.5，推荐系统就是通过填充这些数值，然后推测用户更喜欢什么电影，并进行相应推荐。有一些电影，还有一些用户，用户给一些电影进行了评价打分，推荐系统所做的事情就是，通过这些评分，预测用户会怎样给还没看过的电影打分。<br><img src="/media/14947442269252.jpg"></p>
<h2 id="基于内容的推荐-cotent-based-recommendations">基于内容的推荐 Cotent Based Recommendations</h2>
<p>注意看<span class="math">\(x^{(i)}\)</span>，它表示某部电影是浪漫和动作片程度的多少。算法的目的是学到每个用户喜爱浪漫电影和动作电影程度的变量<span class="math">\(θ^{(j)}\)</span>，然后使用<span class="math">\((θ^{(j)})^Tx^{(i)}\)</span>就可以预测第j个用户可能给第i部电影所打的分数。<br><img src="/media/14947615593109.jpg"></p>
<p>学习每个<span class="math">\(θ^{(j)}\)</span>的值是一个基本的线性回归问题，算法的目的是为了学习到<span class="math">\(θ^{(j)}\)</span>使得下面的损失函数的值最小。但是这个又和我们以前所学习的线性回归问题不同，因为在这里，每个用户的<span class="math">\(θ^{(j)}\)</span>值是不同的，每个用户都有单独属于自己的<span class="math">\(θ^{(j)}\)</span>值，这就使得个性化推荐成为现实。我们可以把对每个观众打分的预测，当成一个独立的线性回归问题。<br><img src="/media/14947649924645.jpg"></p>
<p>优化目标就是图上的公式，对于每一个用户，我们都进行计算。<span class="math">\(θ^{(n_u)}\)</span>表示第n个用户的θ值。<br><img src="/media/14947714065234.jpg"></p>
<p>梯度下降过程如下。 <img src="/media/14947721242572.jpg"> 以上，就是运用线性回归的变体，来预测不同用户对不同电影的评分值。这种做法，叫做『基于内容的推荐』，因为我们具有电影的特征量<span class="math">\(x^{(i)}\)</span>，来表示电影内容的属性，比如电影爱情的成分是多少，动作的成分是多少。但是实际上，我们很少知道所有电影的特征，或者我们要卖的产品有什么特征，所以接下来，我们介绍不具有这些内容特征时，该如何进行推荐。</p>
<h2 id="协同过滤-collaborative-filtering">协同过滤 Collaborative Filtering</h2>
<p>协同过滤，可以自行学习所要使用的特征。<br>如下图所示，假如用户告诉了他们的偏好，那么我们可以知道每个<span class="math">\(θ^{(j)}\)</span>的值，来学习到<span class="math">\(x^{(i)}\)</span>的值。<br><img src="/media/14947754778241.jpg"></p>
<p>根据用户的偏好，学习电影的特征指数，优化目标如下图所示：<br><img src="/media/14947761941215.jpg"></p>
<p>如下图所示，把二者合并起来，就是协同过滤了。根据每个用户对多部电影的评分，以及每部电影由不同用户的评分，可以反复进行这样的过程，来估计出θ和x。协同过滤算法指的是，当你执行这个算法时，你通过一大堆用得到数据，这些数据实际上在高效地进行了协同合作，来得到每个人对电影的评分值。只要用户对某几部电影进行评分，每个用户就都在帮助算法，更好的学习出特征，这些特征又可以被系统运用，为其他人做出更准确的电影预测。协同的另一层意思是，每位用户，都在帮助系统，学习出更好的特征，这就是协同过滤。<br><img src="/media/14947774794944.jpg"></p>
<h2 id="协同过滤算法-collaborative-filtering-algorithm">协同过滤算法 Collaborative Filtering Algorithm</h2>
<p>把上面讲的两个损失函数合并起来，就是我们希望优化的新的代价函数，新的代价函数是同时关于θ和x的函数，二者同时更新，最终是的J最小。<br><img src="/media/14947796300697.jpg"></p>
<p>协同算法步骤：</p>
<ul>
<li><p>首先将会把θ和x初始化为小的随机值，这有点儿像神经网络训练，所有神经网络的权值参数，我们也是用小的随机数值来初始化的。协同过滤的初始化和神经网络初始化一样，也需要打破对称symmetry breaking. <img src="/media/14947804720110.jpg"></p></li>
<li><p>使用梯度下降，或者其他的高级优化算法，把代价函数最小化，如果求导，梯度下降更新写出来的结果如图中第2项所示。通过梯度下降，我们同时更新θ和x的值。<br> <img src="/media/14947802773743.jpg"></p></li>
<li><p>根据以上求出的θ和x的值，我们就可以预测每一个用户了。</p></li>
</ul>
<p>以上就是协同过滤算法的具体过程。通过协同过滤算法，可以同时学习几乎所有电影的特征x，和所有用户的参数θ，然后有很大机会，能对不同用户会如何评价他们尚未评分的电影，做出相当准确的预测。</p>
<h3 id="低秩矩阵分解-low-rank-matrix-factorization">低秩矩阵分解 Low Rank Matrix Factorization</h3>
<p>通过向量化的计算，来对所有的用户和所有的电影，进行评分计算。 <img src="/media/14947819692395.jpg"></p>
<h3 id="向用户做出相关推荐">向用户做出相关推荐</h3>
<p>根据协同过滤算法，当给出一件产品时，可以找到与之相关的其它产品，再例如，一位用户最近看上一件产品，看有没有其它相关的产品，你可以推荐给他。<br>怎么给用户进行相关推荐呢？请看下图。用户曾给电影i打过高分，我们根据算法得到<span class="math">\(x^{(i)}\)</span>，电影i的特征指数，那么我们就找到距离<span class="math">\(x^{(i)}\)</span>最近的几个电影，推荐给用户。<span class="math">\(||x^{(i)}-x^{(j)}||\)</span>表示的是<span class="math">\(x^{(i)}与x^{(j)}\)</span>的欧式距离。<br><img src="/media/14947831732793.jpg"></p>
<h3 id="预处理步骤-均值归一化">预处理步骤-均值归一化</h3>
<p>求出每一个电影的平均分μ，然后每个打分减去该电影的平均分，每一行的总和其实还是0.我们这样做均值归一化了。当我们在预测具体某个用户对某个电影评分的时候，还是要把平均分μ加上的。对于某个用户从来没评论过任何电影，那么也默认他的评分是平均分水平。 <img src="/media/14947848429032.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;推荐系统是机器学习的一个重要应用。很多团队都致力于建立更好的推荐系统，比如亚马逊、Netflix、eBay或者苹果公司的iTunes Genius做的事情，有很多网站或者系统试图向用户推荐新产品，亚马逊向你推荐新书，Netflix想你推荐新电影，诸如此类。而这些推进系统可能
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Dimensionality Reduction 主成分分析PCA</title>
    <link href="http://niuoo.github.io/2017/05/14/Dimensionality-Reduction-%E7%BB%B4%E5%BA%A6%E7%BA%A6%E5%87%8F/"/>
    <id>http://niuoo.github.io/2017/05/14/Dimensionality-Reduction-维度约减/</id>
    <published>2017-05-13T16:30:13.000Z</published>
    <updated>2017-06-05T10:04:42.000Z</updated>
    
    <content type="html"><![CDATA[<p>使用Dimensionality Reduction-维数约减，一般是为了数据压缩，减少内存和硬盘空间存储，并提高算法速度。在特征降维的方法中，PCA(Principle Component Analysis)是最为经典和实用的特征降维技术，特别是在辅助图像识别方面有突出表现。 特征降维是无监督学习的另一个应用。降维/压缩问题是选取数据具有代表性的特征，在保持数据多样性(Variance)的基础上，规避掉大量的特征冗余和噪声，不过这个过程也很有可能会损失一些有用的模式信息。经过大量的实践证明，相较于损失的少部分模型性能，维度压缩可节省大量用于模型训练的时间，使得模型综合效率变得更高。</p>
<p>在实际项目中，我们会得到特征维度非常高的训练样本，有的特征向量之间是有关系的，比如预测房价的时候，得到的房子长度inches英寸和cm厘米，它们两个是线性相关的，所以有一个信息是冗余的，就可以去掉一个。又如下图所示，数据集中在一个平面Z附近，所以就可以把3D的数据降维成2D。 <img src="/media/14945865066705.jpg"></p>
<h1 id="主成分提取principal-component-analysis-problem-formulation">主成分提取(Principal Component Analysis problem formulation)</h1>
<p>对于降维问题，目前最流行的，最常用的算法就是PCA主成分分析法。 PCA做的就是，寻找一条直线，或者面，或者诸如此类，比起特征向量低维的空间，对数据进行投影，并且最小化投影距离，也就是数据点和投影后的点之间的距离。另外，在寻找vectors时，对于所有的数据集和特征向量，都是同等对待的。一般用投影的数据，当做被降维的数据使用。 <img src="/media/14946121526774.jpg"></p>
<p>关于上图的左边，注意区别PCA和线性回归，两个截然不同的概念。 <img src="/media/14946125078718.jpg"></p>
<h3 id="数据预处理-data-preprocessing">数据预处理 Data preprocessing</h3>
<p>在使用PCA之前，我们通常会有一个<strong>数据预处理</strong>的过程，就是对数据进行均值归一化。 <img src="/media/14946138068434.jpg"></p>
<h3 id="pac算法-principal-component-analysispca-algorithm">PAC算法 Principal Component Analysis(PCA) algorithm</h3>
<p>首先要做的是计算出∑协方差矩阵。<span class="math">\(∑=\frac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T\)</span>,∑(Sigma)是个n×n的矩阵，因为<span class="math">\(x^{(i)}\)</span>是个n×1的向量，<span class="math">\((x^{(i)})^T\)</span>是个1×n的向量，两个向量相乘就是n×n的矩阵了。另外，协方差均值∑(Sigma)总满足一个数学性质，称为对称正定(symmetric positive definite)。这块线性代数的知识不懂，不过不必介意，反正就是这么求的，代码也不长。记得线性代数课中曾经讲到如何求特征向量什么的，现在用到了吧，忘了吧？呵呵^_^<br>svd(singular value decomposition)表示奇异值分解，svd(matlab奇异值分界的库函数,eig(Sigma)也有同样功能)将输出三个矩阵，分别是U、S、V，真正需要的是U矩阵，U矩阵也是个n×n矩阵。如果我们想将数据降维到k-dimensions的话，我们只需提取前k列向量，也就是用来投影数据的k个方向。 <img src="/media/14946971878921.png"> <span class="math">\(U_{reduce}\)</span>指U取前k列得到的n×k矩阵，X是训练集中的样本或者交叉训练集中的样本或者是测试集样本，然后Z矩阵的表达，是使用Z=<span class="math">\(U_{reduce}^TX\)</span>，也就是k×n矩阵和n×1矩阵相乘，所以Z就是k维的向量。 <img src="/media/14946981115876.jpg"></p>
<p>总结一下PAC的全过程，如下图所示。 <img src="/media/14946983615287.jpg"></p>
<h1 id="还原压缩数据">还原压缩数据</h1>
<p>压缩数据时，我们也许会把一千维的数据压缩到只有一百个维度，既然我们可以用算法如此压缩数据，那么也应该有办法，可以从压缩过的数据，近似地回到原始高维度的数据。假设有一个已经被压缩过的<span class="math">\(z^{(i)}\)</span>，它有100维度，怎样使它回到其最初的表示<span class="math">\(x^{(i)}\)</span>，也就是压缩前的1000维的数据呢？</p>
<p>呵呵，这一块先不写了。以后再补上。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Dimensionality Reduction-维数约减，一般是为了数据压缩，减少内存和硬盘空间存储，并提高算法速度。在特征降维的方法中，PCA(Principle Component Analysis)是最为经典和实用的特征降维技术，特别是在辅助图像识别方面有突出表
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Anomaly Detection 反常检测</title>
    <link href="http://niuoo.github.io/2017/05/05/Anomaly-Detection-%E5%8F%8D%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
    <id>http://niuoo.github.io/2017/05/05/Anomaly-Detection-反常检测/</id>
    <published>2017-05-05T15:03:49.000Z</published>
    <updated>2017-05-19T06:33:14.000Z</updated>
    
    <content type="html"><![CDATA[<p>异常检测最常用的应用是欺诈检测。系统的用户都在从事不同的活动，可以对不同的用户活动计算特征变量，然后可以建立一个模型，用来表示用户表现出各种行为的可能性，也就是用户行为对应的特征向量出现的频率。</p>
<h4 id="异常检测例子">异常检测例子</h4>
<ul>
<li><p>某个用户在网站上行为的特征变量，也许<span class="math">\(x_1\)</span>是用户登陆的频率，<span class="math">\(x_2\)</span>是用户访问某个页面的次数或者交易次数，<span class="math">\(x_3\)</span>是用户在论坛上发帖的次数，<span class="math">\(x_4\)</span>是用户的打字次数，有些网站是可以记录用户每秒打了多少个字母的，因此可以根据这些数据建一个模型p(x)，可以用它来发现网站上行为奇怪的用户，只需要看哪些用户的p(x)概率小于ε，接下来，你拿来这些用户的档案，做进一步的筛选，或者要求这些用户，验证他们的身份，从而让网站可防御异常行为或者欺诈行为。这样的过程，可以找到行为不正常的用户，而不只是有欺诈行为的用户，也不只是那些被盗号的用户或者有行为比较搞笑的用户，而是行为不寻常的用户。这就是许多在线购物网站，常用来识别异常用户的技术。这些用户行为奇怪，可能表示他们有欺诈行为，或者是被盗号。</p></li>
<li><p>工业生产领域，发现异常的产品，然后要求进一步细查这些产品的质量。比如飞机引擎异常检测。</p></li>
<li><p>数据中心的计算机监控。假如，管理一个计算机集群，或者一个数据中心，其中有许多计算机，那么我们可以为每台计算机计算特征变量，也许某些特征衡量计算机的内存消耗，或者硬盘访问量，CPU负载，或者更加复杂的特征，比如CPU负载于网络流量的比值。建立p(x)模型，找到异常情况不正常工作的计算机，也许它即将停机，因此可以有求系统管理员查看其工作状况。目前这种技术实际正在被各大数据中心使用，用来检测大量计算机可能发生的异常。 <img src="/media/14948382044224.jpg"></p></li>
</ul>
<h3 id="高斯分布-gaussian-normal-distribution">高斯分布 Gaussian (Normal) Distribution</h3>
<p>高斯分布也称为正态分布。μ是均值，代表钟形曲线的对称轴，σ标准差确定了高斯分布概率密度函数的宽度，<span class="math">\(σ^2\)</span>表示方差。 <img src="/media/14948391530737.jpg"></p>
<p>高斯分布的例子，曲线的阴影部分面积是1，这是概率密度函数的特性。注意看高度和宽度和对称轴。 <img src="/media/14948397775893.jpg"> 参数估计问题。其实这里也是极大似然估计。 注意看下图中对μ和<span class="math">\(σ^2\)</span>的确定。 <img src="/media/14948406033181.jpg"></p>
<h3 id="异常检测算法">异常检测算法</h3>
<p>这里运用高斯分布开发异常检测算法。步骤如图所示：</p>
<ol style="list-style-type: decimal">
<li>首先选择特征，找出可能可以看出他们的反常和欺诈行为的特征<span class="math">\(x_i\)</span>，这个特征值或者过大或者过小，尽可能找出那些能够描述数据相关的属性特征；</li>
<li>计算均值和方差；</li>
<li>计算p(x)，如果p(x)&lt;ε,则定为异常。 <img src="/media/14948421441058.jpg"> 如下图，分布在桃红色区域的样本，就是异常的样本。<br><img src="/media/14948440235452.jpg"></li>
</ol>
<h3 id="分配样本数据">分配样本数据</h3>
<p>如果有10000个好的正常的引擎，20个异常的引擎，那么会分配6000个好的引擎作为训练集，2000好的引擎和10个不正常的引擎作为交叉验证集，剩下的2000好的引擎和10个不正常的引擎作为测试集。一般是6:2:2作为Training set:CV:Test<br><img src="/media/14948623383598.jpg"></p>
<h3 id="如何评估异常检测系统">如何评估异常检测系统</h3>
<p>对于异常检测系统，分类准确率不是一个好的评估度量方式。因为如果出现数据非常偏斜，异常数据非常少，我们直接预测所有样本都是正常的，那么这时候准确率还是很高的，但是显然这不是一个好的做法。那么用什么评价度量好呢？取而代之的是，我们应该算出真阳性、假阳性、假阴性和真阴性的比率。我们也可以算出查准率与召回率的比值；或者算出<span class="math">\(F_1-\)</span>积分，通过一个很简单的数字来表现出查准和召回的大小。通过这些方法，就可以较公平地评价异常检测算法在交叉验证和测试集样本中的表现。<br> 那么如何确定阈值ε呢？可以试试多个不同的ε取值，然后选出一个使得<span class="math">\(F_1-Score\)</span>值最大的那个ε，也就是在交叉验证集中表现最好的ε。<br><img src="/media/14948651051920.jpg"></p>
<h3 id="anomaly-detection-vs.supervised-learning">Anomaly detection vs. supervised learning</h3>
<p>那么我们什么时候应该用异常检测，什么时候用监督学习分类算法呢？在异常检测算法中，我们只有一小撮正(异常)样本，因此算法不可能从这些正样本中学出太多东西，因此取而代之的是我们使用一组大量的负(正常)样本，这样样本就能学到更多，或者能从大量的负样本中学出p(x)模型，另外会预留一小部分正样本来评价算法，既用于交叉验证集，也用于测试集。而对于垃圾邮件的样本，我们能得到绝大多数不同类型的垃圾邮件，因为我们有大量的垃圾邮件样本的集合，这就是为什么我们通常把垃圾邮件问题看作是监督学习问题的原因。<br>请看下图： <img src="/media/14949132950585.jpg"></p>
<h2 id="关于特征变量的选择">关于特征变量的选择</h2>
<p>对于异常检测算法效率影响最大的因素之一是使用什么特征变量，选择什么特征变量来输入异常检测算法。</p>
<h4 id="处理不符合高斯分布的特征">处理不符合高斯分布的特征</h4>
<p>在MATLAB中绘制直方图的函数是hist，如果画出来的柱状图近似像高斯分布，那么就可以很放心地把它们送入学习算法了，但如果画出来的图像如下图中的左下角图像，分布很不对称，峰值非常偏向一边，类似高斯分布图像的右半边，通常直接这样使用数据，算法也会运行很好，但是如果使用一些方法使得数据更像高斯分布的话，算法会工作的更好。对于左下角图像，就可以进行取对数log(x)的转换，结果就很像高斯分布了。<br><img src="/media/14949358396911.jpg"></p>
<h4 id="如何得到异常检测的特征变量">如何得到异常检测的特征变量</h4>
<p>通过误差分析步骤。如果最终表现不理想，对于很多正样本和负样本都有很大的p(x)的值，那么我们最好还是多引入其它的特征，以便于更好的区分出正样本和负样本。 <img src="/media/14949381932356.jpg"></p>
<h2 id="多元高斯分布-multivariate-gaussian-distribution">多元高斯分布 Multivariate Gaussian Distribution</h2>
<p><img src="/media/14949427576171.jpg"> 多元高斯分布模型和原始高斯分布模型的关系： <img src="/media/14949431790982.jpg"></p>
<h3 id="高斯分布模型-vs-多元高斯分布">高斯分布模型 VS 多元高斯分布</h3>
<p>原始的高斯分布模型使用的更多，但是多元高斯分布可以自动捕获不同特征变量之间的相关性。但是原始模型也有其他很重要的优势，一个很重要的优势，就是运算量很小，如果n的值非常大，也就是说特征变量很多的情况，即使n=100,000，原始模型都可以很好的运行。但是对于多元模型，计算∑的逆矩阵，∑是个n*n的矩阵，也就是100,000乘100,000的矩阵，那么这个计算量会非常大，所以多元模型不是n很大的情况。另外，对于原始的模型，训练集很小也就是m相对小的情况下，它也能运行的还可以，但是多元模型必须要求m&gt;n，样本的数量要大于特征变量的数量，因为如果m小于等于n，那么∑矩阵是不可逆的，是奇异矩阵，这种情况下，不能使用多元模型。一个合理的经验法则，m大于等于n的10倍的时候，再使用高斯模型。<br> 一般情况下，原始模型比较常用，如果需要捕捉特征变量之间的相关性，一般人都会手动增加这样的额外特征变量，来捕捉特定的不正常的值的组合，但是在训练集m很大，n不太大的情况，那么多元高斯模型是值得考虑的，或许可以运行得更好，还可以帮忙省去为了捕捉不正常的特征值组合而手动建立额外特征变量所花费的时间。<br> 另外，如果协方差矩阵∑是不可逆的，奇异的，那么一般只有两种情况:1.没有满足m&gt;n；2.有冗余的特征变量，就是不小心把一个特征变量复制了两份，或者是高度冗余的特征变量，如<span class="math">\(x_3=x_4+x_5\)</span>，那么<span class="math">\(x_3\)</span>就不含有额外的信息，也是冗余。所谓冗余的特征变量，也就是线性相关的特征变量。 <img src="/media/14950095213212.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;异常检测最常用的应用是欺诈检测。系统的用户都在从事不同的活动，可以对不同的用户活动计算特征变量，然后可以建立一个模型，用来表示用户表现出各种行为的可能性，也就是用户行为对应的特征向量出现的频率。&lt;/p&gt;
&lt;h4 id=&quot;异常检测例子&quot;&gt;异常检测例子&lt;/h4&gt;
&lt;ul&gt;
&lt;l
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>无监督式学习之K-means</title>
    <link href="http://niuoo.github.io/2017/05/05/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%BC%8F%E5%AD%A6%E4%B9%A0/"/>
    <id>http://niuoo.github.io/2017/05/05/无监督式学习/</id>
    <published>2017-05-05T15:03:18.000Z</published>
    <updated>2017-06-05T09:11:26.000Z</updated>
    
    <content type="html"><![CDATA[<p>无监督学习算法，就是从未标记的数据中进行学习。 <img src="/media/14944286743216.jpg"></p>
<ul>
<li>组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。</li>
<li>社交网络里面也有，区分哪些用户之间是很亲密，哪些用户之间仅仅是认识。</li>
<li>根据商业系统中的数据，细分市场，然后把客户再分到不同的细分市场中。</li>
<li>分析星云的状态。</li>
</ul>
<h1 id="聚类问题clustering-k-means-algorithm">聚类问题(Clustering) K-means Algorithm</h1>
<p>有一堆无标签的数据，K-means能够自动的把这些数据分成有紧密关系的子集或者簇，是现在最为广泛运用的聚类方法。</p>
<ul>
<li><p>首先，要确定分类需要几个簇，也就是K的大小。 <img src="/media/14944822633878.jpg"></p></li>
<li><p>随机初始化K个簇中心，分别是<span class="math">\(μ_1,μ_2,…,μ_K ∈ R^n\)</span>(n是特征向量的纬度)。 <img src="/media/14944860345126.jpg"></p></li>
<li>不断重复一下步骤：
<ol style="list-style-type: decimal">
<li>找出数据<span class="math">\(x^{(i)}离哪个簇中心最近，用c^{(i)}记录簇中心的索引，c^{(i)}= min_k||x^{(i)}-μ^{(k)}||^2\)</span>。m是训练数据的总个数。<span class="math">\(c^{(i)}\)</span> = index of cluster(1,2,…,k) to which example <span class="math">\(x^{(i)}\)</span>is currently assigned。这一步就是把m个<span class="math">\(x^{(i)}\)</span>划分给各自所属的聚类中心。</li>
<li>根据当前分开的簇，再重新计算每个簇的簇中心。例如 <span class="math">\(c^{(1)}=2,c^{(5)}=2,c^{(6)}=2,c^{(10)}=2\)</span>，那么<span class="math">\(μ_2=\frac{1}{4}[x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)}]\)</span>。如果某个簇里面没有分配任何1个点，那么就把这个簇中心移除掉，或者重新随机找一个聚类中心，但是直接移除是更为常见的方法。</li>
<li>我们优化的目标是找到最好的<span class="math">\(μ_1,…μ_K\)</span>,使得<span class="math">\(J(c^{(1)},…,c^{(m)},μ_1,…μ_K)=\frac{1}{m}\sum_{i=1}^m||x^{(i)}-μ_{c^{(i)}}||^2\)</span>取得最小值，同时J是一直收敛的。 <img src="/media/14944823083506.jpg"></li>
</ol></li>
</ul>
<h4 id="局部最优解">局部最优解</h4>
<p>另外，局部最优解是可能产生的，有时候2个聚类中心会在一起，卡在了局部最优。如下图所示，右下角的2个函数图像，表示了2种不同的局部最优。 <img src="/media/14944905514428.jpg"> #### 解决局部最优 为了让K-means方法找到较好的局部最优解或者全局最优解，我们可以尝试多次随机的初始化来保证我们最终能得到一个足够好的结果，而不仅仅初始化一次K-means，就希望得到很好的结果。特别是K处于2到10之间的话，聚类数相对较小的体系里，多次随机初始化效果会非常好，会有较大的影响。但是如果K的值几百上千的话，很可能初次随机初始化就得到很好的结果，多次随机初始化也许会得到稍微好一点的结果，但是不会好太多。 <img src="/media/14944862061717.jpg"></p>
<p>看下图的右边，是一个市场细分的例子，根据数据，将市场分为3个部分，然后区别对待三类不同的顾客群体，更好的适应他们不同的需求，为大中小号3种聚类的用户，设计更合身的S,M,L尺码的衣服。 <img src="/media/14944871641347.jpg"></p>
<h4 id="关于聚类数目k的选取">关于聚类数目K的选取</h4>
<p>关于K应该取什么值，这个问题没有非常标准的解答，或者能自动解决他的方法。目前用来决定聚类数目最常用的方法仍然是通过看可视化的图，或者看聚类算法的输出结果，或者通过其他一切东西来手动选择聚类的数目。嘿嘿，就通过洞察力决定呗。 一般情况下使用肘部法则，但是不用期待表现很好。更多情况下，选择聚类数目的更好方法是，去问一下运行K-means是为了什么目的，然后想想聚类的数目是多少，才适合K-means聚类的后续目的。</p>
<ol style="list-style-type: decimal">
<li><p>肘部法则(Elbow method) 在选择聚类数目K的时候，我们可以使用下肘部方法，如果图像如下图左边的曲线，折点非常明显，那么选择肘点K是个很好的方式，但是如果图像如下图右边所示，那么选取哪个数目，则看起来非常困难。肘部方式，我们可以使用，但是很多时候，往往得不到左图那种有个明显的折点的情况，所以此方式值得尝试，但是我们也不要太期待。 <img src="/media/14944940159311.jpg"></p></li>
<li><p>看不同的聚类数量能为后续下游的目的提供多好的结果。从生意的角度来选择聚类数量，如下图，生产5种大小的T恤可以更加适合顾客，但是3种T恤的话，公司也可以降低成本，更便宜的卖给更多的顾客，因此T恤销售业务的观点，可能会提供一个决定采用3个类还是5个类的方法。 <img src="/media/14944958853442.jpg"></p></li>
</ol>
<h3 id="使用k-means进行图像压缩">使用K-means进行图像压缩</h3>
<p><img src="/media/14966526213973.jpg"> 以上图片是一个使用K-means方法压缩图片的例子。原始图片为128×128像素，每个像素是24位bit长度，每8位表示红绿蓝(red,green,blue)的强度，即RGB编码。原始图片有数千种颜色，现在我们需要将颜色减少到16种。原始图片的存储空间为128×128×24 = 393,216 bits，经过压缩后，所需的存储空间为16×24 + 128×128×4 = 65,920 bits。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;无监督学习算法，就是从未标记的数据中进行学习。 &lt;img src=&quot;/media/14944286743216.jpg&quot;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。&lt;/li&gt;
&lt;li
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Support Vector Machines</title>
    <link href="http://niuoo.github.io/2017/05/05/Support-Vector-Machines/"/>
    <id>http://niuoo.github.io/2017/05/05/Support-Vector-Machines/</id>
    <published>2017-05-05T15:02:21.000Z</published>
    <updated>2017-07-24T06:56:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong><a href="http://blog.csdn.net/sealyao/article/details/6442403" target="_blank" rel="external">SVM中的数学和算法</a></strong> 关于支持向量机，这里有<a href="https://www.zhihu.com/question/21094489" target="_blank" rel="external">一个比较有意思的讲解</a>，还有这里<a href="https://www.youtube.com/watch?v=3liCbRZPrZA" target="_blank" rel="external">一个小视频</a>。<br>SVM是数据挖掘里面很重要的算法，我们有时会把SVM看做是一个大间距分类器Large Magin Intuition。SVM Decision Boundary具有鲁棒性，因为它努力用一个最大间距，来分离样本。<br>有个小的知识点，向量内积以及<a href="http://www.cnblogs.com/vive/p/4563803.html" target="_blank" rel="external">证明</a></p>
<h1 id="线性核函数">线性核函数</h1>
<p>SVM线性核函数，就是SVM不使用核函数。其实是在逻辑回归的基础上进行稍微变化而成的。<br><img src="/media/14943143749438.jpg"></p>
<p>使用SVM软件包来确定参数θ的时候，我们需要指定常数C，C值如果太大，那么就会过拟合，效果不好。如以下示图，注意看左下角那个红叉叉，如果C太大的时候，决策边界就会是斜着的那条很线，这样并不好: <img src="/media/14943142227688.jpg"></p>
<h1 id="kernels">Kernels</h1>
<p>构造非线性复杂的分类器，我们用“Kernels函数”来达到此目的。<br>先来看一张图，如果决策分界是曲线的时候，那么就会有多项式特征变量的出现，如果不进行变化，就直接去求解，那么运算量是非常大的，有太多的高阶项需要被计算，所以我们需要通过其他方式来构造特征变量，来嵌入到假设函数中：<br><img src="/media/14943156955239.jpg"></p>
<p>Kernels核函数有很多，我们用的最多的是高斯核函数(Gaussian kernel)，$ f_i=exp{ }$ 其中 <span class="math">\(l^(i) =x^(i) ，x^(i)\)</span>表示m个训练数据的第i个样本，高斯核函数描述了某个样本和其他样本的距离程度。这个函数类似高斯分布，因此称为高斯核函数。也叫做径向基函数(Radial Basis Function 简称RBF)。它能够把原始特征映射到无穷维。 <img src="/media/14943147936462.jpg"></p>
<p>看下面这张图，可以更加理解高斯核函数的作用，注意看图中的红色大圈，如果<span class="math">\(f_1≈1,f_2≈0,f_3≈0\)</span>，那么证明这个点离<span class="math">\(l^(1)\)</span>很近，y=1。图中训练出的θ值，表明了红圈内是预测y=1，红圈外是y=0，即离点<span class="math">\(l^(1),l^(2)\)</span>都很远。这就是我们如何通过标记点以及核函数，来训练出非常复杂的非线性决策边界的方法。</p>
<div class="figure">
<img src="/media/14943222658500.jpg">

</div>
<p>高斯核函数的损失函数如下图所示，不过这些我们自己平时直接使用优化好的SVM包，并不需要关心这些细节，这些都是内部优化好的，不需要来定义。 <img src="/media/14943195312828.jpg"></p>
<p>在使用高斯核函数的时候，我们需要选择<span class="math">\(σ^2\)</span>，如果<span class="math">\(σ^2\)</span>偏大我们就会得到一个较大误差较低方差的分类器，高斯核函数返回的是(0,1)区间的实数。另外，在使用核函数的时候，进行数据归一化也是很必要的，因为如果不进行归一化，那么值比较大的特征向量会占据很大的地位，弱化其他特征向量的影响，这是不公平的。 <img src="/media/14943198501799.jpg"></p>
<p>支持向量机算法的核函数必须满足莫塞尔定理“Mercer’s Theorem”。对于其他的核函数，此处不做过多介绍。这里有个<a href="http://www.cnblogs.com/jerrylead/archive/2011/03/18/1988406.html" target="_blank" rel="external">核函数的介绍</a></p>
<h1 id="如何选择使用何种算法呢">如何选择使用何种算法呢</h1>
<ol style="list-style-type: decimal">
<li>如果特征向量的个数很大,而训练集很小时，我们通常使用逻辑回归，或者使用SVM线性核函数。因为没有足够的数据来拟合非常复杂的非线性函数。</li>
<li>如果特征向量很小，而训练数据量是中等大小，那么核函数表现就可以很好。<br></li>
<li>如果如果特征向量很小，而训练数据量巨大，那么高斯核函数就会运行很慢，这种情况下，可以尝试手动建立更多的特征变量，然后使用逻辑回归或者SVM线性核函数。</li>
</ol>
<div class="figure">
<img src="/media/14943200907676.jpg">

</div>
<p>以上，第2种，例如，特征向量为1000左右，训练集是10,000的情况下，高斯函数的支持向量机会表现的非常突出。另外第1种和第3种情况，其实使用逻辑回归和SVM线性核函数效果都差不多。<br>SVM是凸优化的，所以局部最优，就是全局最优了。</p>
<p>在使用SVM软件包的时候，需要我们自己按照需求，配置以下参数：<br><img src="/media/14943202565702.jpg"></p>
<p>多种分类时：<br><img src="/media/14943205457069.jpg"></p>
<p>其实呢这课，我没怎么看懂，向量内积怎么使用我也不知道，SVM算法怎么使用倒是有个模糊的印象，但是具体内部的算法原理，没搞太明白，不知道最后的决策边界怎么产生的最大边距。这篇SVM我写的好吃力啊，重新刷斯坦福视频的时候居然看不懂了，真是……，不过还好最后看的自己觉得明白了，就先记录下来。吃力。</p>
<p>练习题中，垃圾邮件分类器，采用SVM线性核函数。通过训练集，训练出字典表中每个单词的权重(就是θ参数)。进行垃圾邮件分类时，是需要对邮件内容进行预处理的，比如网址啊，钱啊，之类的，都会用一个单词或者符号代替，出现的哪些单词(当然最后都向量化数字处理了，类似于x = [ 0 0 0 0 1 0 0 0 … 0 0 0 0 1 … 0 0 0 1 0 ..]，x的长度是字典长度，数据1表示出现了这个单词)，最后带入我们的式子<span class="math">\(θ^TX\)</span>里面，可进行是否垃圾邮件的预测，如果大于等于0，就是垃圾邮件，小于0，是正常邮件。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;&lt;a href=&quot;http://blog.csdn.net/sealyao/article/details/6442403&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;SVM中的数学和算法&lt;/a&gt;&lt;/strong&gt; 关于支持向量机，这里有
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>神经网络初探</title>
    <link href="http://niuoo.github.io/2017/05/04/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%9D%E6%8E%A2/"/>
    <id>http://niuoo.github.io/2017/05/04/神经网络初探/</id>
    <published>2017-05-04T06:14:20.000Z</published>
    <updated>2017-08-08T13:37:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>神经网络是当今最强大的学习算法之一，自动驾驶对周围景物的识别，就用到了神经网络。<br>我们以下讲述，神经网络在分类问题中的应用，在给定训练集下，为神经网络拟合参数的学习方法。<br><span class="math">\(\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \newline a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \newline a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \newline h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \newline \end{align*}\)</span><br>在整个神经网络算法过程中，我们的最终目标是找到最佳的Θ矩阵中的权值。<br><img src="/media/14943440970717.jpg"></p>
<h1 id="神经网络的损失函数">神经网络的损失函数</h1>
<p>其中逻辑回归的损失函数为：<br><span class="math">\(J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2\)</span><br>神经网络的损失函数在逻辑回归的基础上，稍微复杂了一点儿：<br><span class="math">\(\begin{gather*} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather*}\)</span></p>
<ul>
<li>L = 神经网络的层数。<br></li>
<li><span class="math">\(s_l\)</span> = 在第l层，神经单元的个数(不包括偏移量单元)</li>
<li>K = 神经网络输出层的单元个数。就是分类的类别总数。数字识别的话就是10.</li>
</ul>
<p>在Θ矩阵中，列代表当前层的单元个数，包括偏移单元；行代表下一层的神经元个数，不包含偏移单元。</p>
<p>注意：</p>
<ul>
<li>双和嵌套，只是简单的把输出层每个分类的逻辑回归损失加在一起了。m代表训练集的个数。</li>
<li>三个和嵌套的，是把神经网络里面，所有Θ矩阵的元素平方加在一起了。</li>
<li>i在三和嵌套中，代表的不是训练样本个数，而是代表当前层的神经元个数。</li>
</ul>
<h1 id="反向传播算法-backpropagation-algorithm">反向传播算法 Backpropagation Algorithm</h1>
<p>Back propagation的本质就是复合函数求导（following the chain rule）链式法则，本可以对这个网络里的每一个参数分别求偏导，但何苦呢，因为计算过程中的很多项都是重复的。为了不重复运算把把后层算好的导数传回前层，因为前层一定用得到。<br>关于反向传播算法，这里有个<a href="https://www.zhihu.com/question/27239198?rf=24827633" target="_blank" rel="external">浅显易懂的描述</a>。这里请看<a href="http://www.cnblogs.com/dengdan890730/p/5537451.html" target="_blank" rel="external">反向传播算法的推导过程</a>和<a href="http://blog.csdn.net/u014403897/article/details/46347351" target="_blank" rel="external">这里最后一层的求导</a>,这几个地方写的都不错。Andrew Ng老师的公开课对反向传播算法没有做过多的介绍，讲的也不够清晰。反向传播算法，就是为了计算损失函数的导数。<br>先贴个图片留作记号，便于下面Δ计算的理解</p>
<div class="figure">
<img src="/media/14939758124978.jpg">

</div>
<h2 id="反向传播算法的使用过程">反向传播算法的使用过程：</h2>
<ol style="list-style-type: decimal">
<li>Set <span class="math">\(a^{(1)} := x^{(t)}\)</span></li>
<li><p>正向计算每层的每个节点的<span class="math">\(a^{(l)}\)</span>for l=2,3,…,L</p>
<div class="figure">
<img src="/media/14939747131225.jpg">

</div></li>
<li>使用<span class="math">\(y^{(t)}，计算出\delta^{(L)} = a^{(L)} - y^{(t)}\)</span>，最后输出层的错误率。</li>
<li><p>使用<span class="math">\(\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .*\ a^{(l)}\ .*\ (1 - a^{(l)})\)</span>，计算出<span class="math">\(\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}\)</span>。<br> 其中$g’(z^{(l)}) = a^{(l)} .* (1 - a^{(l)})；_j^{(l)} =  cost(t)； $</p></li>
</ol>
<p><span class="math">\(cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))\)</span></p>
<ol start="5" style="list-style-type: decimal">
<li><span class="math">\(\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}，或者向量化一下，\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T\)</span>(Δ的初始化为0)<br> 至此，我们更新的权值累加器矩阵的为new Δ matrix，其中<span class="math">\(\frac \partial {\partial \Theta_{ij}^{(l)}} J(\Theta)=D_{ij}^{(l)}\)</span>
<ul>
<li><p><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)\)</span> if j≠0.</p></li>
<li><p><span class="math">\(D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}\)</span> If j=0</p></li>
</ul></li>
</ol>
<h3 id="梯度检验">梯度检验</h3>
<p>在神经网络中使用反向传播算法的时候，因为有很多细节，会导致各种各样小bug，即使J看起来每次都是下降的，但是，最终结果的误差却很大。因此梯度检验也是需要的过程，它减少这种错误的概率。在其他比较复杂的模型中使用梯度算法的时候，进行这种检查也是有意义的，这么做，将会对模型更加自信，确信模型是100%正确。<br><span class="math">\(\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}\)</span></p>
<p>在确定算法无误后，真正执行学习算法前，一定要关掉梯度检验，否则会很慢的哟。</p>
<h3 id="θ的初始化">Θ的初始化</h3>
<p>Θ初始化在逻辑回归的时候是可以为0的，但是在神经网络中不可以全部为0的。因为都为0的时候，隐藏层的结果都是一样的，这样做，隐藏层就完全是冗余的，神经网络就没作用了，完全就是个逻辑回归。所以我们需要打破对称，随机初始化。</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;神经网络是当今最强大的学习算法之一，自动驾驶对周围景物的识别，就用到了神经网络。&lt;br&gt;我们以下讲述，神经网络在分类问题中的应用，在给定训练集下，为神经网络拟合参数的学习方法。&lt;br&gt;&lt;span class=&quot;math&quot;&gt;\(\begin{align*} a_1^{(2)}
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>线性回归和逻辑回归</title>
    <link href="http://niuoo.github.io/2017/05/02/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    <id>http://niuoo.github.io/2017/05/02/线性回归和逻辑回归/</id>
    <published>2017-05-02T07:25:46.000Z</published>
    <updated>2017-12-19T08:55:37.000Z</updated>
    
    <content type="html"><![CDATA[<p>回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 <img src="/media/14938262502932.jpg"></p>
<h1 id="线性回归">线性回归</h1>
<p>线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如<span class="math">\(θ_{1}x_{1}+θ_2x_{2}+θ_3x_{3}=y\)</span>。都使用向量vector表示，<span class="math">\(θ^T =[θ_1,θ_2,θ_3],X^T= [x_1,x_2,x_3]\)</span>，假设函数为<span class="math">\(h_\theta(x)=θ^TX\)</span>。我们的目的是为了求出最恰当的θ，即<span class="math">\(θ_1,θ_2,θ_3\)</span>三者的值，使得损失函数<span class="math">\(J(θ)=\frac{1}{2m}\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})^2\)</span>的值最小。</p>
<h3 id="什么是梯度gradient">什么是梯度(Gradient)？</h3>
<p>简单来说，梯度就是损失函数关于权值θ的导数，也就是关于θ的变化率(导数即变化率吧)。</p>
<h3 id="梯度下降算法gradient-descent">梯度下降算法(Gradient descent)</h3>
<p>我们得到的一般是个局部最优解。如果是个碗状或者叫做倒钟型的函数，那局部最优也是全局最优解。<br>The gradient descent algorithm is:<br>repeat until convergence:<br><span class="math">\(θ_j:=θ_j−α\frac∂{∂θ_j}J(θ_1,θ_2,θ_3)\)</span><br>where j=1,2,3represents the feature index number.<br><a href="https://d3c33hcgiwev3.cloudfront.net/_ec21cea314b2ac7d9e627706501b5baa_Lecture2.pdf?Expires=1493856000&amp;Signature=GSAbIM5AmG64UdFqdjCCWIn5hN~JZ8IheTVb6mliIEMdfhHgTrecl9toRVElelfaWZGY3vPkI33K7uOHicFc52EldArFxSunmfh4Mr4yjiEpZBbSF8-Tl9cWVTy2pAixsdpkmlL37Lku8VGax-LoenwwvR0i055g8j2wKJCGOrQ_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A" target="_blank" rel="external">斯坦福课程线性回归教案拉到三分之二处看</a><br><a href="http://m.blog.csdn.net/article/details?id=51188876" target="_blank" rel="external">几种梯度下降方法</a><br><a href="http://blog.csdn.net/qq_34206952/article/details/54316285" target="_blank" rel="external">求导公式证明</a><br>α我们称之为learning rate步长，如果太大，那么损失函数不能收敛，如果太小，会收敛过慢。</p>
<p>另外需要注意特征缩放(feature scaling)，确保不同特征的取值在相近的范围内，这样梯度下降法就能更快的收敛。因为如果不做这个，J(θ)在θ1和θ2轴上的投影将会非常的瘦长，收敛的过程需要走很多步，才能到达最小值，做特征缩放，就是为了让偏移没那么严重，投影看起来更圆一些。进行特征缩放时，将特征的取值约束到−1≤x(i)≤1或者−0.5≤x(i)≤0.5。<br><span class="math">\(x_i:=\frac{x_i−μ_i}{s_i}\)</span> μi表示feature(i)的平均值，si是feature(i)值的范围(max - min),或者是标准差。<br>For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, <span class="math">\(xi:=\frac{price−1000}{1900}\)</span>.<br>说句题外话，这个特征缩放也很像将一般的正态分布转化成标准正态分布的过程:p</p>
<p>另外还有对应线性回归的，多项式回归(曲线回归)。有时候，线性函数无法合适的进行数据的拟合，这时候也许多项式进行拟合效果更好（曲线曲面等）。</p>
<h3 id="正规方程法normal-equation">正规方程法(Normal Equation)</h3>
<p>此乃最小二乘法，<span class="math">\(θ = (X^TX)^{-1}X^Ty\)</span>,不需要进行特征的缩放，不需要选择α，不需要迭代，但梯度下降法在很多特征变量的情况下，也能运行地相当好，即使有上百万的特征变量，通常很有效，时间复杂度为<span class="math">\(O(kn^2)\)</span>;而正规方程法，为了求解参数θ，需要求解<span class="math">\((X^TX)^{-1}\)</span>，其中<span class="math">\(X^TX\)</span>这是个n*n的矩阵，然后求逆矩阵的计算量，大概是矩阵纬度的三次方,<span class="math">\(O(n^3)\)</span>，因此当n很大时，这个方法会非常慢的。n&gt;1W就考虑梯度下降吧。只要特征数目不是很大，用正规方程法是非常好的，对于线性回归的模型。</p>
<table>
<thead>
<tr class="header">
<th align="left">Gradient Descent</th>
<th align="left">Normal Equation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Need to choose alpha</td>
<td align="left">No need to choose alpha</td>
</tr>
<tr class="even">
<td align="left">Needs many iterations<span class="Apple-tab-span" style="white-space:pre"></span></td>
<td align="left">No need to iterate</td>
</tr>
<tr class="odd">
<td align="left"><span class="math">\(O(kn^2)\)</span></td>
<td align="left"><span class="math">\(O(n^3)\)</span>,need to calculate inverse of <span class="math">\(X^TX\)</span></td>
</tr>
<tr class="even">
<td align="left">Works well when n is large</td>
<td align="left">Slow if n is very large</td>
</tr>
</tbody>
</table>
<p>如果<span class="math">\(X^TX\)</span>是不可逆(不可逆的矩阵为奇异或退化矩阵)的时候呢？如果存在两个特征值有线性相关的时候，矩阵不可逆，或者特征向量太多的时候(e.g. m ≤ n)，矩阵或许也不可逆，这种情况就删除重复特征的一个，无须同时保留。一般情况下，算法库也会给我们一个伪逆矩阵或者是逆矩阵的解。<br>注:设A是数域上的一个n阶方阵，若在相同数域上存在另一个n阶矩阵B，使得：AB=BA=E。则我们称B是A的逆矩阵，而A则被称为可逆矩阵。</p>
<h1 id="逻辑回归logistic-regression">逻辑回归(Logistic Regression)</h1>
<p>sigmoid可以轻松处理0/1分类问题sigmoid函数，又称逻辑函数Logistic Function，用来进行归一化处理(促使<span class="math">\(y^{(i)}∈(0,1)\)</span>)。逻辑回归的主体还是回归操作：回归对象是sigmoid函数，它将输入映射为处于0到1之间的小数，得到这个小数之后人为将其解读成概率，然后根据事先设定的阈值进行分类。 <span class="math">\(\begin{align*}&amp; h_\theta (x) = g ( \theta^T x ) \newline \newline&amp; z = \theta^T x \newline&amp; g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}\)</span></p>
<p>sigmoid函数图像如下 <img src="/media/14938277151973.jpg"> <span class="math">\(h_θ(x)\)</span>代表了输出是1的概率为多少。如<span class="math">\(h_θ(x)\)</span>=0.7表示结果有70%的可能性是1。<br><span class="math">\(\begin{align*}&amp; h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \newline&amp; P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}\)</span><br>因为结果都是离散数值，0或者1，所以当结果大于等于0.5的时候，就认为是1，反之为0.<br><span class="math">\(\begin{align*}&amp; h_\theta(x) \geq 0.5 \rightarrow y = 1 \newline&amp; h_\theta(x) &lt; 0.5 \rightarrow y = 0 \newline\end{align*}\)</span><br>等价于<br><span class="math">\(\begin{align*}&amp; \theta^T x \geq 0 \Rightarrow y = 1 \newline&amp; \theta^T x &lt; 0 \Rightarrow y = 0 \newline\end{align*}\)</span><br>另外<span class="math">\(z = \theta^T x\)</span> 函数也不一定是线性的，例<span class="math">\(z =θ_0+θ_1x^2_1+θ_2x^2_2\)</span>也可以是个圆，或者其他的形状，只要拟合数据就OK。<br>逻辑回归的损失函数如下：<br><span class="math">\(\begin{align*}&amp; J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; &amp; \text{if y = 1} \newline &amp; \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; &amp; \text{if y = 0}\end{align*}\)</span><br>当y = 1的时候，关于J(θ)和$ h_θ(x)<span class="math">\(的函数\)</span>Cost(h_(x),y) = -(h_(x))$图像如下: <img src="/media/14938660514848.jpg"></p>
<p><span class="math">\(\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \newline \end{align*}\)</span></p>
<p>当y = 0的时候，关于J(θ)和$ h_θ(x)<span class="math">\(的函数\)</span>Cost(h_(x),y) = -(1-h_(x))$图像如下: <img src="/media/14938664062585.jpg"> <span class="math">\(\begin{align*}&amp; \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y\newline &amp; \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \newline \end{align*}\)</span></p>
<p>综上，得到损失函数为<span class="math">\(\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))\)</span><br>也写成<span class="math">\(J(\theta) = - \frac{1}{m} \displaystyle \sum_{i=1}^m [y^{(i)}\log (h_\theta (x^{(i)})) + (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]\)</span><br>向量化可表示成<br><span class="math">\(\begin{align*} &amp; h = g(X\theta)\newline &amp; J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align*}\)</span><br>梯度下降:<br><span class="math">\(\begin{align*}&amp; Repeat \; \lbrace \newline &amp; \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta)=\theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \newline &amp; \rbrace\end{align*}\)</span><br>向量化表示为:<span class="math">\(\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})\)</span></p>
<h1 id="综述">综述</h1>
<p><img src="/media/14938678719388.jpg"> 第一个图像是欠拟合的(underfitting),第二个相对合适，第三个过拟合。 欠拟合，一般都是因为h函数太简单，或者是特征向量过少。而过度拟合问题(Overfitting)，往往是因为函数太复杂了，虽然很完美的拟合了输入数据，但是产生了许多不必要的曲线角度，对于往后的测试，其实是不利的。<br>解决过度拟合问题:</p>
<ol style="list-style-type: decimal">
<li>减少特征数：手动选择要保留的特征，或者使用模型选择算法</li>
<li>正则化：保留所有的特征，但是减轻θ的影响。 <span class="math">\(min_\theta\ \dfrac{1}{2m}\  \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2\)</span><br>λ太大，就会欠拟合，因为θ会很小，甚至为0；如果λ太小，就会过拟合，因为θ会很大，正则化也失去了相应的作用。<br>线性回归的正则化的梯度下降和最小二乘法，看这里<a href="https://www.coursera.org/learn/machine-learning/supplement/pKAsc/regularized-linear-regression" target="_blank" rel="external">课程资料</a><br>逻辑回归正则化看这里<a href="https://www.coursera.org/learn/machine-learning/supplement/v51eg/regularized-logistic-regression" target="_blank" rel="external">课程资料</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;回归一词指的是我们根据之前的数据，预测出一个准确的输出值。 &lt;img src=&quot;/media/14938262502932.jpg&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;线性回归&quot;&gt;线性回归&lt;/h1&gt;
&lt;p&gt;线性回归是统计学领域的方法，特征和结果的关系满足线性，即不大于1次方，如&lt;s
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>关于机器学习</title>
    <link href="http://niuoo.github.io/2017/05/02/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8F%AF%E4%BB%A5%E5%81%9A%E4%BB%80%E4%B9%88/"/>
    <id>http://niuoo.github.io/2017/05/02/机器学习可以做什么/</id>
    <published>2017-05-02T02:26:03.000Z</published>
    <updated>2017-08-16T03:27:09.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="先开个玩笑">先开个玩笑</h2>
<div class="figure">
<img src="/media/14938266335179.jpg">

</div>
<p>以上这个图片只是开个玩笑啦！哈哈哈<br>我目前做的事情呢，比较偏向于倒数第三张图片，目前，我是个观望的门外汉。<br>最后一整图片，scikit-learn封装了大部分机器学习算法的Python库。通常ML都在调用现有算法库，然后调参数。有一句话说，你以为我是科学家，其实我是调包侠。那么接下来呢，我不会告诉大家怎么来正确调包，我只是想分享一下关于机器学习，我了解到的都有什么知识。<br>另外，我讲解的所有东西，来源是吴恩达Andrew Ng教授的机器学习课程，或者网络知乎回答等。全部内容皆非原创。<br><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="external">Andrew Ng机器学习课程</a></p>
<h2 id="机器学习可以做什么">机器学习可以做什么</h2>
<ol style="list-style-type: decimal">
<li>数据挖掘。房价预测之类。<br></li>
<li>不能通过精确编程完成的应用。如手写数字的识别，自然语言解析，计算机视觉等。<br></li>
<li>推荐系统。亚马逊和Netflix的产品推荐。<br><a href="https://vimeo.com/57513893" target="_blank" rel="external">用机器学习做过什么有趣的事情</a></li>
</ol>
<h2 id="监督式学习supervised-learning">监督式学习–Supervised Learning</h2>
<p>监督式学习可以分为“回归regression”和“分类classification”问题。输入数据集a data set(features)，和每条数据对应的正确输出(label)，并且数据集和输出之间有一定的关系(map or function)。在回归问题中，我们尝试找到一个连续函数，可以最好的映射输入输出，然后通过这个continuous function来进行后续的预测。而分类问题也差不多如此，只不过，输出都是离散的值。</p>
<ol style="list-style-type: decimal">
<li>根据房屋大小，预测房子价格。这是个线性回归的问题。(这很像数据挖掘或者统计学的问题:p)<br></li>
<li>给一张人物照片，预测这个人的年龄。线性回归问题。<br></li>
<li>给定一个病人的肿瘤数据，预测肿瘤是良性的还是恶性的。分类问题。<br></li>
<li>手写数字的识别、区分是否垃圾邮件、是否金融欺诈。分类问题。</li>
</ol>
<h2 id="无监督式学习unsupervised-learning">无监督式学习–Unsupervised Learning</h2>
<p>在未加标签的数据中，试图找到隐藏的结构。</p>
<ol style="list-style-type: decimal">
<li>聚类。给定数据集，分类成一个个组合，如新闻的分类等。组织大型计算机集群，并试图找出那些机器趋向于协同工作，如果把这些机器放在一起，就可以让数据中心更高效工作。社交网络里面也有，区分哪些用户之间是很亲密，哪些用户之间仅仅是认识。根据商业系统中的数据，细分市场，然后把客户再分到不同的细分市场中。(k-means)<br></li>
<li>信号分离。使用特征提取的技术降维。鸡尾酒会，说话声音和音乐声音分离。(PCA)</li>
</ol>
<h2 id="半监督式semi-supervised">半监督式–Semi-supervised</h2>
<p>就是只有某些样本有对应的lable，例如有很多照片，猫猫狗狗人人，但是标记的只是一部分，一般就是根据聚类，对照片进行分类，分类后，根据那部分已知标签，对所有照片进行标记。使用例子facebook的人脸标记等(应该是识别出照片中不同的人脸都是睡吧)。或者是药品效果预测，只有少量药品都有对应的效果标签，然后通过半监督，对所有药品进行效果预测。 ## 强化学习–Reinforcement Learning<br>强化学习是agent(与环境有交互的对象)自己去学习，并且得到反馈reward，以便RL进行迭代，学习到策略链，是个决策模型，而监督学习是跟着programmer的idea在收敛。<br>强化学习，犹如训练宠物狗狗的过程，一般用在广告系统，广告通过用户的点击量等行为，发现怎么放广告效果更好；或者棋牌类，策略类等游戏对战中的AI。<br><a href="https://zhuanlan.zhihu.com/p/25319023" target="_blank" rel="external">强化学习的知识整理</a></p>
<p>小总结： <img src="/media/14967616034724.jpg"></p>
<p><img src="/media/14967630764689.jpg"> 1. 填鸭式学习，如监督式学习，扔给学生一本练习题和对应的答案。 2. 被动式学习，如课堂老师现场不断地教授知识 3. 主动式学习，学生可以主动问问题，并且问的问题都比较有价值，能够帮助学生更快更好的学到知识。</p>
<p>对于强化学习，我几乎没有任何了解。接下来的文章，会描述一下机器学习中，比较基础简单的算法。 :p<br><img src="/media/14967643300102.jpg"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;先开个玩笑&quot;&gt;先开个玩笑&lt;/h2&gt;
&lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/media/14938266335179.jpg&quot;&gt;

&lt;/div&gt;
&lt;p&gt;以上这个图片只是开个玩笑啦！哈哈哈&lt;br&gt;我目前做的事情呢，比较偏向于倒数第三张图片，目
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LeetCode 416 Partition Equal Subset Sum</title>
    <link href="http://niuoo.github.io/2017/01/22/LeetCode-416-Partition-Equal-Subset-Sum/"/>
    <id>http://niuoo.github.io/2017/01/22/LeetCode-416-Partition-Equal-Subset-Sum/</id>
    <published>2017-01-22T12:40:07.000Z</published>
    <updated>2017-05-05T07:59:04.000Z</updated>
    
    <content type="html"><![CDATA[<div class="figure">
<img src="/media/14939711433257.jpg">

</div>
<p>Given a non-empty array containing only positive integers, find if the array can be partitioned into two subsets such that the sum of elements in both subsets is equal.</p>
<p>Note: 1. Each of the array element will not exceed 100. 2. The array size will not exceed 200. Example 1:</p>
<p>Input: [1, 5, 11, 5]</p>
<p>Output: true</p>
<p>Explanation: The array can be partitioned as [1, 5, 5] and [11]. Example 2:</p>
<p>Input: [1, 2, 3, 5]</p>
<p>Output: false</p>
<p>Explanation: The array cannot be partitioned into equal sum subsets.</p>
<p><strong>这个题目的思路是先算出数组的总和，然后取总和的一半，如果数组存在总和等于sum/2的子序列，那么数组就可以被分成两组总和和相等的子序列。 主要问题是如何确定数组是否存在总和等于sum/2的子序列，就是做深搜了。 关于求数组子序列总和等于target的所有子序列集合，可以看向这里<a href="http://niuoo.me/2017/01/22/LeetCode-40-Combination-Sum-II/" target="_blank" rel="external">LeetCode 40 Combination Sum II</a></strong></p>
<p><strong>Runtime: 11 ms runtime beats 99.63% of java submissions.</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">canPartition</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</div><div class="line">	<span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> num : nums) &#123;</div><div class="line">		sum += num;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">if</span> (sum % <span class="number">2</span> != <span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">	sum = sum / <span class="number">2</span>;</div><div class="line">	Arrays.sort(nums);</div><div class="line">	<span class="keyword">return</span> dfsPartition(<span class="number">0</span>, nums, sum);</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">dfsPartition</span><span class="params">(<span class="keyword">int</span> begin, <span class="keyword">int</span>[] nums, <span class="keyword">int</span> rest)</span> </span>&#123;</div><div class="line">	<span class="keyword">if</span> (rest == <span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">	<span class="comment">/**分别从nums[i]开始往下深搜*/</span></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = begin; i &lt; nums.length &amp;&amp; nums[i] &lt;= rest; i++) &#123;</div><div class="line">		<span class="keyword">if</span> (i &gt; begin &amp;&amp; nums[i] == nums[i - <span class="number">1</span>]) <span class="keyword">continue</span>;</div><div class="line">		<span class="keyword">if</span> (dfsPartition(i + <span class="number">1</span>, nums, rest - nums[i])) <span class="keyword">return</span> <span class="keyword">true</span>;<span class="comment">//搜索下个数字</span></div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;div class=&quot;figure&quot;&gt;
&lt;img src=&quot;/media/14939711433257.jpg&quot;&gt;

&lt;/div&gt;
&lt;p&gt;Given a non-empty array containing only positive integers, find if the
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LeetCode 40 Combination Sum II</title>
    <link href="http://niuoo.github.io/2017/01/22/LeetCode-40-Combination-Sum-II/"/>
    <id>http://niuoo.github.io/2017/01/22/LeetCode-40-Combination-Sum-II/</id>
    <published>2017-01-22T12:35:06.000Z</published>
    <updated>2017-01-22T12:38:15.000Z</updated>
    
    <content type="html"><![CDATA[<p>Given a collection of candidate numbers (C) and a target number (T), find all unique combinations in C where the candidate numbers sums to T.</p>
<p>Each number in C may only be used once in the combination.</p>
<p>Note: All numbers (including target) will be positive integers. The solution set must not contain duplicate combinations. For example, given candidate set [10, 1, 2, 7, 6, 1, 5] and target 8, A solution set is: <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[</div><div class="line">  [1, 7],</div><div class="line">  [1, 2, 5],</div><div class="line">  [2, 6],</div><div class="line">  [1, 1, 6]</div><div class="line">]</div></pre></td></tr></table></figure></p>
<p><strong>Runtime: 17 ms runtime beats 94.63% of java submissions.</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">public</span> List&lt;List&lt;Integer&gt;&gt; combinationSum2(<span class="keyword">int</span>[] candidates, <span class="keyword">int</span> target) &#123;</div><div class="line">	Arrays.sort(candidates);</div><div class="line">	List&lt;List&lt;Integer&gt;&gt; result = <span class="keyword">new</span> LinkedList&lt;List&lt;Integer&gt;&gt;();</div><div class="line">	List&lt;Integer&gt; tmp = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</div><div class="line">	dfs(<span class="number">0</span>, candidates, target, tmp, result);</div><div class="line">	<span class="keyword">return</span> result;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> begin, <span class="keyword">int</span>[] candidates, <span class="keyword">int</span> rest, List&lt;Integer&gt; tmp,</span></span></div><div class="line">				 List&lt;List&lt;Integer&gt;&gt; result) &#123;</div><div class="line">	<span class="keyword">if</span> (rest == <span class="number">0</span>)</div><div class="line">		result.add(<span class="keyword">new</span> ArrayList&lt;Integer&gt;(tmp));<span class="comment">//tmp是变动的,所以此处需要new一个</span></div><div class="line">	<span class="comment">/*分别从candidates[i]开始往下深搜*/</span></div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = begin; i &lt; candidates.length &amp;&amp; candidates[i] &lt;= rest; i++) &#123;</div><div class="line">		<span class="keyword">if</span> (i &gt; begin &amp;&amp; candidates[i] == candidates[i - <span class="number">1</span>]) <span class="keyword">continue</span>;</div><div class="line">		tmp.add(candidates[i]);</div><div class="line">		dfs(i + <span class="number">1</span>, candidates, rest - candidates[i], tmp, result);<span class="comment">//搜索下个数字</span></div><div class="line">		tmp.remove(tmp.size() - <span class="number">1</span>);<span class="comment">//回溯</span></div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Given a collection of candidate numbers (C) and a target number (T), find all unique combinations in C where the candidate numbers sums t
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>LeetCode 377 Combination Sum IV</title>
    <link href="http://niuoo.github.io/2016/07/26/IV/"/>
    <id>http://niuoo.github.io/2016/07/26/IV/</id>
    <published>2016-07-26T06:55:15.000Z</published>
    <updated>2016-07-26T11:08:11.000Z</updated>
    
    <content type="html"><![CDATA[<p>Given an integer array with all positive numbers and no duplicates, find the number of possible combinations that add up to a positive integer target. Example: nums = [1, 2, 3] target = 4</p>
<p>The possible combination ways are: (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1)</p>
<p>Note that different sequences are counted as different combinations. Therefore the output is 7.</p>
<p>刚开始写了个递归，就去提交了。Time Limit Exceeded.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">/***这是错误的代码！***/</span></div><div class="line">	<span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">combinationSum4</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</div><div class="line">		<span class="keyword">int</span> len = nums.length;</div><div class="line">		Arrays.sort(nums);</div><div class="line">		<span class="keyword">int</span> sum = <span class="number">0</span>;</div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</div><div class="line">			<span class="keyword">if</span> (target - nums[i] == <span class="number">0</span>) sum += <span class="number">1</span>;</div><div class="line">			<span class="keyword">if</span> (target - nums[i] &gt; <span class="number">0</span>) sum += combinationSum4(nums, target - nums[i]);</div><div class="line">			<span class="keyword">if</span> (target - nums[i] &lt; <span class="number">0</span>) <span class="keyword">break</span>;</div><div class="line">		&#125;</div><div class="line">		<span class="keyword">return</span> sum;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>以上解法，毫无疑问，Time Limit Exceeded了，因为很多步骤都是重复的，如 [1,50]的nums，targe 为200，所以根据这错的方法我们就会重复多计算比200小的target，导致超时，所以，我们要提前把小于 target的数，都先计算出来，防止递归的时候，重复计算，导致超时。 正确的解法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">combinationSum4</span><span class="params">(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target)</span> </span>&#123;</div><div class="line">	Arrays.sort(nums);</div><div class="line">	<span class="keyword">int</span>[] sum = <span class="keyword">new</span> <span class="keyword">int</span>[target + <span class="number">1</span>];</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= target; i++) &#123;<span class="comment">//提前计算出所有小的target有几种组合数</span></div><div class="line">		<span class="keyword">for</span> (<span class="keyword">int</span> num : nums) &#123;</div><div class="line">			<span class="keyword">if</span> (i == num) sum[i] += <span class="number">1</span>;</div><div class="line">			<span class="keyword">if</span> (i &gt; num) sum[i] += sum[i - num];</div><div class="line">			<span class="keyword">if</span> (i &lt; num) <span class="keyword">break</span>;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	<span class="keyword">return</span> sum[target];</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Given an integer array with all positive numbers and no duplicates, find the number of possible combinations that add up to a positive in
    
    </summary>
    
    
      <category term="LeetCode 377 Combination Sum IV" scheme="http://niuoo.github.io/tags/LeetCode-377-Combination-Sum-IV/"/>
    
  </entry>
  
  <entry>
    <title>LeetCode 179 Largest Number</title>
    <link href="http://niuoo.github.io/2016/07/23/Number/"/>
    <id>http://niuoo.github.io/2016/07/23/Number/</id>
    <published>2016-07-23T03:42:52.000Z</published>
    <updated>2016-07-23T03:54:34.000Z</updated>
    
    <content type="html"><![CDATA[<p>Given a list of non negative integers, arrange them such that they form the largest number. For example, given [3, 30, 34, 5, 9], the largest formed number is 9534330. Note: The result may be very large, so you need to return a string instead of an integer. 思路：比较两个数连接后的大小进行排序。 Runtime: 125 ms  beats 88.37%</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">largestNumber</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</div><div class="line">	String[] s = <span class="keyword">new</span> String[nums.length];</div><div class="line">	<span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) s[i] = String.valueOf(nums[i]);</div><div class="line">	Arrays.sort(s, (b, a) -&gt; (a.concat(b).compareTo(b.concat(a))));</div><div class="line">	StringBuffer result = <span class="keyword">new</span> StringBuffer(<span class="string">""</span>);</div><div class="line">	<span class="keyword">for</span> (String num : s) result.append(num);</div><div class="line">	<span class="keyword">return</span> result.substring(<span class="number">0</span>, <span class="number">1</span>).equals(<span class="string">"0"</span>) ? <span class="string">"0"</span> : result.toString();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用java8的特性： Runtime: 153 ms beats 10.23%</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">public</span> String <span class="title">largestNumber2</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</div><div class="line">	String result = Arrays</div><div class="line">			.stream(nums)</div><div class="line">			.mapToObj(Integer::toString)</div><div class="line">			.sorted((b, a) -&gt; (a.concat(b).compareTo(b.concat(a))))</div><div class="line">			.collect(Collectors.joining());</div><div class="line">	<span class="keyword">return</span> result.charAt(<span class="number">0</span>) == <span class="string">'0'</span> ? <span class="string">"0"</span> : result;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Given a list of non negative integers, arrange them such that they form the largest number. For example, given [3, 30, 34, 5, 9], the lar
    
    </summary>
    
    
  </entry>
  
</feed>
